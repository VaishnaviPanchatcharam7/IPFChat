Core Overview
IPF is a collection of modules designed to work together to produce an overall application capable to supporting high throughput, low latency payment transactions.  These modules aim to let the engineer focus on developing the business needs rather than complex technical details.
The following diagram shows a high level view of the overall IPF solution.
The 'Core' in terms of IPF is considered to be:
Orchestration - this is at the heart of the IPF Product. Orchestration is provided through Icon’s flo-lang module.
Data Processing & Persistence - being able to manage data is crucial to the success of any IPF project.  IPF provides it’s own canoncial data model based on the ISO standards, but it’s able to support any other types required too.
System Connectivity - being able to quickly and reliably integrate to other platforms is crucial for any payments platform.  The connector framework provides a fluent approach to utilise the power of the alpakka framework.
Testing - testing of reactive message based systems can be hard.  With this in mind, IPF provides the Icon Test Framework together with specific extensions and libraries for common IPF scenarios to simplify this complex area.
Orchestration
------------------------------
Flo-Lang
Orchestration is one of the key areas where IPF sets itself apart from competitors. We have built our own unique payments domain specific language (DSL) leveraging JetBrains MPS which enables clients to use it with minimal dependency on Icon.
Besides being a low-code language to quickly and flexibly model complex payment orchestration flows, one of the key benefits is that the code itself is generated automatically, along with test scenarios, graphical visualisation and documentation, meaning it is continually up to date and in sync.
The use of a payments domain specific language (DSL) simplifies and accelerates process definitions empowering the business whilst de-risking code delivery through alignment with documentation and testing.
The key point to understand here is that by using flo-lang, developers and business analysts can come together to talk in one common language and define the required process in a way that both sides can be confident the right process is in place.
Once the process has been defined, flo-lang then takes care of both:
Automated generation of a self-contained Akka powered Java domain.
Automated generation of BDD testing scripts, graphical views and supporting documentation (always in sync)
Flo-lang provides a number of key features to support payment processing, including:
Support for the Icon Payments Model, a pre-built ISO20022 type library.
Support for custom business types.
'Fan-out/Fan-in' processing, for asynchronous processing
Customisable response & reason codes
Retry, Timeout and Exception handling
Process aborting and resuming
Process flow chaining
Ability to define sub-flows that are reusable multiple times within a single flow and across multiple flows
All these features and more are discussed at length in the following documentation.
We recommend starting at Concepts to understand key concepts of the DSL.
Once these are understood, you can get instructions on how to set up a DSL environment in Getting Started.
Core
Concepts
------------------------------
Flo-Lang Concepts
Business Data
We start by considering data.  Data is what drives processing and decision making throughout IPF.
The first concept we consider therefore is the "Business Data Element".  It has four properties:
A name
A description
A "data type" - the data type can be any java type, whether it be standard classes like String, Integer etc or your own bespoke types.
A "data category" - an optional field, the possible values are an enumerated set that refers to the type of data that is being represented by this BusinessDataElement. This Data Category label is used by various IPF components such as IPF Data Egress and Operational Data Store, which can automatically record data captured from Process Flows automatically, depending on the Data Category. There are four core data categories:
PAYMENT - This is payment data that is modelled as ISO20022 message components within IPF.
PAYMENT_PROCESSING - This is data that relates to the processing of payments, such as meta-data and payment type information
CUSTOM - This represents custom data which may be attached to the payment
ADDITIONAL_IDENTIFIER - This applies to data elements that represent additional identifiers to be associated with the payment
Any MPS project can have as many different business data elements as you need.  These elements are defined within a "Business Data Library" which is simply a collection of related business data and as many different business data libraries can be defined as needed.
IPF provides a number of pre-configured business data libraries.  By default, any process is given the "error"  library which provides default elements for handling flow failures, namely:
Failure Event Name - this is the name of the event that registered the first failure in a flow.
Failure Response Code - this is the IPF response code for the failure.
Failure Reason Code - this is the IPF reason code for the failure.
Failure Reason Text - this is the IPF text description of a failure.
Failure Original Response Code - This allows specification of any original response code involved (which may have then been mapped to an IPF one)
Failure Original Reason Code - This allows specification of any original reason code involved.
Failure Original Reason Text - This allows specification of any original reason text involved.
The concepts of reason and response codes are discussed later in this document.
Within the lifetime of a payment each business data element is unique and it can be updated as required.
Flow
The processing of a payment is performed by a "Flow".   A flow represents a single business process from end to end and is designed to specify the lifetime of a single payment.  A single flow might have many paths through it, each representing a different way of processing an individual payment based on the data provided for that flow.  A flow contains a number of things:
A name
A description
A version
A global state set
A list of "States"
A list of "Events"
An "Initiation Behaviour"
A list of "Input Behaviours"
A list of "Event Behaviours"
A list of "Aggregate Functions"
A definition of each of these aspects are discussed in the following sections.
The combination of "Flow Name" and "Flow Version" uniquely identify a flow.  The version is just an optional numeric identifier, so for example a flow may be called "Test" and have version 3.  Then the flow can be unique identified as "TestV3".  If there was no version defined it can be identified simply by the name "Test".   This identifier is known as the "FlowId"
Global States
First we consider the "Global State Set".   The global state set is a set of states that represent the overall state of a payment.  It is particularly used where a payment may span multiple flows (for example if the payment processing is split into "initiation" and "execution" parts) but can also apply an overall grouping type state to the individual flow parts to simplify the apparent state transitions from a payment level.   Each flow level state can be mapped to a global state such that multiple flow level states can all be considered to leave the payment in the same overall global state.
A default global state set is provided which provides the following standard states: Pending, Accepted, Rejected, Manual Action Required and Cancelled.
States
The next concept to consider within our flow is a "State".  This is very simply a resting point on the flow that the payment can pass through in it’s journey, so for example we may have a very simple flow that goes from "State A" to "State B".
A state itself has a number of properties:
A name
A description
A global state
A terminal flag - the terminal flag is used to indicate that this ends the flow to which the state belongs.
Each flow can contain many different states.
Events
When a flow moves from one state to another, this is known as a "State Transition".  Within IPF, for a state transition to occur then the system needs to receive an "Event" on the processing journey of the payment.   In this case, it is actually a specific type of event known as a "Domain Event".  A domain event is a persisted factual occurrence - the arrival of an event means that something explicit has occurred which may cause some form of change to the processing of our payment.
An event has a number of properties:
A name
A description
A list of business data elements.
When an event is formed, then the system will check it’s own behaviour to determine what actions should be performed.  Whilst this behaviour is explored later in this document, it is worth noting here that there are three occasions when an event can cause a change to the processing, these are known as the "Event Criteria" conditions and are defined as:
On - this movement will happen upon the arrival of a single event (e.g. we may transition when receiving "Event 1")
On any of - this movement will happen upon the arrival of one of multiple events  (e.g. we may transition when receiving either of "Event 1" or "Event 2")
On all of - this movement will only occur upon the arrival of multiple events (e.g. we may transition only after receiving both "Event 1" and "Event 2")
Here we have described the "Domain Event" which is the type of event that is explicitly declared within any MPS solution.  However, IPF as a whole uses a number of different types of event:
"System Event" - these occur when something happens to the system and can be tailored to be specific to individual needs.
"Action Timeout Events" - these events occur during processing when configured timeout settings are broken.
"Decision Events" - these event are used as a response to receiving results from decisions.
All these event types are discussed later in this document.
External Domains
After an event is processed, the application can then perform one or more activities to determine what happens next on a payment.  So for example on receipt of "Event A" we may wish to do some validation and call some other application to ask it to validate our data.
To support this post-event processing, the most important concept is the "External Domain".  This represents some business domain - not our current flow’s - that we need to interact with.
For example, let’s assume that we need to talk to a sanctions system during part of the flow.  To support this, we would model that sanctions system as an external domain.
Each external domain consists of the three types of interaction we can make with it:
"Instructions" - instructions are the simplest thing we receive from an external domain.  It can be triggered by the external domain at any time and we will start processing.  This can be thought of as the external domain pushing information to us.
"Notifications" - notification are the opposite of instructions.  These are used when we want to push our data out to an external domain.
"Requests" - a request is used when we need a "response" back from the external domain in reply.
Instructions
Firstly let’s consider the instruction.  These can be initiated by an external system and contain the following properties:
A name
A description
A list of "Business Data Elements"
When the IPF application receives an instruction it will raise a corresponding event (the decision of which event to raise is described later).  The event’s business data is then populated with all the matching business data elements.
Notifications
Next up is the notification, like an instruction it has the following properties:
A name
A description
A list of "Business Data Elements"
When the IPF application sends out a notification it will populate on it all the business data elements it has a matching record for.
Requests
Finally, we consider the requests.   The request can be thought of to have to parts, the request itself and the corresponding response.
The request part contains:
A name
A description
A list of business data
A list of responses
The response part is slightly different and has some new features to consider:
A name
A description
A list of business data
A "response code set" - this is a group of response codes.  A "Response Code" is an expected outcome code for a response that could be used for onward processing.  In ISO terms this is analogous with a Status.
A "reason code set" - this is a group of reason codes.  A "Reason Code" is a reason why the response is set the way it.  So for example your response code could be "Rejected" with a reason "Incorrect Currency".  In ISO terms a reason code with a Status Reason.
A completing flag - this defines whether the calling request should be considered completed when this response arrives.  So for example consider a request where the external system sends a technical acknowledgement following by a final business response.  In this case we would define two responses - one to represent the technical ack (non completing) and one the business response (completing).
In ISO terms, a response code is analogous with a "Status", whilst a reason code is analogous with a "Status Reason"
The system provides a default "AcceptOrReject" response code set which is used for standard pass / fail type responses.   It also provides a set of all the ISO reason codes.
Now let’s put these elements together and form the basis of any flow:
So here we can see that when IPF receives something from an external domain (either an instruction or a response), it leads to an event being raised which may cause a state transition followed by the invocation of a notification or request to an external domain.
Domain Functions
It’s possible that we don’t want to have to call an external domain in order to continue processing our flow.  This might happen because either we know what to do next or we can calculate what to do next.  For this there are two other concepts that we need to consider:
In this case, one option is to use the "Domain Function" capability that the flow itself offers.  It works in a very similar way to a request / response pair in an external domain call except that in the case of a domain function the IPF application itself is a domain so the actual call stays internal (consider for example creating an external domain that represents our current flow - this would work the same way as a domain function but would be a mis-representation of the actual control logic).  So when we call a domain function, we will expect to get a response and then that response will be transformed into an event which can then cause onward processing.
Like a request, the domain function has a number of properties:
A name
A description
A list of business data
A list of responses
Additional Events
The second option is an "Additional Event"- these can also be used to move the flow on.
When an additional event is raised, the system will process it as though it has been received into the application via an instruction or response.
Let’s add these to our diagram:
Decisions
What however if we want to perform some logic conditionally.  So for example, we may only want to run a fraud check if the value of the payment is over £50.  In this case we can use a "Decision".
A decision allows us to perform some logic and then take different processing routes subsequently based on the outcome of that decision. A decision has a number of properties:
A name
A description
A list of business data - this is the data that is sent when calling the decision so that it can process based upon it.
A list of "Decision Outcomes" - these are the possible results of running the decision, each decision can have as many different outcomes as needed and these outcomes are unique to the decision.  They are defined simply by providing a name.
The decisions themselves are stored within a "Decision Library".  The libraries are flow-independent and as such the same decision can be used in multiple flows.
We can use a decision in two places:
To determine which event needs to be raised in response to an input (response or instruction)
To determine which actions need to be performed after a state transition.
Lets add these to our diagram:
A special type of event "A Decision Outcome Event" will also be raised so that the fact the decision has been invoked and a result returned will be audited and can be used on onward processing.
Aggregate Functions
Another useful utility to consider is the "Aggregate Function".  An aggregate function is a piece of logic that can be executed upon receipt of an event to perform some kind of logic upon the data received. This data is considered "in flight" and thus is not persisted by the application.
So a good example of this is say a counter that tracks the number of times something has occurred during a flow - each time the function is called we may update that counter.  The outcome of the aggregate function then becomes available down the flow.
Another good use case may to perform a data mapping exercise to transform the data into something that can be used downstream.
Let’s add the aggregate function to our diagram:
Behaviours
The next concepts to consider are both types of grouping.  In order to separate the logic we need to define when processing an input to the system (from a response or instruction) and generating the event to the logic required when processing the actual behaviour of the system based off that event we have two grouping concepts:
"Input Behaviour" - this is a the behaviour that specifies for each input what event will be generated.
"Event Behaviour"  - this is the behaviour that specifies what actions should be taken on receipt of an event.
Input Behaviour
An input behaviour has a number of properties:
An input - this is the input (instruction or response) upon which the behaviour is triggered.
A response code - this is the response code (linked to the response if the response is an input, otherwise this field is not applicable) for which the behaviour applies
An event - this can be either an event directly or via the execution and resulting outcome of a decision.
Note that when using response codes, if one is not defined on an input behaviour this will be considered the "default" behaviour for all response codes.
Event Behaviour
The event behaviour is a little more complicated.  It has a number of properties:
A "Current State" - this is the state upon which the flow must be in for the behaviour to apply.
A "Criteria" - this is when the behaviour applies ( on / on all of / on any of)
A list of events - one or more events, these can be any type of event (e.g. domain, timeout etc)
A "Move to State" - the destination state of the behaviour
A list of actions - these are the actions that should be performed after the state transition, i.e. requests, notifications etc.
Lets update our diagram to show these:
Note that the aggregate function, as a self contained unit of calculation is not considered as either the event or input behaviour but as a functional capability of it’s own
Initiation Behaviour
There is one more key type of behaviour to consider, that is the "Initiation Behaviour".  The initiation behaviour is a specialised version the previously defined input behaviour but is only used to start a flow.  It is not linked to an external domain so that we can initiate the flow potentially from many different sources.
An initiation behaviour has a number of properties:
A list of business data
An initial state to move to
A list of actions to perform
Note that when the initiation behaviour is invoked, a flow will start and the "FlowInitiated" event will be raised.
We have now reviewed all the components that make up a single flow.
Subflows
The next thing to consider is reusable segments of flow.
For example,  consider a sanctions check that may be required to be called from various different places within the flow.   We could specify each section of the flow separately and write out the logic each time but ideally we would like to be able to simply reuse common logic each time. This is where the "Subflow" concept is introduced.
A subflow is a reusable flow component.  It is essentially the same as a flow in that it has states, input behaviours and event behaviours.  However, a subflow has no life of it’s own and is only used as a mechanism of reuse and therefore MUST be contained within an outer flow.  When calling a subflow it is very similar in behaviour to receiving an event:
The key thing to understand here that is instead of moving to a state and then calling an action like the normal processing above, here we move to a pseudo-state that acts as an entry point into the subflow.  "Control" of the flow is then handed over into the subflow, at this point it will process through the input and event behaviours until it reaches a terminal state in the subflow.  When it reaches a terminal state, control will be passed back to the calling flow with a result of the name of the terminal state.  This can then be used for onward processing.
Note that in order to achieve reuse of the subflow in multiple places, then when a subflow is placed within a main flow it’s state’s will be displayed as "<subflowid> <subflow state name>" where <subflowid> is the psuedo-state name of the calling flow and <subflow state name> is the name of the state within the subflow.
Flow Calls
Finally, it’s also possible to directly call one flow from another.  In this case control is handed over to the secondary flow and we wait for a result execution back.  The child flow can report that it has reached any state back to the parent flow.  Most commonly, this will be when a terminal state is reached and the child flow has finished processing, but it also allows for feedback from the child flow for intermediary states before it finishes.  This provides an opportunity to pass control back and forth between the child and parent as required.
Orchestration
Getting Started
------------------------------
Getting Started
Flo-Lang is a DSL built with Jetbrains MPS that was designed to model orchestration workflows and generate various
software components that can be used in a Java application. Currently, these are:
Akka Event-Sourced Domain model + Finite State Machine
Graphiz graphs for visualising the flow
Test-FW complaint BDD Test scenarios for the possible permutations through the flow
Embeddable documentation from the model, in the form of Asciidoc
MPS itself is traditionally IDE driven, so we have built a set of modules and configurations that allow transparent integration
into downstream projects uses, providing that the downstream project is Maven based, and adheres to a set of conventions.
Note that whilst the MPS IDE (Or Intellij with MPS Plugins) is needed to create and modify solutions, it is not needed to compile
existing solutions via the Maven build. The Maven build process actually downloads an MPS binary and compiles the solution in a headless mode.
Users have several options on workflow with regard to obtaining MPS.
1. Installing MPS
Current working MPS version for command line builds and IDE is 2021.3.1.
The most common way to install MPS is via Jetbrains ToolBox as this manages installation of different versions.
An alternative is to use the MPS binary that gets downloaded and compiled against as part of the build process.
There is a shell script to start the IDE in the following equivalent folder:
<your_project_root>/<your_domain_root>/mps/target/mps/bin/mps.sh
Project Setup
It is always advised that to create a new project setup you use the IPF archetype.  This provides a simple mechanism to bootstrap a new project and gives you the ideal launch pad to create your payment flows.
Once your new project has been set up using the archetype, we can simply open the project in MPS by selecting File/Open and then navigating to <generated_archetype_root>/domain-root/mps.
Concepts
How to review changes
------------------------------
Code Reviewing
How do I review code?
MPS generates xml file which are not easy to understand especially if you are trying to review changes from them.
If you view diffs in your VCS you will be shown a xml diff which is not very readable.
Luckily, MPS provides review integration into their IDE which makes thinks much easier and provides the opportunity to resolve conflicts in a human-readable way.
The Git tab in MPS shows you the commits and by simply double-clicking on a change from another branch it will open a DiffPreview window that shows the changes from the editor perspective.
How do I use BitBucket to add review comments?
MPS models are AST and persisted as an XMI, which makes them less trivial to code review remotely, meaningfully
There is a plugin for the MPS IDE which leverages BitBucket Server to allow meaningful code reviewing.
(Code reviewing/adding/replying to comments is done in the IDE).
Firstly, create a Personal Access token in BitBucket from the Account page (it only needs read permissions), Copy the key.
Next, download both com.workday.mps.git4mps.zip and com.workday.mps.review.zip from
github.com/Workday/mps-code-reviewer/releases.
Once installed, hit the settings and there should now be an option for review integration that requires your access key and the repo url:
bitbucket.iconsolutions.com/projects/IPFV/repos/flo-lang
Once set up the right-hand side pane should update as new pull request are added for the checked out branch.
The workflow is that to review someone else’s code you need to check out the branch and then add comments in MPS.
Comments can be added to nodes or generally on the review if needed.
the branch name must match exactly the JIRA ID (no feature prefix )
Best practices to minimize merge conflicts
To minimise the chance of merge conflicts it’s recommended to save the model per-root, which would create a xml file per root object (flow etc). To do this select the "Pre-installed XML-based File-Per-root Format" when creating a new model:
Getting Started
How to use the generated code
------------------------------
How do I use the generated code?
This page explains how to get started with the code generated by the flo-lang project.  It describes all the different interfaces that will be generated and reviews the central domain class used to interact with the domain itself.
The flo-lang language has three key levels to consider:
Solutions - this is effectively an overall project.  A solution can contain many models.
Models - this represents a single self-contained domain.  A model can contain many flows.
Flows - this is a single executable process flow.
From a generation perspective, generation is done at the model level.
This means that for each model an isolated set of code is generated that is not dependent on any other model.  Each model will provide one central class for interacting with the domain, and that class can contain access to multiple different flows within it.
The Generated Interfaces
Action Adapters: Connecting to things outside the domain
For each external domain that is used within the model an action adapter interface is generated.  This class will provide a method which is what the domain will call when it determines it needs to interact with the external system.
The implementation of the action adapter methods, therefore is expected to include the code to physically pass the request onto the respective external domain:
@Override
public CompletionStage<Void> execute(SampleExternalDomainAction action) {
    var externalDomainDto = convertToExternalDomain(action);
    return sampleExternalDomainConnector.send(action.getProcessingContext(), externalDomainDto)
            // during development it is sometimes useful to include debug information like this
            .thenAccept(report -> log.debug("Send completed with {}", report.getOutcome()));
}
Decision Adapters: Providing the business logic for the decision
If the domain uses any decisions, then a decision adapter will be generated.  The respective decision adapter methods will be called when the flow decides it needs to execute a decision, therefore the implementation should contain the business logic that is required to execute that decision correctly.
Domain Function Adapters: Providing the business logic for domain functions
Domain functions are in reality very similar to external domains.  The main difference is that whereas an external domain is expected to hand over control of processing to some external system, the domain function is expected to process that logic within it’s own boundaries.
If domain functions are used within the model, then a domain function adapter interface will be generated providing method definitions for each domain function to be used.
Aggregate Function Adapters: Providing additional data manipulation within a flow
An aggregate function provides the capability to hook into the arrival of an event within a flow and provide a utility to manipulate data.  For each flow that uses aggregate functions, an interface will be generated.  An implementation of these interfaces will be required.
Impacts of Aliasing
An alias is a type of business data concept that allows the call to one of the capabilities defined above to be called in different circumstances using different data sets.  As a result, when alias’s are used then the generation may produce more than method on each interface, but with different parameter inputs to reflect the different aliases implementations.
Use of external models
It is possible to define various supporting parts of a flow in a different model.  For example, you may choose to define your external domains in a separate model that can then be reused from multiple different models rather than defining the same external domains repeatedly within different functional models.  From a generation perspective, this is no different to the domains having been defined locally within the functional model itself.
Building the domain
When a model is generated, it will generate a class that represents the domain as a whole.  This class will be called <ModelName>v<VersionNumber>Domain (note that if there is no version number then just the model name is used).  This class provides all the functionality to interact with the domain.
Building the domain class
A Builder is provided with the domain class, it allows for the provision to the domain of everything it needs to be able to work:
For each external domain used in the model, an implementation of the appropriate action adapter should be provided using the with<ActionAdapterName>ActionAdapter methods.
If decisions are required, an implementation of the model’s decision adapter interface should be provided using the withDecisionAdapter method.
If domain functions are required, an implementation of the model’s domain function adapter should be providing using the withDomainFunctionAdapter method.
When building the domain, the minimum that must be specified is:
The actor system for it to run on
Implementations of all the generated interfaces (external domains, domain functions & decisions)
The following shoes how a simple domain may be built.
 new XYZDomain.Builder(actorSystem)
                .withDomainFunctionAdapter(new SampleXYZDomainFunctionAdapter())
                .withDecisionAdapter(new SampleXYZDecisionAdapter())
                .withABCFlowAggregateFunctionAdapter(new SampleABCFlowAggregateFunctionAdapter())
                .withAnExternalDomainActionAdapter(new SampleExternalDomainActionAdapter())
                .build();
When the build method is invoked, then all the flows within the model will be started.
Optional domain parameters
When configuring the domain, in addition to the mandatory parameters defined above it is possible to define a number of optional attributes which, if not provided, are defaulted.
EventBus: the event bus implementation to be used when the domain raises system events.
ConfigAdapter
SchedulerAdapter
RetrySupport
EventProcessorSettings (per flow)
MetaDataExtractor (per flow)
Using the domain
Handling responses from an external domain
The domain class provides methods to be able to provide responses back into the domain.  These are split out on a per external domain basis. To invoke these we call:
 XYZDomain.externalDomainPort().handle(new ExternalDomainResponseInput());
The result of the handle method call is a CompletableFuture which completes once the flow has applied the input to the current state. The future carries with it a Done object, which provides a detailed view of what the end result of applying the input was:
@AllArgsConstructor
@Getter
public class Done implements CborSerializable {
    private final String aggregateId;
    private final String aggregateStatus;
    private final String commandName;
    private final Result result;
    public enum Result {
        EXECUTED, DUPLICATE, UNEXPECTED, ERROR
    }
}
Out of the listed fields, the result is in most cases the important one to look at. Result it can have the following values, each representing a different outcome:
EXECUTED means that the input was successfully applied to the current state but offers no guarantees about the resulting operations (actions, notifications, domain functions)
DUPLICATE means that the input was deemed a business duplicate of a previously handled input and was thus not applied to the current state
UNEXPECTED means that the input was deemed inappropriate for the current state of the flow and was thus not applied to the state
ERROR means that an issue was encountered while executing one of the operations that were triggered as a result of applying the input (actions, notifications, domain functions)
Generally speaking, unless you have specific requirements (e.g. raising a system event on certain duplicate responses), you can ignore the outcome of applying the input and consider the operation a success as long as the future itself doesn’t fail.
Handling of duplicate responses from an external domain
By default, the external domain port that the domain provides comes bundled with configurable retry support. In order to ensure that retries are not interpreted as independent responses from the external domain, the port will assign random UUIDs as inputId and physicalMessageId if those values are not set on the input object.
inputId represents a unique business identifier (from the perspective of the flow) of an external domain’s response and is the identifier based on which idempotent processing will be performed
physicalMessageId represents a unique transport identifier of an external domain’s response, and it is useful for cases where the same physical message can be consumed multiple times (e.g. coming from message brokers), and the system needs to distinguish between a duplicate consumption (same message consumed twice) and a duplicate message (two messages with the same resulting inputId)
When using the IPF connector framework, all supported broker-based receive connectors will contain a suitable PhysicalMessageId within their ReceivingContext object, which can then be used to populate the input.
If the flow receives two inputs with the same inputId, the second input will always skip execution and either result in a Done.Result.EXECUTED or a Done.Result.DUPLICATE outcome, depending on the physicalMessageId. If the physicalMessageId is also the same on both inputs, the second input is treated as a duplicate consumption and Done.Result.EXECUTED is returned. Otherwise, the input is treated as a duplicate message and Done.Result.DUPLICATE is returned.
While the default approach works for most scenarios, there are some corner cases where it may not be enough to ensure correct processing in your flows - e.g. when your flow handles the same input type from multiple non-exclusive states - thus requiring you to specify at least an inputId in order to avoid duplicate consumptions incorrectly triggering a flow transition:
 XYZDomain.externalDomainPort().handle(new ExternalDomainResponseInput
    .Builder(id, ExternalDomainResponseCodes.OK)
    .withInputId(someInputIdFromResponseMessage)
    .build());
Calling another flow from an action
When communicating between different flows that are part of the same logical unit of work, it is important to pass along the current flow’s EventId to the other flow. An EventId carries within itself its owner flow’s Lamport timestamp that the other flow will continue incrementing. Thus, by passing along an EventId from one flow to another, we enable domain events - belonging to different flows within a single logical unit of work - to be ordered in a causal, happened-before order.
Every generated Action provides access to the current EventId via its getCurrentEventId method, whereas every Input.Builder provides a withCausedByEventId that can be used to pass along the EventId of the collaborating flow:
@Override
public CompletionStage<Void> execute(SampleExternalDomainAction action) {
    var externalDomainDto = convertToExternalDomain(action);
    return AnotherDomain.appropriateDomainPort().handle(new AppropriateDomainResponseInput
                .Builder(id, AppropriateDomainResponseCodes.OK)
                .withCausedByEventId(action.getCurrentEventId())
                .build());
}
Being able to determine the causal order of events in a distributed computer system is crucial when building read-side projections of a unit of work  based on the domain events of its constituent flows. Without EventIds and their Lamport timestamps, projections would be forced to use system clock timestamps in their stead and as different nodes or processes will typically not be perfectly synchronized, clock skew would apply and incorrect event order would be arrived at as a result.
Informing the domain that a payment has timed out
When a payment times out, it is necessary to tell the domain that this has occured.  To do this, a helper class is available on the domain.
 XYZDomain.timeout().execute(aggregateId, actionName);
Ad-hoc functions within the domain
The domain class also provides the ability to perform ad-hoc functions within the flow these are:
Aborting a payment
Accessing the payments status
Accessing the payments aggregate
These can all be done directly by calling the appropriate method on the domain class itself:
 XYZDomain.abort(aggregateId, reason);
 XYZDomain.getStatus(aggregateId);
 XYZDomain.getAggregate(aggregateId);
Identifying the available flows and versions
To be able to clearly understand what flows are available within a generated domain, we can interogate the domain class.  For each flow we have a static access available as per below:
 XYZDomain.Flows.AFlow.name();
 XYZDomain.Flows.AFlow.getAggregate();
 XYZDomain.Flows.AFlow.getLatestVersion();
How to review changes
How to increase memory for BDD generation
------------------------------
How do I increase the memory of the MPS build?
Depending on how complex your MPS flow is there could be the chance that you would get an OutOfMemoryError exception when building your flow through Maven. This would largely be due to fact that we generate BDD tests that have a test scenario per transitional state.
To increase the memory during the build and generation phase you can alter the "generation max heap size in mb" MPS Settings in the build script in as seen in the following screenshot:
You also need to alter the heap size in the build.xml file of the root mps maven module as follows:
How to use the generated code
System Connectivity
------------------------------
Connector
Connectors provide an abstraction around communication transports which aims to make integration simpler.
This page introduces the core concepts used in the connector library.
What is a Connector?
A connector is simply a set of common interfaces that provide a means to communicate with external systems, synchronously or asynchronously.
Implementation details specific to the configured transport mechanism are hidden behind the interface.
This helps to improve understandability and lets teams standardise integration instead of creating separate implementations each time a new system needs to be added.
The main benefit of using connectors is that they reduce the amount of boilerplate code required for integrating with systems which use common data interchange formats (such as XML or JSON) and transports (such as HTTP, JMS or Kafka).
Connector Features
Connectors implement a number of features out of the box which are commonly required when integrating with external systems.
Backpressure
Message Throttling
Message Encryption
Message Validation
Message Association
Message Logging
Error Handling
Resilience
Metrics
Types of Connector
Connectors can be used for either sending or receiving messages.
Detailed documentation specific to each type can be found in the following pages.
Sending Connector
Receiving Connector
Connector Stages
Each connector is a composition of stages that a message flows through.
The stages provide different capabilities, many of which are optional and can be tailored to specific requirements dependent on the situation.
Receive
The purpose of a receiving connector is to take a message from the configured transport and map it into a known format which can then be handled.
Received messages must be associated (correlated or identified) so that they can be linked to past/future messages.
The stages that can be configured for receiving are.
Payload Decryption (optional)
Conversion to Target Type
Message Association
Message Validation (optional)
Message Logging (optional)
Message Handling
See the Receive Stages section in the Receiving Connector documentation for more further details around each of the stages.
Send
The purpose of a sending connector is to take a message and send it over the configured transport.
Since networks are unreliable, the message sending can be configured to send over multiple transports (with load balancing or fallback strategies) and can employ circuit breaking and retries to overcome transient networking issues.
The stages which can be configured for sending are.
Message Validation (optional)
Message Correlation (optional)
Message Logging (optional)
Payload Encryption (optional)
Resilient Send
See the Send Stages section in the Sending Connector documentation for more further details around each of the stages.
Akka
The Connector implementation uses Akka Streams.
When a connector is built, the individual stages known in Akka Streams nomenclature as operators, are joined into a single stream.
The provided transport implementations use Alpakka, the Akka Streams subproject dedicated to integration.
It is similar to Apache Camel in its goals; though is implemented with the actor model, that allows for highly scalable, concurrent processing.
The Alpakka project has implemented integrations with a large number of common technologies.
For more information on what Alpakka supports, visit the following documentation.
How to increase memory for BDD generation
Concepts
------------------------------
Concepts
Receiving Connector
Sending Connector
Connector Transports
System Connectivity
Receiving Connector
------------------------------
Receiving Connector
Receiving connectors are responsible for processing messages received on a configured transport and transforming the message into a known type, before delegating the handling of the message to the client.
All receiving connectors implement the ReceivingConnector interface, which itself extends OperableConnector.
public interface ReceivingConnector extends OperableConnector {
}
public interface OperableConnector {
    /**
     * Retrieve name of the connector.
     */
    String getName();
    /**
     * Starts the connector.
     */
    void start();
    /**
     * Starts the connector's health check procedure.
     */
    void startHealthCheck();
    ConnectorHealth getHealth();
    /**
     * Shuts down the connector.
     * @param reason the reason for shutdown
     */
    CompletionStage<Void> shutdown(ShutdownReason reason);
    /**
     * Returns the connectors running status
     */
    boolean isRunning();
}
Generic Type T
The ReceivingConnector interface is not concerned with types, though the default implementation ReceiveConnector<T> is generically typed with type T, where T is short for 'target type'.
Messages received in the transport layer are passed to the connector wrapped in a TransportMessage object and are converted to the target type as soon as possible.
This allows future stages to work with a known type which can optionally be validated to ensure data integrity.
Stages
The connector implementation uses Akka Streams.
Connectors are comprised of a number of configurable stages that are executed asynchronously, where each stage performs some transformation or checks to ensure the messages are valid before delegating control back to the client to handle the received message.
The image below roughly describes the stage-based approach when receiving messages.
Note that some stages are optional and will be skipped if they are not configured when connector is built.
The following sections will briefly cover each stage.
Filtering
Some connector transports such as JMS already have the functionality to filter a message (using JMS Selectors), but
others such as Kafka or HTTP don’t. For this reason the Connector framework offers a filtering functionality.
To learn how to filter messages, see the
Encryption documentation.
Payload Decryption
Sometimes transport encryption protocols, such as TLS are not sufficient.
In these cases application level encryption can be applied to the messages themselves.
When receiving an encrypted message, its payload must be decrypted before we can transform it to the target type.
To learn how to configure decryption, see the
Encryption documentation.
Convert to Target Type
The payload found in the TransportMessage object passed from the transport layer to the connector is untyped.
The connector is responsible for mapping the payload type into a known type.
This can be achieved by providing an implementation of the ReceiveTransportMessageConverter when building a receiving connector.
@FunctionalInterface
public interface ReceiveTransportMessageConverter<T> {
    T convert(TransportMessage transportMessage);
}
Message Association
Received messages are either a response to a request we made earlier or a new request.
When setting up a connector to receive messages we typically know which of these scenarios is expected.
If a message is a response to a previously sent request, then it should be correlated back to the original message using the correlation service.
Otherwise, if the message is a request we should generate (or extract) an identifier for the message so that we can correlate it later.
To learn more about message association, see the
Message Association documentation.
Message Validation
When receiving messages, it often makes sense to make sure the message is valid before continuing processing.
This can be in terms of schema validation or business logic validation.
Connectors can optionally be configured to validate message by providing an implementation of the BeanValidator interface.
To learn more about message validation, see the
Message Validation documentation.
Message Logging
Message logging is a common requirement when building integrations across multiple systems, as it can aid in debugging or provide a comprehensive audit of all messaging.
Connectors can optionally be configured to log messages by providing an implementation of the MessageLogger interface.
To learn more about message logging, see the
Message Logging documentation.
Message Handling
The final and arguably most important stage in the receiver flow is the message handling stage.
This is where the message, after passing through decryption, validation, association and logging can finally be handled by the client code.
Handling the message is done by providing an implementation of the ReceiveHandler functional interface.
@FunctionalInterface
public interface ReceiveHandler<T> {
    CompletionStage<Void> handle(ReceivingContext receivingContext, T payload);
}
The handle function takes a ReceivingContext and the generically typed payload and returns a void future.
Resilience
The resilience of a given receive connector is fairly configurable and relies on two concepts: automated retries and a dead letter channel.
The image below shows some potential scenarios that may be encountered when processing a message and the mechanisms used to handle them.
Retries
It is possible to configure a retry policy for receiving connectors by supplying an instance of ResiliencySettings.
For more details, see the Resilience documentation.
The resilience settings are used to distinguish between recoverable (retryable) exceptions and their counterparts.
If an exception is deemed recoverable, retries will be attempted according to the configuration.
Once the retries have been exhausted and no recovery was possible, the error and the connector message that caused it are then passed to the dead letter channel for further processing and the message is NACKd (negatively acknowledged).
Dead Letter
The dead letter channel component is represented by the DeadletterAppender interface.
@FunctionalInterface
public interface DeadletterAppender {
    CompletableFuture<Void> append(ReceiveConnectorException receiveConnectorException);
}
The general idea behind the appender is to allow clients to support potentially complex processing that may need to happen when messages fail to be received.
Such as persisting them in an external storage, scheduling their reprocessing with a distributed scheduler, etc.
Concepts
Sending Connector
------------------------------
Sending Connector
Sending connectors are responsible for taking messages supplied by a client and sending it over the configured transport mechanism.
All sending connectors implement the SendingConnector interface, which itself extends OperableConnector.
public interface SendingConnector<D, R> extends OperableConnector {
    /**
     * Send a request via the connector without a supporting context
     */
    default CompletionStage<R> send(@NotNull ProcessingContext processingContext,
                                    @NotNull D domainMessage) {
        return send(processingContext, domainMessage, SupportingContext.empty());
    }
    /**
     * Send a request via the connector with a full sending context
     * that allows users to supply a supporting context as well
     */
    CompletionStage<R> send(@NotNull ProcessingContext processingContext,
                            @NotNull D domainMessage,
                            @NotNull SupportingContext supportingContext);
    /**
     * Allows setting of the meterRegistry implementation for a connector
     * after connector build
     * This allows implementations to set the registry for all SendingConnector instances
     * at once instead of having to set them at a per connector bases at build time
     */
    void setMeterRegistry(MeterRegistry meterRegistry);
}
public interface OperableConnector {
    /**
     * Retrieve name of the connector.
     */
    String getName();
    /**
     * Starts the connector.
     */
    void start();
    /**
     * Starts the connector's health check procedure.
     */
    void startHealthCheck();
    ConnectorHealth getHealth();
    /**
     * Shuts down the connector.
     * @param reason the reason for shutdown
     */
    CompletionStage<Void> shutdown(ShutdownReason reason);
    /**
     * Returns the connectors running status
     */
    boolean isRunning();
}
Generic Types D and T
Let’s first start by describing what the D and T generic mean.
D is the Domain type.
This is a message in our "internal" canonical data model
T is the Target type.
This is a message converted to the "external" data model with which we’re integrating.
Stages
The connector implementation uses Akka Streams.
Connectors are comprised of a number of configurable stages that are executed asynchronously, where each stage performs some transformation or checks to ensure the messages are valid before sending it to the transport mechanism.
The image below roughly describes the stage-based approach when sending messages.
Note that some stages are optional and will be skipped if they are not configured when connector is built.
The following sections will briefly cover each stage.
Message Validation
When sending messages, we may need to check the validity of the payload, especially if any of it is user generated since this could contain errors.
Connectors can optionally be configured to validate message by providing an implementation of the Validator interface.
To learn more about message validation, see the
Message Validation documentation.
Message Correlation
One of the limitations around sending and receiving message asynchronously is that responses to messages need to be correlated with one another.
This can be achieved by providing an implementation of the CorrelationService interface when building a sending connector.
The correlation details will be persisted before sending the message and then can be retrieved later when the response is received in a separate receiving connector.
To learn more about message correlation, see the
Message Association documentation.
Message Logging
Message logging is a common requirement when building integrations across multiple systems, as it can aid in debugging or provide a comprehensive audit of all messaging.
Connectors can optionally be configured to log messages by providing an implementation of the MessageLogger interface.
To learn more about message logging, see the
Message Logging documentation.
Payload Encryption
Sometimes transport encryption protocols, such as TLS are not sufficient.
In these cases application level encryption can be applied to the messages themselves.
When sending a message, its payload can optionally be encrypted before it is sent over the transport.
To learn how to configure encryption, see the
Encryption documentation.
Resilient Send
The final and only mandatory stage in the sending connector flow is the resilient sending stage.
This is where we can configure how to handle failure scenarios typically encountered when sending messages over networks, by utilising resiliency strategies, such as retries, circuit breaking and re-routing.
To learn how to configure resilience settings, see the
Resilience documentation.
Multiple Connector Transports
Some systems represented by a connector can be reached over multiple transports.
Example of this could be a series of non-clustered MQ queue managers or Kafka brokers.
To cater to this use-case, it is possible to pass a list of connector transports when building a sending connector.
        SendConnector<ExampleType, ExampleType> connector = SendConnector
                .<ExampleType, ExampleType>builder("ExampleSystem")
                .withConnectorTransports(List.of(transport1, transport2)) (1)
                .withRoutingLogic(routingLogic) (2)
                .withQueueSize(0)
                .withMaxConcurrentOffers(1)
                .withSendTransportMessageConverter(converter)
                .withCorrelationIdExtractor(correlationIdExtractor)
                .withActorSystem(actorSystem)
                .build();
1
Passes transport1 and transport2 to the connector.
2
Configures the routing strategy.
The combination of withRoutingLogic and withConnectorTransports allows for load-balancing of requests over a connector across multiple transports, using various implementations of routing logic.
The default routing logic is round-robin.
For further details around this topic, see the Routing Logic section in the Resilience documentation.
Exactly one of the builder methods withConnectorTransports or withConnectorTransport should be called.
If both are used then an IllegalArgumentException will be thrown when calling the build method.
Backpressure
Backpressure is discussed in the Backpressure documentation page, check it out for more details on this topic.
Streams are started (materialised) with a queue.
This means that it is possible to configure the amount of work that a connector, and by association the system with which it is communicating, can handle at a time.
This is useful for backpressure situations where there is a fast producer which is producing at a rate much faster than what the downstream system (represented by this connector) can handle.
The queue size is configurable using the withQueueSize builder method.
SendConnector<ExampleType, ExampleType> sendConnector;
sendConnector = new SendConnector
        .Builder<ExampleType, ExampleType>(connectorName)
        .withQueueSize(1) (1)
        .withConnectorTransport(transport)
        .withSendTransportMessageConverter(messageConverter)
        .withCorrelationIdExtractor(correlationIdExtractor)
        .withActorSystem(actorSystem)
        .build();
The queue size argument.
It is possible to source this value from a placeholder, for example, to enable replacing this value from a configuration file.
If no queue size is provided, then the default queue size is defined by the DEFAULT_BUFFER_SIZE which can be found on the SendingConnector interface.
int DEFAULT_BUFFER_SIZE = 50;
int DEFAULT_MAX_CONCURRENT_OFFERS = 500;
int DEFAULT_PARALLELISM = 500;
int DEFAULT_CALL_TIMEOUT_SECONDS = 30;
Receiving Connector
Connector Transports
------------------------------
Connector Transports
The purpose of connectors is to abstract away the details of the underlying transport by placing the implementation behind a common set of interfaces.
This enables us to plug in any transport mechanism that implements the required interfaces for sending and receiving messages and the connector core library takes care of all other aspects, such as mapping, validation, logging, etc.
When creating a new connector transport there are two interfaces that need to be implemented.
These are SendingConnectorTransport and ReceivingConnectorTransport.
The connector library provides some of the most commonly used transport mechanisms "out of the box".
These are for Kafka, JMS, HTTP and Filesystem, though custom transports can be developed with relative ease, and they should "just work" with the connector library.
Receiving Transports
Receiving connector transports implement the ReceivingConnectorTransport interface, that has the following signature.
public interface ReceivingConnectorTransport extends OperableConnectorTransport {
    void startProcessingMessagesVia(Flow<ReceivedMessage<TransportMessage>, ReceivedMessage<Void>, ? extends KillSwitch> receiverFlow, Criteria filterCriteria);
    default void acknowledge(ReceivedMessage<?> receivedMessage) {
    }
}
A well implemented startProcessingMessagesVia method takes an un-materialised flow and filter criteria.
If filter criteria is null it will pass any received messages to the flow. If filter criteria is specified, only messages which fulfill
the criteria will be passed down the flow.
The acknowledge method can optionally be implemented if the transport mechanism needs to acknowledge the receipt of a message.
Sending Transports
Sending connector transports implement the SendingConnectorTransport interface, that has the following signature.
public interface SendingConnectorTransport<T> extends OperableConnectorTransport {
    void startProcessingMessages(Integer maxConcurrentOffers);
    CompletionStage<DeliveryOutcome> offer(MessageDelivery<T> messageDelivery);
}
The generic type T is "Target Type", i.e. the type of message to be sent to the target system, before serialisation.
The offer method takes a MessageDelivery<T> and returns a future containing the outcome of the message send.
Typically, the offer method takes the message to send and adds it to an internal queue of messages that are to be sent to the transport mechanism.
The startProcessingMessage method should enable message to be sent via the transport, in most cases this is where the transport flow is materialised.
Custom Connector Transports
To develop a custom transport, some basic knowledge about Akka Streams is required since this is ultimately what is used to drive the flow of messages through the system.
For an introduction to Akka Streams, consider the 
Lightbend Akka Streams for Java course that is provided by the Lightbend Academy.
Sending Connector
Features
------------------------------
Features
Connectors implement a number of features out of the box which are commonly required when integrating with external systems.
The following pages describe the features provided by the connector library.
Examples and configuration details are provided, wherever possible.
Backpressure
Message Throttling
Message Encryption
Message Validation
Message Association
Message Logging
Error Handling
Resilience
Metrics
Connector Transports
Backpressure
------------------------------
Backpressure
By virtue of using the Akka Streams library, the connectors are built to be reactive, i.e. demand-based.
This means we benefit from backpressure and are able to deal with heavy loads without a large performance hit, or systems crashing due to over-consumption of resources (provided the system is tuned correctly).
Akka Streams are composed of three main components which, together, form a runnable graph.
These are sources, sinks and flows.
Sources are producers (emit messages), sinks are consumers (receive messages) and flows are a combination of both (transform messages).
Consumers are able to signal demand for more messages, which is beneficial as this way consumers don’t get inundated with messages.
Using backpressure this way is most effective when messages are written to some intermediate data storage, i.e. event store, db, kafka topic or jms queue as this way peaks in message production won’t affect the downstream consumers.
In some situations (e.g. HTTP transports), backpressure can cause requests to be rejected under heavy load as messages have nowhere to go if the consumers cannot process them fast enough.
Though this situation is the same with or without backpressure.
Features
Message Throttling
------------------------------
Message Throttling
The rate of processing messages may sometimes need to be throttled to avoid overloading downstream systems and even out spiky traffic.
Both sending and receiving connectors can be configured to throttle the rate of messages they can process.
This is in addition to backpressure (demand signalling) we get by virtue of using Akka Streams in the implementation of the connector library.
That is to say if some part of the processing pipeline is running slowly then the backpressure would kick in and would effectively rate limit the system.
The difference with throttling is we can tune the maximum rate of messages over a window of time.
This is especially useful if we have a slower downstream system which we don’t want to overload.
Tuning Considerations
The two parameters that can be tuned for throttling are the window duration and the message count.
The throttling mechanism will allow messages through at whatever rate they come in up until the maximum number of messages have been reached within the window duration.
This is important because we could have two configurations that provide the same overall throughput, for example:
Window Duration
Message Count
Throughput (TPS)
10 seconds
1000
100
1 second
100
100
In the first example it is possible that 1000 message are processed within the first second, this would mean for the remaining nine seconds, nothing would be processed.
Whereas in the second example, under the same load we would see a smoother rate of processing as the window is smaller.
Throttle Configuration
To configure throttling on both sending and receiving connectors, a duration and count must be supplied when building the connector.
If the throttle parameters are not set, then the connector will process messages as they come, unconstrained, albeit with backpressure from the downstream consumers.
initiatingReceiveConnectorBuilder()
        .withThrottleDuration(Duration.ofMillis(500))
        .withThrottleCount(1);
Backpressure
Message Encryption
------------------------------
Message Encryption
Sometimes messages require security above and beyond transport level security, such as SSL.
For these cases connectors can be configured to encrypt or decrypt messages when sending or receiving messages, respectively.
This places security at the application level in addition to any transport level security mechanisms that may be in place.
Crypto Interface
To add message encryption, an implementation of the Crypto interface must be supplied when building the connector.
If no Crypto implementation is provided then this stage is skipped and the transport message is received as is.
The interface is simple and consists of two methods, encrypt and decrypt.
public interface Crypto {
    /**
     * @param plaintext        message to send
     * @param cryptoProperties properties to define encryption
     * @return encrypted message
     */
    String encrypt(String plaintext, CryptoProperties cryptoProperties);
    /**
     * @param payload          encrypted message received
     * @param cryptoProperties properties to define encryption
     * @return decrypted message
     */
    String decrypt(String payload, CryptoProperties cryptoProperties);
}
Crypto Configuration
The connector library provides two implementations of the Crypto interface.
These are:
NoopCrypto
This is a plaintext passthrough class which will log the data at DEBUG level.
It is generally used for testing.
NoopCrypto requires no configuration.
SymmetricCrypto
Requires a key for encryption and decryption which is then used by the encryption methods.
It is assumed that the key password is the same as the keystore password.
The SymmetricCrypto class requires some configuration in order to function.
The following properties should be provided when building an instance of SymmetricCrypto.
Key
Description
keystorePath
The absolute path to the keystore
keystoreType
Keystore type such as PKCS12
keystorePassword
Password used for the keystore
transformation
Cipher transformation to be used, e.g. "AES/CBC/NoPadding" (See the Javadocs)
Using the SymmetricCrypto builder we can instantiate an instance.
SymmetricCrypto symmetricCrypto = SymmetricCrypto.builder()
        .withKeystorePath(keystorePath)
        .withKeystoreType("PKCS12")
        .withKeystorePassword("keystore-password")
        .withTransformation("AES/CBC/PKCS5Padding")
        .build();
Usage
Both send and receive connectors have a crypto field, which allows the developer to provide a Crypto implementation, or not if it is not required.
CryptoHeaders are required to ensure a key is passed and that encryption can be switched on or off at a message level.
Send Connector Example
Crypto crypto = getSymmetricCrypto(); (1)
SendConnector
        .<ExampleType, ExampleType>builder(connectorName)
        .withActorSystem(actorSystem)
        .withConnectorTransport(connectorTransport)
        .withSendTransportMessageConverter(messageConverter)
        .withCorrelationIdExtractor(correlationIdExtractor)
        .withCorrelationService(correlationService)
        .withCrypto(crypto) (2)
        .build();
1
Gets a Crypto implementation (symmetric in this example)
2
Provides the Crypto implementation.
Receive Connector Example
Crypto crypto = new NoopCrypto(); (1)
ReceiveConnector
        .<ExampleType>builder(connectorName)
        .withActorSystem(actorSystem)
        .withConnectorTransport(connectorTransport)
        .withReceiveTransportMessageConverter(messageConverter)
        .withReceiveHandler(receiver)
        .withMessageLogger(messageLogger)
        .withProcessingContextExtractor(processingContextExtractor)
        .withCrypto(crypto) (2)
        .build();
1
Instantiates the Crypto implementation (no-operation in this example)
2
Provides the Crypto implementation.
Crypto Message Headers
When using Crypto with connectors, messages need to be sent with require the following headers.
If they are not provided then the defaults values will be used instead.
Key
Description
Default
keyAlias
The key alias being used
empty string
encryptionScheme
The encryption scheme in use.
Can be set to NOPS to disable encryption
AES
The CryptoHeader enum is included in the API.
public enum CryptoHeaders {
    KEY_ID("keyAlias"),
    SCHEME("encryptionScheme");
    private final String headerName;
    CryptoHeaders(String headerName) {
        this.headerName = headerName;
    }
}
Putting this all together we can create a method to instantiate the required message headers.
private MessageHeaders messageHeaders() {
    return new MessageHeaders()
            .putHeader(CryptoHeaders.KEY_ID.getHeaderName(), "GWPAY01")
            .putHeader(CryptoHeaders.SCHEME.getHeaderName(), "AES")
            .putHeader("msgKey", "value1");
}
Then set them on the transport message, alongside the payload in the SendTransportMessageConverter functional interface that can be declared with using a lambda.
private SendTransportMessageConverter<ExampleType> sendTransportMessageConverter() {
    return payload -> new TransportMessage(
            messageHeaders(), (1)
            serialisePayload(payload)
    );
}
The SendTransportMessageConverter can be used when building a sending connector and all messages sent through it will have the CryptoHeaders set.
This simple example demonstrates how to set the headers statically, however they can be set dynamically based off of the payload if this is required with minimal additional effort.
Message Throttling
Message Validation
------------------------------
Message Validation
When working with messages sent between separate machines, even when working against a specific schema, there can often be issues with the validity of the messages.
These are often due to bugs, data entry errors or schema version mismatches.
Both sending and receiving connectors can be optionally configured to validate messages before sending or after receiving.
Send Connectors
To add validation to a sending connector, an implementation of the Validator interface must be provided when building the connector.
Validator.java
public interface Validator {
    ValidationReport validate(TransportMessage transportMessage);
}
The connector library provides two validators, one for JSON Schema and another for XML Schema.
Validation is performed against a TransportMessage before it is sent to the transport.
XML Schema Validation
The XML validation implementation of Validator is XmlSchemaValidator.
It takes an InputStream representing the schema to use against which all messages passing through this validator will be validated.
String xmlSchema = "/validation/sample-schema.xsd";
InputStream xmlSchemaStream = getClass().getResourceAsStream(xmlSchema);
XmlSchemaValidator xmlValidator = new XmlSchemaValidator(xmlSchemaStream);
SendConnector<ExampleType, ExampleType> sendConnectorWithXmlValidator
        = new SendConnector.Builder<ExampleType, ExampleType>(connectorName)
        .withActorSystem(actorSystem)
        .withConnectorTransport(transport)
        .withCorrelationIdExtractor(correlationIdExtractor)
        .withValidator(xmlValidator) (1)
        .build();
1
Adds the xml schema validator to the connector
In the above example, we are loading an XML Schema document from the classpath as an input stream.
This is the most common usage pattern for XmlSchemaValidator.
It is possible to set up using other types of input stream, such as a ByteArrayInputStream for loading the schema from a string.
The XML validator implements external entity injection (XXE) protection according to the  OWASP cheat sheet on the topic.
JSON Schema Validation
The JSON validation implementation of Validator is JsonSchemaValidator.
It is configured similarly to the XML validator, where we must pass an InputStream representing the schema to validate messages against.
String jsonSchema = "/com/github/fge/jsonschema/examples/fstab.json";
InputStream jsonSchemaStream = getClass().getResourceAsStream(jsonSchema);
JsonSchemaValidator jsonValidator = new JsonSchemaValidator(jsonSchemaStream);
SendConnector<ExampleType, ExampleType> sendConnectorWithJsonValidator
        = new SendConnector.Builder<ExampleType, ExampleType>(connectorName)
        .withActorSystem(actorSystem)
        .withConnectorTransport(transport)
        .withCorrelationIdExtractor(correlationIdExtractor)
        .withValidator(jsonValidator) (1)
        .build();
1
Adds the json schema validator to the connector
The default JSON schema version is draft-04 which is the de facto standard.
Receive Connectors
To add validation to a sending connector, an implementation of the BeanValidator interface must be provided when building the connector.
BeanValidator.java
public interface BeanValidator<T> {
    BeanValidationReport validate(T message);
    BeanValidator<T> withConnectorName(String connectorName);
}
The connector library provides a validator for bean validation using the Hibernate validator implementation.
Validation is performed against the message once it has been transformed from a TransportMessage to the target type by the provided ReceiveTransportMessageConverter function.
Bean Validator
Messages are expected to be annotated with javax.validation annotations if they are to be validated with the bean validator implementation.
Bean validation is available with the BeanValidatorImpl.
It optionally takes a javax.validation.Validator representing the validator to use against which all messages passing through this validator will be validated.
If no validator is passed in, then a default validator is created.
public BeanValidatorImpl() {
    this(Validation.buildDefaultValidatorFactory().getValidator());
}
public BeanValidatorImpl(javax.validation.Validator validator) {
    this(null, validator);
}
BeanValidatorImpl(String connectorName, javax.validation.Validator validator) {
    this.connectorName = connectorName;
    this.validator = validator;
}
Validation is optional and can be enabled by providing an implementation of BeanValidator when building the connector.
receiveConnector = initiatingReceiveConnectorBuilder()
        .withConnectorTransport(connectorTransport)
        .withEventBus(eventBus)
        .withBeanValidator(new BeanValidatorImpl<>()) (1)
        .build();
1
Adds the bean validator to the connector
Message Encryption
Message Association
------------------------------
Message Association
In the context of connectors, association is a hypernym (umbrella term) for correlation and identification.
Correlation is used to relate a response to a message sent asynchronously by another process, whereas identification is used to create an identity for new message which has no relation to any previous messages sent before.
Correlation
Correlation Identifier Extraction
When sending or receiving a message that requires correlation an implementation of CorrelationIdExtractor or ConnectorMessageCorrelationIdExtractor must be provided.
These are both functional interfaces, where the CorrelationIdExtractor can extract from the target message type and the ConnectorMessageCorrelationIdExtractor has access to headers passed from the transport layer which could be used to store the correlation identifier.
In both cases we return an instance of CorrelationId that is used to either persist or fetch the context.
@FunctionalInterface
public interface ConnectorMessageCorrelationIdExtractor<T> {
    CorrelationId extract(ConnectorMessage<T> connectorMessage);
}
@FunctionalInterface
public interface CorrelationIdExtractor<T> {
    CorrelationId extract(T payload);
    static <T> ConnectorMessageCorrelationIdExtractor<T>
    forConnectorMessage(CorrelationIdExtractor<T> correlationIdExtractor) {
        return connectorMessage ->
                correlationIdExtractor.extract(connectorMessage.getTargetMessage());
    }
}
Send Correlation
Messages sent using a sending connector must pass a ProcessingContext and can optionally pass a SupportingContext too, see the Send Connector documentation for more details around this.
Both these context objects will be persisted via an implementation of CorrelationService, whose purpose is store the context into a datastore against a correlation identifier.
The default datastore implementation used for the correlation service is MongoDB, though this can be swapped out with any other repository, e.g. Redis, PostgreSQL.
Receive Correlation
When messages are received in response to another message sent in a separate process, provided the receiving connector has been configured with a correlation extractor and a correlation service, correlation will be performed.
The correlation identifier will be extracted from the received message and is used to retrieve the context which was persisted during the send request.
If the context is found it is passed through to later processing stages, otherwise an exception is thrown and passed to error handlers.
Identification
Identification is used for initiating requests.
These requests are not responses to messages sent previously, so they need to generate a ProcessingContext to be used going forward, since there is no context to fetch from the correlation service.
In some cases the message might be correlated in some way to previous messages and the generated ProcessingContext may need to reflect this.
Setting up identification for a receiving connector only requires an implementation of ProcessingContextExtractor.
This makes identification flexible as the context can be either generated randomly or be derived from the received message.
@FunctionalInterface
public interface ProcessingContextExtractor<T> {
    ProcessingContext extract(ConnectorMessage<T> connectorMessage);
}
Helper Class
The InitiatingProcessingContextExtractor is an implementation of ProcessingContextExtractor which aims to reduce boilerplate, especially in the case where want to either fully or partially generate random values for processing context fields.
For the simplest case the generateRandom static factory method can be used.
Regardless of the input it will generate a ProcessingContext with a randomly generated UnitOfWorkId; and unknown ProcessingEntity and ClientRequestId.
ProcessingContextExtractor<TestObject> extractor = InitiatingProcessingContextExtractor.generateRandom();
Alternatively, the extractor can be constructed using the builder pattern and individual extractors for each field can be supplied, otherwise the defaults will be used.
var extractor = InitiatingProcessingContextExtractor.<TestObject>builder()
        .unitOfWorkIdExtractor(message -> UnitOfWorkId.of(message.getTargetMessage().unitOfWorkId))
        .clientRequestIdExtractor(message -> ClientRequestId.of(message.getTargetMessage().clientRequestId))
        .processingEntityExtractor(message -> ProcessingEntity.of(message.getTargetMessage().processingEntity))
        .build();
Since it is often the case that the ProcessingEntity does not change between request and remains static, there is static factory method defined to supply a static ProcessingEntity to cut down on the boilerplate code.
private static final ProcessingEntity PROCESSING_ENTITY = ProcessingEntity.of("processingEntity");
var extractor = InitiatingProcessingContextExtractor.<TestObject>builder()
        .processingEntityExtractor(staticSupplier(PROCESSING_ENTITY))
        .build();
Message Validation
Message Logging
------------------------------
Message Logging
In many applications there are often requirements around keeping a record of every message, this could be for monitoring, auditing, or data warehousing purposes.
Regardless of the use case, the connector library provides an optional message logging facility that will publish both sent and received messages.
Logged messages are published using the MessageLogEntry class defined in the message-logger-api.
Message Logger Interface
To add message logging, an implementation of the MessageLogger functional interface must be provided when building the connector.
The provided implementation can be more complex than demonstrated here and may publish to a database or a queue.
MessageLogger messageLogger = messageLogEntry -> {
    log.info("logging message: {}", messageLogEntry);
};
initiatingReceiveConnectorBuilder()
        .withMessageLogger(messageLogger);
Supporting context
The MessageLogEntryEnricher class contains an element called supportingData.  This has a key of type String and a value of type Object, and gets populated with a combination of header elements that were added to the TransportMessage and then any passed supportingData, in that order.
Any key clashes between the two maps will be resolved by adding header details as they are, and clashing supportingData elements will get a supportingData- prefix, i.e. supportingData-kafka-key.
Message Log Entry Enrichment
MessagesLogEntry objects can optionally be enriched before they are published by the MessageLogger.
public interface MessageLogEntryEnricher<T> {
    void enrich(ConnectorMessage<T> connectorMessage, MessageLogEntry messageLogEntry);
}
To add message log entry enrichment, provide an implementation of the MessageLogEntryEnrichment functional interface when building the connector.
MessageLogEntryEnricher<ExampleType> messageLogEntryEnricher = (connectorMessage, messageLogEntry)
        -> messageLogEntry.setSupportingData(Map.of("enriched", "true"));
initiatingReceiveConnectorBuilder()
        .withMessageLogger(messageLogger)
        .withMessageLogEntryEnricher(messageLogEntryEnricher);
Message Association
Error Handling
------------------------------
Error Handling
Errors are an inevitable consequence when processing messages from external systems over unreliable networks.
The connector library attempts to mitigate these with error handling strategies, which differ depending on the nature and context of the error in question.
The main distinguishing factor on how errors are handled is whether the error occurs whilst sending or receiving.
Error Handling in Send Connectors
Error handling in the send connectors is mostly focused around handling cases where a message fails to be delivered over the transport.
This class of error is transient in nature.
To overcome these errors the send connector utilises the resilience4j library to wrap calls with circuit breaking, fallback/routing and retries, all of which can be configured.
Refer to the Resilient Message Sending documentation for a more detailed explanation of how this works.
If a message fails to send, despite retries and other resilience strategies, then the message delivery completes exceptionally and the sender must handle the exception themselves as there is nothing more the connector can do without knowing about the client application.
Error Handing in Receive Connectors
Exceptions can occur during receipt of a message, such as failing to deserialize a TransportMessage to the target type for example.
All failed messages are appended to a configurable deadletter, this way the failed message can be acknowledged and processing can continue.
Deadletter Appender
The DeadletterAppender is a functional interface which is called whenever a message fails during processing.
@FunctionalInterface
public interface DeadletterAppender {
    CompletableFuture<Void> append(ReceiveConnectorException receiveConnectorException);
}
Providing a DeadletterAppender implementation is optional and if one is not provided the connector will use the default implementation.
The default behaviour is to simply log both the failed message and the exception that caused the error.
All failed messages will be provided as an exception that extends ReceiveConnectorException.
ReceiveConnectorException wraps the original exception as the cause alongside the received message.
public class ReceiveConnectorException extends RuntimeException {
    private final ReceivedMessage<? extends ConnectorMessage<?>> receivedMessage;
    public ReceiveConnectorException(ReceivedMessage<? extends ConnectorMessage<?>> receivedMessage, Throwable e) {
        super(e);
        this.receivedMessage = receivedMessage;
    }
    public ReceiveConnectorException(ReceivedMessage<? extends ConnectorMessage<?>> receivedMessage, String message) {
        super(message);
        this.receivedMessage = receivedMessage;
    }
}
Exception Classification
There are some exceptions that occur due to transient failures and for these it may make sense to attempt to recover from these.
However, since the majority of connector functionality is built on passing immutable objects through functions, we can avoid attempting to retry failing operations and move on as fast as possible since the result should never change.
To account for both of these situations, we classify exceptions as recoverable or unrecoverable.
Exceptions can be classified as unrecoverable by extending UnrecoverableReceiveConnectorException, a marker class which itself extends the base exception class ReceiveConnectorException.
Otherwise, for exceptions which are recoverable, extend ReceiveConnectorException and add the exception to the recoverable-exceptions key in the configuration.
recoverable-exceptions is a list of RecoverableExceptionFilter objects consisting of the following:
When an exception is thrown the entire stack trace is scanned for Recoverable exceptions and if found, then the exception is classified as Recoverable.
key
definition
example
filter
Regular expression to match against the fully qualified exception name e.g. HttpErrors$HttpServerErrorException
".*\\$HttpServerErrorException"
properties
List of key-value pairs, all of which must be matched in addition to the filter for the exception to be classified as a recoverable exception.
[{"statusCode": "503 Service Unavailable"}]
The default configuration is included below:
recoverable-exceptions = [
  {
   filter = ".*\\$HttpServerErrorException"
   properties = [{"statusCode": "503 Service Unavailable"}]
  },
  {
   filter = ".*\\.ReceiveConnectorException"
  }
]
Acknowledgements
Depending on the transport implementation, acknowledgments may need to be made by the receiving connector.
After handling an error the receiving connector will call the acknowledge method on the transport so that processing can continue.
Error Handlers
For each stage we can optionally provide a function which takes a ReceiveConnectorException and returns a CompletionStage<Void>.
    public DefaultMapToConnectorMessageStage(ReceiveTransportMessageConverter<T> transportMessageConverter,
                                             Integer parallelism,
                                             ExecutionContextExecutor dispatcher,
                                             ReceiveErrorHandler doOnError) {
        if (transportMessageConverter == null) {
            throw new IllegalArgumentException("'transportMessageConverter' must not be null");
        }
        if (dispatcher == null) {
            throw new IllegalArgumentException("'dispatcher' must not be null");
        }
        this.transportMessageConverter = transportMessageConverter;
        this.parallelism = Objects.requireNonNullElseGet(parallelism, () -> {
            final int processors = Runtime.getRuntime().availableProcessors();
            log.debug("Using default parallelism of {}", processors);
            return processors;
        });
        this.dispatcher = dispatcher;
        this.doOnError = doOnError;
    }
This makes it easy to add error handling specific to each stage.
We can also provide the doOnError parameter to the ReceiveConnector, which will be called on every ReceiveConnectorException in addition to the stage error handler.
Message Logging
Resilience
------------------------------
Resilience
Messaging is always a fraught process since networks and remote systems are not up 100% of the time.
The connector library incorporates a number of strategies to cope with transient failures in external systems.
The included strategies are, retries, circuit breaking and re-routing.
As with other connector features, resilience is configurable; so as much, or as little of it can be used, dependent on the situation.
Retry
One of the simplest resilience strategies is to retry the failed operation.
This can resolve transient failures that may have caused a previous message send to fail or timeout.
Retries can be configured for both sending and receiving messages.
For sending the retry decorates the message send over the transport.
When receiving messages the client supplied receiver function is decorated with the retry.
Retry Parameters
Where retries can get more complex is in determining what circumstances a retry is performed and when to give up.
The answer to these questions are contextual and as such it has been made highly configurable so that it can fit the needs of most use cases.
To configure resilience settings as a whole the data class ResilienceSettings should be provided when building a connector.
Below is table of all the resilience setting parameters that relate specifically to the retry mechanism.
Parameter
Type
Description
Default
maxAttempts
Integer
Determines the maximum number of retries to be made.
Note that this includes the first failed attempt.
1
initialRetryWaitDuration
Duration
How long to wait before retrying.
This sets the initial duration and may increase on successive retry attempts due to the backoff multiplier.
1 second
backoffMultiplier
Integer
Each successive retry will wait the previous wait duration multiplied by the backoff multiplier.
2
retrySchedulerThreadPoolSize
Integer
Determines how many threads the retry scheduler can use.
Equal to number of CPUs available
retryOnFailureWhen
Predicate<Throwable>
Given an exception thrown by the decorated code, returns a boolean to determine whether to retry.
This is to avoid retrying on exceptions where multiple attempts will not resolve the failure.
Returns true for all exceptions
retryOnResultWhen
Predicate<Object>
Given a successful result, i.e. no exception was thrown, returns a boolean to determine whether to retry.
This is used to trigger retries based on the object returned from the decorated code in a receiving connector.
Returns false for all results
retryOnSendResultWhen
Predicate<Object>
Given a successful result, i.e. no exception was thrown, returns a boolean to determine whether to retry.
This is used to trigger retries based on the object returned from the decorated code in a sending connector.
An example of where this may be useful is if a http server responds with a successful http response code to a request, but the response body contains error details that indicates a retry may be necessary.
Returns true when the returned DeliveryOutcome is a failure and retryOnFailureWhen returns true for the exception that caused the failure
Circuit Breaking
Electronics Analogy
Circuit breakers are based on the same named term in electronics.
In electronics a circuit breaker is effectively a resettable fuse which cuts the power whenever dangerous amounts of current pass through.
The circuit breaker keeps the power off until the problem is resolved at which point it can be reset.
Circuit Breakers in Connectors
Circuit breakers used in messaging system, such as the connector library, work in very much the same way.
Their purpose is to protect systems from being bombarded with messages while they attempt to recover.
If a connector transport is failing for an extended period of time, the circuit breaker may deem it unhealthy and will prevent any messages from being sent to it.
In circuit breaking terms this is referred to as an OPEN circuit, i.e. rejects messages fast, without sending anything to the downstream system.
This gives the remote system some time to come back, and also prevents the sender from wasting time trying to send messages that will probably fail.
After some time passes, the circuit breaker may begin to allow some messages to be sent through to the transport.
The circuit breaker is now considered to be HALF OPEN.
If the transport continues to fail, then the circuit breaker goes back to OPEN and the circuit breaker will wait again before attempting to close again.
While in the HALF OPEN state; if messages are responded to successfully, then the circuit breaker will change state to CLOSED and will begin to function as normal.
Circuit Breaker Parameters
Below is table of all the resilience setting parameters that relate specifically to the circuit breaker mechanism.
Parameter
Type
Description
Default
minimumNumberOfCalls
Integer
Determines the minimum number of calls (within a sliding window period) that need to be made before the circuit breaker can calculate the error rate to determine the transport health.
1
resetTimeout
Duration
How long to wait while in the OPEN state before transitioning to HALF_OPEN and attempting to close the circuit.
1 second
Routing Logic
Routing logic is another resilience mechanism specific to sending connectors.
Sending connectors can be configured to send using one or more transports and routing logic determines the strategy used to select the transport to use when sending a message.
Routing logic ties in quite nicely with circuit breaking and typically the strategies will avoid selecting transports whose circuit breaker is not closed.
To configure the routing strategy, an implementation of the RoutingLogic interface must be provided when building a sending connector.
RoutingLogic.java
@FunctionalInterface
public interface RoutingLogic<T> {
    CircuitBreakerTransport<T> select(List<CircuitBreakerTransport<T>> transports);
}
The connector library provides three RoutingLogic implementations, these are Failover, Round Robin  and Weighted Round Robin.
If no RoutingLogic is provided then the round-robin strategy will be used by default.
Failover
The failover strategy is used with two transports, where one is the primary and the other a failover.
The failover transport is selected when the primary transport is failing, i.e. has a non-closed circuit breaker.
Round Robin
The round-robin strategy can be used with 1 or more transports, where each message sent will use the next transport, effectively load balancing equally across transports.
Weighted Round Robin
Weighted round-robin is similar to the standard round-robin strategy.
The main difference is that each transport is given a weighting that will cause more or less messages to be sent to it relative to the others.
Error Handling
Metrics
------------------------------
Metrics
The connector library exposes a series of metrics which can be used to monitor the health of a particular connector.
By default, metrics are exposed using Prometheus.
The default location where connector metrics are hosted is localhost:9001. If statistics are not displaying, then ensure that the Lightbend Telemetry agent is running.
For more information check out the Cinnamon Agent Documentation.
Connector Metrics Breakdown
Each connector has its own set of metrics in the table below.
Every metric is tagged with the relevant connector’s name.
To distinguish which metrics relate to which connector, use the connector hint in the Prometheus exporter.
Metric name
Type
Description
Source
application_ipf_requests_sent
Counter
Logs the number of messages sent through a sending connector
Akka Cinnamon (default TCP/9001)
application_ipf_requests_received
Counter
Logs the number of messages received through a receiving connector
Akka Cinnamon (default TCP/9001)
application_ipf_correlations_saved
Counter
The number of correlation records persisted by the correlation service
Akka Cinnamon (default TCP/9001)
application_ipf_correlations_found
Counter
The number of correlation records fetched from the correlation service
Akka Cinnamon (default TCP/9001)
application_ipf_failed_correlations
Histogram/Recorder
The number of failed correlation lookups from the correlation service
Akka Cinnamon (default TCP/9001)
application_ipf_failed_requests
Counter
The number of failed message requests through a sending connector
Akka Cinnamon (default TCP/9001)
application_ipf_failed_receives
Counter
The number of failed message receives through a receiving connector
Akka Cinnamon (default TCP/9001)
application_ipf_response_times
Histogram/Recorder
A complex Recorder type which records min/max/average and percentiles for response times for this connector
Akka Cinnamon (default TCP/9001)
WARNING: These metrics are based on the  CorrelationFound system event, so it only applies for async request reply operations where we found the correlation, so it’s per-connector and only for connectors that participate in a request-reply session by using the correlation ID service
Circuit Breaker Metrics
Circuit breaker metrics use MeterRegistry implementations to publish metrics. This can be set at a per connector level through ResiliencySettings:
ResiliencySettings.builder()
    .initialRetryWaitDuration(Duration.ofMillis(10))
    .maxAttempts(1)
    .resetTimeout(Duration.ofSeconds(1))
    .meterRegistry(SIMPLE_METER_REGISTRY)
    .build()
Alternatively this can be set through the SendingConnector interface. This allows setting the meterRegistry implementation at an application level.
For example, if we want to set all the sendingConnectors to have the same meterRegistry implementation we can loop through and set them with the API after we have already built the connectors. This allows us to quickly switch implementations without the need to change each connector definition.
Application Metrics
Aggregated application level metrics are also exposed with the same mechanism
Metric name
Type
Description
Event information
Source
ipf_behaviour_end_to_end_latency_seconds_bucket
Histogram
Time in seconds that a flow execution takes
behaviour - IPF flow name, event - time taken from start of flow until this event, type - one of the follow CSM_STATES_ONLY, NO_CSM_STATES or FULL_FLOW
Spring Actuator (TCP/8080)
ipf_behaviour_end_to_end_latency_seconds_count
Counter
Total number of invocations
same tags as used in ipf_behaviour_end_to_end_latency_seconds_bucket
Spring Actuator (TCP/8080)
ipf_behaviour_end_to_end_latency_seconds_sum
Counter
Total time spent by all invocations
same tags as used in ipf_behaviour_end_to_end_latency_seconds_bucket
Spring Actuator (TCP/8080)
ipf_behaviour_per_state_latency_seconds_bucket
Histogram
Time spent by flows in that state
behaviour - Tge flow name, status - status
Spring Actuator (TCP/8080)
ipf_behaviour_per_state_latency_seconds_count
Counter
Total number of calls to that state
behaviour - The flow name, status - status
Spring Actuator (TCP/8080)
ipf_behaviour_per_state_latency_seconds_sum
Counter
Time spent by all calls to that state
behaviour - The flow name, status - status
Spring Actuator (TCP/8080)
flow_started_total
Counter
Number of transactions have been created (started)
behaviour - The flow name, description - same as behaviour
Spring Actuator (TCP/8080)
flow_finished_total
Counter
Number of transactions that have reached a terminal (final) state
behaviour - The flow name, description - same as behaviour, reasonCode - reasonCode used in final state, state - name of the terminal state
Spring Actuator (TCP/8080)
state_timeout_total
Counter
Number of flows that have raised a timeout system event
behaviour - The flow name, description - state that timed out
Spring Actuator (TCP/8080)
action_invoked_total
Counter
Raised when the IPF domain invokes an action on an external system
behaviour - The flow name, description - called action name
Spring Actuator (TCP/8080)
action_timeout_total
Counter
Raised when an action invoked by IPF has not received a reply within the configured timeout
behaviour - The flow name, description - called action name
Spring Actuator (TCP/8080)
domain_event_persisted_total
Counter
Raised when a Domain Event has been successfully persisted
behaviour - The flow name, description - event name
Spring Actuator (TCP/8080)
unexpected_command_total
Counter
Raised when the IPF domain receives a command that cannot be handled in the current state of the aggregate
command_name - The command name, status - status of the flow when command recieved
Spring Actuator (TCP/8080)
ipf_processing_data_journal_latency_seconds_bucket
Histogram
which records the duration between the time a domain event has been created and the time it has been sent to ODS; the durations will be sensitive to time skew between the servers, so they should be treated as estimates only
Spring Actuator (TCP/8080)
ipf_processing_data_journal_latency_seconds_count
Counter
Count of the total number of domain events sent to ODS
ipf_processing_data_journal_latency_seconds_sum
Type
Usage
CSM_STATES_ONLY
Records only the time spent in CSM-related states (e.g clearing and settling, requesting status etc)
NO_CSM_STATES
Records the time between the first and last event in the flow, minus the time spent
 in CSM-related states (e.g clearing and settling, requesting status etc)
FULL_FLOW
Records the time between the first and last event in the flow
Grafana
There are Grafana dashboards available as part of the standard IPF Grafana setup which graphs these data points meaningfully to the user.
Worked Example of Using PromQL to Generate a Custom Graph
Requirement
A pie-chart to represent all the CSM completion states of all transactions that happened in this calendar day that took less than the SLA of 5 seconds.
— Business Analyst
Step 1 - Choice of Metric
First you should find the closest metric that would provide the data required, in this case the ipf_behaviour_end_to_end_latency_seconds histogram data should work.
sum by(event) (
 increase(
  ipf_behaviour_end_to_end_latency_seconds_bucket{type="CSM_STATES_ONLY", service=~".*creditor.*", le="5.0"}[15m]
 )
)
This query will return the number of states completed in 15 minute interval grouped by event
increase - calculates the increase in the time series in the range vector
sum - calculates the total in the time series
Further PromQL functions that can be used can be found here.
Step 2 - Grouping into Days
Now you can create a daily today with the following query
last_over_time(
 sum by(event) (
  increase(
   ipf_behaviour_end_to_end_latency_seconds_bucket{type="CSM_STATES_ONLY", service=~".*creditor.*", le="5.0"}[1d] offset -1d
   )
  )
 [1d:1d]
)
last_over_time - the most recent point value in specified interval
Step 3 - Limit to a Single Day
Alter the panel’s Query options to add a relative time of now/d
Step 4 - Change the Graph Style
Using the Visualization picker on the right hand side, choose the Pie Chart option, and alter the Title to Calls to Scheme. You will also be able to add the following changes:
Add Piechart labels of Name
Change Legend mode to Table, set placement to Right and add Label Values of Percent and Value
You can now save the dashboard which will then look something like
Resilience
Getting Started
------------------------------
Getting Started
Getting started guides for getting connectors up and running in as few steps as possible.
These can be augmented with additional features provided by the connector library to fit the specific needs of a particular use case.
Connector Operations Quickstart
Receiving Connector Quickstart
Sending Connector Quickstart
Connector Configuration
Metrics
Connector Operations Quickstart
------------------------------
Connector Operations Quickstart
This page provides details on how to get started with the connector operations API.
OperableConnector Interface
Before getting started, ensure the connectors you want to operate implement the OperableConnector interface.
Without this, the API cannot perform operations on them.
public interface OperableConnector {
    /**
     * Retrieve name of the connector.
     */
    String getName();
    /**
     * Starts the connector.
     */
    void start();
    /**
     * Starts the connector's health check procedure.
     */
    void startHealthCheck();
    ConnectorHealth getHealth();
    /**
     * Shuts down the connector.
     * @param reason the reason for shutdown
     */
    CompletionStage<Void> shutdown(ShutdownReason reason);
    /**
     * Returns the connectors running status
     */
    boolean isRunning();
}
Getting Started
To register the controller simply include the following maven dependency.
Spring autoconfiguration will do the rest.
 <dependency>
    <groupId>com.iconsolutions.ipf.core.connector</groupId>
    <artifactId>connector-operations-api</artifactId>
    <version>${connector-operations-api.version}</version>
</dependency>
The latest version of the connector operations library can be found using this Nexus search.
Auditing
This section goes over how to set up audit logging on connector operator endpoints.
Prerequisites
Auditing requests to the connector operations API assumes that the Spring security context has been configured and only authenticated users can make requests to protected resources.
Without an authenticated user it is difficult to audit who made the request.
Reactive Web Setup
Auditing is implemented with the Spring framework’s reactive web stack.
An auditing filter stage can be added as part of request handling by implementing the WebFilter interface and annotating it with @Component.
An example of this is shown below.
@Slf4j
@Component
public class AuditLogFilter implements WebFilter {
    private final ServerWebExchangeMatcher matcher;
    public AuditLogFilter() {
        webExchangeMatcher = ServerWebExchangeMatchers.pathMatchers("/connectors/**"); (1)
    }
    @Override
    public Mono<Void> filter(ServerWebExchange exchange, WebFilterChain chain) {
        ServerHttpRequest request = exchange.getRequest();
        return webExchangeMatcher.matches(exchange)
                .filter(ServerWebExchangeMatcher.MatchResult::isMatch)
                .flatMap(m -> exchange.getPrincipal()
                    .doOnNext(principal -> logPrincipal(request, principal))) (2)
                .then(chain.filter(exchange));
    }
    private void logPrincipal(ServerHttpRequest request, Principal principal) { (3)
        log.info("{} {} principal.name: {}",
            request.getMethod(),
            request.getPath(),
            principal.getName());
    }
}
1
The web exchange filter allows us to only log on specific endpoints.
2
The principal will only be logged if it is set on the exchange, otherwise nothing will happen.
3
The logging can be customized, in this example it logs the request method and path with the principal’s name.
Blocking Web Setup
Traditional blocking web servers need to be configured differently.
Fortunately this is also quite simple as Spring has implemented a Filter that can be used for logging requests.
This can also be configured to only work on specific endpoints if required.
The code snippet below shows how this can be set up by registering a couple of beans in a configuration class.
@Configuration
public class AuditLogConfig {
    @Bean
    public FilterRegistrationBean<AbstractRequestLoggingFilter> (1)
    loggingFilterRegistration(AbstractRequestLoggingFilter requestLoggingFilter) {
        var registration = new FilterRegistrationBean<>(requestLoggingFilter);
        registration.addUrlPatterns("/connectors/*");
        return registration;
    }
    @Bean
    public AbstractRequestLoggingFilter requestLoggingFilter() { (2)
        CommonsRequestLoggingFilter loggingFilter = new CommonsRequestLoggingFilter();
        loggingFilter.setIncludeClientInfo(true);
        loggingFilter.setIncludeQueryString(true);
        loggingFilter.setIncludePayload(true);
        loggingFilter.setMaxPayloadLength(64000);
        return loggingFilter;
    }
}
1
The filter registration bean can be configured to only log requests matching the provided url patterns.
2
Here, the logging filter is configured.
A custom implementation of the AbstractRequestLoggingFilter could be used instead if more control over the behaviour is required.
Getting Started
Receiving Connector Quickstart
------------------------------
Receiving Connector Quickstart
This page provides details on how to get started with receiving messages from external systems, using receiving connectors provided by the connector library.
Dependencies
Before building a receiving connector, the connector-core library must be included as a dependency.
 <dependency>
    <groupId>com.iconsolutions.ipf.core</groupId>
    <artifactId>connector-core</artifactId>
    <version>${connector.version}</version>
</dependency>
The latest version of the connector library can be found using this Nexus search.
Unless providing your own implementation, at least one transport library should be declared.
The naming scheme for all transports included in the connector library is connector-[transport], where [transport] matches the transport scheme that this connector should use.
For more details on connector transports check out the Connector Transports documentation.
Here’s an example of declaring the dependency to use JMS.
<dependency>
    <groupId>com.iconsolutions.ipf.core</groupId>
    <artifactId>connector-jms</artifactId>
    <version>${connector.version}</version>
</dependency>
Getting Started: Receiving Connector
Receiving connectors are used for receiving messages, either as a response to a previously sent message or a new request.
Builder Pattern
Receiving connectors are instantiated using the builder pattern.
This is because connectors have many parameters to configure and most are optional or have default values.
Let’s see how we use the builder pattern to instantiate a receiving connector.
When building a receiving connector we set it up to be either an initiating receiver or a response receiver.
An initiating receiver receives requests from an external system, whereas a response receiver expects messages to be responses to requests made previously via a sending connector.
Initiating Receiver
The following example demonstrates the minimum properties that must be provided when building an initiating receive connector.
ReceiveConnector<ExampleType> connector = ReceiveConnector
        .<ExampleType>builder("ExampleSystem") (1)
        .withConnectorTransport(transport) (2)
        .withReceiveTransportMessageConverter(converter) (3)
        .withProcessingContextExtractor(processingContextExtractor) (4)
        .withReceiveHandler(receiver) (5)
        .withActorSystem(actorSystem) (6)
        .build();
1
Sets the name of the connector.
The name should represent what the connector is connecting to.
2
Provides an implementation of the ReceivingConnectorTransport interface.
3
Provides an implementation of the ReceiveTransportMessageConverter interface.
Takes the received TransportMessage and converts it to the target type T (ExampleType in this instance).
4
Provides an implementation of the ProcessingContextExtractor interface.
This field is what makes this an initiating receiving connector as it extracts (or generates) a ProcessingContext from the message instead of fetching one from the correlation service as would be the case in a response receiving connector.
5
An implementation of ReceiveHandler.
This is where application logic would go to decide how to handle requests.
6
Sets the actor system used throughout the application.
Response Receiver
The next example demonstrates how to build a minimal response receiving connector.
ReceiveConnector<ExampleType> connector = ReceiveConnector
        .<ExampleType>builder("connector-name") (1)
        .withConnectorTransport(transport) (2)
        .withReceiveTransportMessageConverter(converter) (3)
        .withCorrelationIdExtractor(correlationIdExtractor) (4)
        .withCorrelationService(correlationService) (5)
        .withReceiveHandler(receiver) (6)
        .withActorSystem(actorSystem) (7)
        .build();
1
Set the name of the connector.
The name should represent what the connector is connecting to.
2
Provides an implementation of the ReceivingConnectorTransport interface.
3
Provides an implementation of the ReceiveTransportMessageConverter interface.
Takes the received TransportMessage and converts it to the target type, ExampleType in this case.
4
Provides an implementation of the CorrelationIdExtractor interface.
Takes the received message and extracts the correlation identifier so that we can correlate it with the original request made via a sending connector.
5
Provides an implementation of the CorrelationService interface.
The correlation service takes the extracted correlation identifier and returns the associated ProcessingContext used when the original request was sent via a sending connector.
6
An implementation of ReceiveHandler.
This is where application logic would go to decide how to handle responses.
7
Sets the actor system used throughout the application.
Start Receiving Messages
The final step is to start the connector by calling its start method.
At this point you should have a connector that can receive messages via the configured transport.
Connector Builder Helper
In the previous sections we saw how to instantiate a receiving connector using the builder pattern.
One way to reduce some boilerplate and share configuration is to use the ReceiveConnectorBuilderHelper and a configuration file.
To create a connector builder with default configuration, the ReceiveConnectorBuilderHelper.defaultBuilder static method can be used.
ReceiveConnector<Object> connector = ReceiveConnectorBuilderHelper
        .defaultBuilder("ExampleSystem", actorSystem)
        .withConnectorTransport(connectorTransport)
        .withReceiveTransportMessageConverter(transportMessageConverter)
        .withProcessingContextExtractor(processingContextExtractor)
        .withReceiveHandler(receiver)
        .build();
For reference, the default configuration is.
default-receive-connector {
  manual-start: true
}
To change any of the configuration, create a .conf file and pass it to the ReceiveConnectorBuilderHelper.builder static method.
application.conf
example-receive-connector {
  manual-start = false
  throttle-count = 5
  throttle-duration = 10s
}
ReceiveConnector<Object> connector = ReceiveConnectorBuilderHelper.builder("our-receive-connector", "example-receive-connector", actorSystem)
        .withConnectorTransport(connectorTransport)
        .withReceiveTransportMessageConverter(t -> t)
        .withProcessingContextExtractor(processingContextExtractor)
        .withReceiveHandler(receiver)
        .build();
The values that can be configured via configuration properties are shown in the following table.
Property
Description
Example
manual-start
When set to 'false', connector is started automatically after creation
throttle-count
If the value is set, limits the throughput to a specified number of consumed messages per time unit.
If it is set.
If this value is set, throttle-duration also needs to be set.
10
throttle-duration
If set, it is used along with 'throttle-count' to set the maximum rate for consuming messages.
For more details, see doc.akka.io/japi/akka/2.6/akka/stream/javadsl/Flow.html#throttle(int,java.time.Duration)
1s
mapping-parallelism
If set, limits the number of concurrent mapping operations executed on consumed messages.
Defaults to the number of available processors.
receiver-parallelism
If set, limits the rate at which mapped messages are sent to behaviors for further processing.
Defaults to the number of available processors.
Connector Operations Quickstart
Sending Connector Quickstart
------------------------------
Sending Connector Quickstart
This page explains details on how to get started with sending messages to external systems, using sending connectors provided by creating the connector library.
Dependencies
Before building a sending connector, the connector-core library must be included as a dependency.
<dependency>
    <groupId>com.iconsolutions.ipf.core</groupId>
    <artifactId>connector-core</artifactId>
    <version>${connector.version}</version>
</dependency>
The latest version of the connector library can be found using this Nexus search.
Unless providing your own implementation, at least one transport library should be declared.
The naming scheme for all transports included in the connector library is connector-[transport], where [transport] matches the transport scheme that this connector should use.
For more details on connector transports check out the Connector Transports documentation.
Here’s an example of declaring the dependency to use JMS.
<dependency>
    <groupId>com.iconsolutions.ipf.core</groupId>
    <artifactId>connector-jms</artifactId>
    <version>${connector.version}</version>
</dependency>
Getting Started: Sending Connector
Sending connectors are used for sending messages to some destination, either as a reply to a previously received message or to make a request.
Builder Pattern
Sending connectors are instantiated using the builder pattern.
This is because connectors have many parameters to configure and most are optional or have default values.
Let’s see how we use the builder pattern to instantiate a sending connector.
SendConnector<ExampleType, ExampleType> connector = SendConnector
        .<ExampleType, ExampleType>builder("ExampleSystem") (1)
        .withConnectorTransport(transport) (2)
        .withSendTransportMessageConverter(converter)(3)
        .withCorrelationIdExtractor(correlationIdExtractor) (4)
        .withCorrelationService(correlationService) (5)
        .withActorSystem(actorSystem) (6)
        .build();
1
Sets the name of the connector.
The name should represent what the connector is connecting to.
2
Provides an implementation of the SendingConnectorTransport interface.
3
Provides an implementation of the SendTransportMessageConverter interface.
Takes the message payload of type T (ExampleType in this instance) and converts it to a TransportMessage object.
4
Provides an implementation of the CorrelationIdExtractor interface.
Takes the message payload and extracts (or generates) a correlation identifier to use when persisting the ProcessingContext via the correlation service.
5
Provides an implementation of the CorrelationService.
The correlation service takes a ProcessingContext and a CorrelationId and persists them using the services' configured repository.
This is used to correlate a request with its response which will be handled by a separate process.
6
Sets the actor system used throughout the application.
Start Sending Messages
At this point we should have successfully instantiated a SendConnector<ExampleType>, which can be used to send messages over the configured transport.
The SendingConnector interface defines two methods for sending messages, the signatures for these are.
public interface SendingConnector<D, R> extends OperableConnector {
    /**
     * Send a request via the connector without a supporting context
     */
    default CompletionStage<R> send(@NotNull ProcessingContext processingContext,
                                    @NotNull D domainMessage) {
        return send(processingContext, domainMessage, SupportingContext.empty());
    }
    /**
     * Send a request via the connector with a full sending context
     * that allows users to supply a supporting context as well
     */
    CompletionStage<R> send(@NotNull ProcessingContext processingContext,
                            @NotNull D domainMessage,
                            @NotNull SupportingContext supportingContext);
    /**
     * Allows setting of the meterRegistry implementation for a connector
     * after connector build
     * This allows implementations to set the registry for all SendingConnector instances
     * at once instead of having to set them at a per connector bases at build time
     */
    void setMeterRegistry(MeterRegistry meterRegistry);
}
The generic domain message type D in this instance is our ExampleType.
ExampleType has the following definition.
@Data
@AllArgsConstructor
public static class ExampleType {
    private final String name;
    private final LocalDate dob;
    private final int shoeSize;
    private final String context;
    public ExampleType(String name, LocalDate dob, int shoeSize) {
        this(name, dob, shoeSize, null);
    }
}
We can now create an instance of the domain message and send it to the transport via the connector.
ExampleType message = new ExampleType(
        "Isaac Newton",
        LocalDate.of(1642, 12, 25),
        8
);
CompletionStage<DeliveryOutcome> future = connector.send(processingContext, message);
This returns a  CompletionStage<DeliveryOutcome>
which we can either block to await the DeliveryOutcome (not ideal), or use various methods on CompletionStage
to perform other tasks asynchronously when the future completes.
Getting Started: Request-Reply Sending Connector
The default sending connector works in a one-way fashion where the response of the request is the delivery result.
However, for some protocols (namely HTTP) it is desirable and expected that a well-formed reply would be returned from a request.
A RequestReplySendConnector is built similarly to other connectors, using the builder pattern.
RequestReplySendConnector<ExampleRequest, ExampleRequest, ExampleResponse, ExampleResponse> connector;
connector = RequestReplySendConnector
        .<ExampleRequest, ExampleRequest, ExampleResponse, ExampleResponse>builder("ExampleSystem")
        .withConnectorTransport(transport)
        .withCorrelationIdExtractor(correlationIdExtractor)
        .withSendTransportMessageConverter(sendMessageConverter)
        .withReceiveTransportMessageConverter(receiveMessageConverter)
        .withActorSystem(actorSystem)
        .build();
This connector is slightly more complex as it requires converting to and from the TransportMessage.
However, some complexity is reduced because there is no longer any need for correlation related parameters.
The signature of the send method now returns a future containing the response.
Since the transport is unreliable, a fail-safe exists to ensure that the request eventually completes, albeit with an exception after some timeout duration.
A default is provided, but can be configured with the builder method,
.withCallTimeoutSeconds.
When a message send times out, the response future will complete exceptionally with a TimeoutException as the cause.
Connector Builder Helper
In the previous sections we saw how to instantiate sending connectors using the builder pattern.
One way to reduce some boilerplate and share configuration is to use the SendConnectorBuilderHelper and a configuration file.
To create a connector builder with default configuration, the SendConnectorBuilderHelper.defaultBuilder static method can be used.
SendConnector<Object, Object> connector = SendConnectorBuilderHelper
        .defaultBuilder("ExampleSystem", actorSystem)
        .withConnectorTransport(connectorTransport)
        .withDomainToTargetTypeConverter(domainToTargetTypeConverter)
        .withCorrelationIdExtractor(correlationIdExtractor)
        .withCorrelationService(correlationService)
        .withSendTransportMessageConverter(transportMessageConverter)
        .build();
For reference, the default configuration is.
default-send-connector {
  manual-start = false
  call-timeout = 30s
  queue-size = 50
  max-concurrent-offers = 500
  parallelism = 500
  resiliency-settings {
    minimum-number-of-calls = 1
    max-attempts = 1
    reset-timeout = 1s
    initial-retry-wait-duration = 1s
    backoff-multiplier = 2
  }
}
To change any of the configuration, create a .conf file and pass it to the SendConnectorBuilderHelper.builder static method:
application.conf
example-send-connector {
  parallelism = 1000
  throttle-count = 5
  queue-size = 100
  resiliency-settings {
    max-attempts = 10
  }
}
SendConnector<Object, Object> sendConnector = SendConnectorBuilderHelper
        .builder("send-connector", "example-send-connector", actorSystem)
        .withConnectorTransport(connectorTransport)
        .withDomainToTargetTypeConverter(domainToTargetTypeConverter)
        .withCorrelationIdExtractor(correlationIdExtractor)
        .withCorrelationService(correlationService)
        .withSendTransportMessageConverter(transportMessageConverter)
        .build();
The values that can be configured via configuration properties are shown in the following table.
Property
Description
Example
manual-start
When set to 'false', connector is started automatically after creation.
false
queue-size
Size of a source queue which can be used to handle backpressure, for example fast producer situations.
50
call-timeout
Maximum duration to wait for an acknowledgement before completing the returned future exceptionally with a TimeoutException.
30s
max-concurrent-offers
Maximum number of pending offers when buffer is full.
500
parallelism
Maximum number of parallel calls for send connector
500
throttle-count
Limits the throughput to a specified number of consumed messages per time unit.
When this value is set, throttle-duration must also be provided.
10
throttle-duration
Is used with 'throttle-count' to set the maximum rate for consuming messages.
For more details, see the Message Throttling documentation.
1s
resiliency-settings
The resiliency settings that will be used when sending.
For more details, see the Resilience documentation.
Receiving Connector Quickstart
Connector Configuration
------------------------------
Connector Configuration
The configuration for a Connector can be set in IPF configuration files. This section explains the various configuration options available for the various types of connector, their default values, and
the fallback mechanism which allows you to only supply a partial config and otherwise use the defaults documented below.
Configuration Options
Sending Connectors
The allowed configuration options for a sending connector (send connector & request-reply connectors) are:
Property Name
Description
Type
Default Value
manual-start
If true, the connector will not start on application startup but will require an invocation to it’s 'start' method.
boolean
false
call-timeout
The length of time the connector will wait until it timesout the call (in seconds)
Integer
30
queue-size
TBC
Integer
50
max-concurrent-offers
TBC
500
parallelism
TBC
500
send-message-association
Defines whether the connector should add (as headers) the processing context information of the outbound request
true
throttle-count
TBC
Integer
Not set
throttle-duration
TBC
Duration
Not set
resiliency-settings
TBC
ResiliencyConfig
Receiving Connectors
The allowed configuration for a receiving connector is:
Property Name
Description
Type
Default Value
manual-start
If true, the connector will not start on application startup but will require an invocation to it’s 'start' method.
boolean
true
mapping-parallelism
TBC
Integer
Not set
receiver-parallelism
TBC
Integer
Not set
throttle-count
TBC
Integer
Not set
throttle-duration
TBC
Duration
Not set
resiliency-settings
TBC
ResiliencyConfig
default resiliency config
Resiliency Settings
Both the sending and receiving connector config allows definition of the resiliency config with a 'resiliency-settings' block.  The options for this are:
Property Name
Description
Type
Default Value
enabled
Allows resiliency to be toggled on / off
boolean
true
minimum-number-of-calls
TBC
Integer
1
max-attempts
TBC
Integer
1
reset-timeout
TBC
Duration
1s
initial-retry-wait-duration
TBC
Duration
1s
backoff-multiplier
TBC
Integer
2
retry-on-failure-when
TBC
boolean
true
retry-on-result-when
TBC
boolean
false
Default Configuration
All connectors come with a pre-defined set of base configuration which will be used if not overriden at the point of construction of the connector.
The default configuration is provided below, and matches that defined in the previous section.
ipf.connector {
  default-resiliency-settings {
    enabled = true
    minimum-number-of-calls = 1
    max-attempts = 1
    reset-timeout = 1s
    initial-retry-wait-duration = 1s
    backoff-multiplier = 2
    retry-on-failure-when = true
    retry-on-result-when = false
  }
  # tag::default-receive-connector-config[]
  default-receive-connector {
    manual-start: false
    resiliency-settings = ${ipf.connector.default-resiliency-settings}
  }
  # end::default-receive-connector-config[]
  # tag::default-send-connector-config[]
  default-send-connector {
    manual-start = false
    call-timeout = 30s
    queue-size = 50
    max-concurrent-offers = 500
    parallelism = 500
    send-message-association = true
    resiliency-settings = ${ipf.connector.default-resiliency-settings}
  }
}
Using Configuration
When constructing a connector, it is possible to override these properties by providing a configRoot.  When provided, the connector will use this configRoot as the base of the properties to use.  So for example, if we construct a connector like:
var sendConnector = new SendConnector.Builder<String, String>("TestConnector")
    .withActorSystem(actorSystem) (1)
    .withConfigRoot("test-config") (2)
    .build();
Here we are supplying both:
1
the ActorSystem
2
the config root - a string value.
We are telling our connector that our config root here is test-config.  What this means is that where in our default connector settings, the send connector defaults are at ipf.connector.default-send-connector, here we will use test-config as the primary config and then ipf.connector.default-send-connector as the fallback.
To illustrate this, suppose we were to set the config file like:
test-config {
    parallelism = 700
    manual-start = false
}
Here we are defining new configuration values for parallelism and manual-start.  We could just as easily have provided the config values for the other properties. By only supplying these two, our connector will then have:
A parallelism of 700
A manual-start setting of false.
All other properties inherited from the default config, so for example the queue-size will be set to 50.
We can also supply overrides to the configuration settings in the constructor through our java code. So for example, consider the following setup:
var sendConnector = new SendConnector.Builder<String, String>("TestConnector")
    .withActorSystem(actorSystem)
    .withParallelism(700)
    .withManualStart(false)
    .build();
This would lead to the same connector being built as per our configuration implementation.
Sending Connector Quickstart
How to implement asynchronous request reply
------------------------------
Asynchronous Request-Reply
How do I correlate messages from my own internal format to some external domain, if responses are async?
This Connector example connects to a fake service which takes a person’s name and returns their age.
The service on the other end takes requests on a request queue, and sends responses back on a response queue with a correlation ID to help the consumer determine which request this response is for.
This guide is also available in the separate connector-samples Git repository
here.
Correlation
We use an in-memory correlation service to store mappings from our "internal" flow ID concept to the external "correlation ID" concept.
There might be multiple reasons why we cannot use our own internal ID as the correlation ID for an external service (hence the need for a correlation ID service).
I don’t want to publish my internal ID to the outside world.
Technical limitations, e.g. external system’s correlation IDs must follow a specific format.
There are multiple interactions with this external system for a single one of my internal ID, which would make correlation non-unique per individual invocation.
Connector Setup
We have set up a SendConnector and ReceiveConnector pair.
First the SendConnector.
        var sendConnector = new SendConnector
                .Builder<AgeRequest, TheirAgeRequest>("OurSender")
                .withActorSystem(actorSystem)
                .withCorrelationService(correlationService)
                .withConnectorTransport(sendingTransport)
                .withMessageLogger(logger())
                .withCorrelationIdExtractor(request -> CorrelationId.of(request.getCorrelationId())) (1)
                .withDomainToTargetTypeConverter(this::createTheirAgeRequest) (2)
                .withSendTransportMessageConverter(toJson()) (3)
                .build();
1
Telling the SendConnector where to look in the target message type for the correlation ID to save in the Correlation ID service.
2
Creating an external domain request with a random number as the external Correlation ID.
3
Converting the external domain POJO type to a JSON representation.
And here’s the ReceiveConnector.
        new ReceiveConnector
                .Builder<TheirAgeResponse>("OurReceiver")
                .withActorSystem(actorSystem)
                .withConnectorTransport(receivingTransport)
                .withMessageLogger(logger())
                .withCorrelationService(correlationService)
                .withCorrelationIdExtractor(response -> CorrelationId.of(response.getCorrelationId())) (1)
                .withReceiveTransportMessageConverter(fromJson(TheirAgeResponse.class)) (2)
                .withManualStart(false)
                .withReceiveHandler((receivingContext, response) -> {
                    var processingContext = receivingContext.getProcessingContext();
                    ageMap.put(processingContext.getUnitOfWorkId(), response.getAge()); (3)
                    return CompletableFuture.completedFuture(null);
                })
                .build();
1
Function to tell the ReceiveConnector where in the response message to look for the correlation ID.
2
Converting the TransportMessage from JSON to a TheirAgeResponse POJO.
3
Populating the age in the map for our flow ID.
Exercise
At the moment this example uses an in-memory Map implementation of CorrelationIdService to store and retrieve correlation IDs.
If this application were deployed in a distributed environment then it would not work in a multi-node setup.
You may have noticed that this sample starts an ActiveMQ container using Testcontainers.
Try the following.
Tell Testcontainers to also start a database container (RDBMS, MongoDB, etc).
Create a new CorrelationIdService implementation which saves and stores correlation IDs to and from your newly.
created database container, and wire it into the test
Create a new MessageLogger implementation which saves logged messages to this database, and wire it into the test.
Connector Configuration
How to chain request-reply Connectors
------------------------------
Chained Request-Reply with OAuth
How do I use the output of one synchronous call as the input to another synchronous call?
This connector example demonstrates the following features of connectors.
Message logging
Chaining request-reply connector calls (output of one call as an input to the next)
Request decoration for security
Media upload
This guide is also available in the separate connector-samples Git repository
here.
Setup
This example was originally written back when it was much simpler to get started developing applications for the Twitter API.
Unfortunately there are some hoops we must jump through to get things set up before running the example, but hopefully it’ll be worth it!
Twitter Account
To run this example, you will need a Twitter account.
If you already have one then skip to the next step.
You can sign up to Twitter using this link.
Twitter Developer Account
You also need a developer account as we will be creating a Twitter application to call APIs on behalf of your user.
You should be able to sign up for a developer account by going to the developer portal.
Twitter App
Once you have a developer account you should be able to create a new App from this page.
Elevated Access
To use the upload feature, our app must have elevated API access.
To get this we must apply for it, though it just a short questionnaire to make sure we don’t have any nefarious plans with all of their data.
Elevated access can be applied for on this page.
API Keys
At this point our Twitter app should be ready to go.
All we need to do is generate some credentials that can be used to authenticate requests to Twitters APIs on behalf of us.
The page for the app should have a tab for "Keys and tokens" where all the credentials can be generated.
The first of the credentials are the Consumer Keys.
You may have saved them when you first created the app but if not you can regenerate them.
The second set of credentials is an Access Token and Access Token Secret, which need to be generated.
Once all the credentials have been generated, the following environment variables need to be set using them.
.setOAuthConsumerKey(System.getenv("TWITTER_CONSUMER_KEY")) // API KEY
.setOAuthConsumerSecret(System.getenv("TWITTER_CONSUMER_SECRET")) // API KEY SECRET
.setOAuthAccessToken(System.getenv("TWITTER_ACCESS_TOKEN"))
.setOAuthAccessTokenSecret(System.getenv("TWITTER_ACCESS_TOKEN_SECRET"))
Domain Types
Our Twitter connector supports two operations, and as a result we have two domain types which both extend the
TwitterRequest abstract type.
We could also create two separate for each of these request if we wanted to.
UploadMediaRequest: Upload some media to share in a tweet later
StatusUpdateRequest: Update a Twitter status
And we also have their respective responses, under the TwitterResponse abstract type.
UploadMediaResponse: Response to media upload (containing the media ID)
StatusUpdateResponse: Response to tweet (containing the newly created tweet)
Connector Setup
We first create an HTTP request-reply connector transport like so.
HttpConnectorTransportConfiguration transportConfiguration = HttpConnectorTransportConfiguration
        .create(actorSystem.settings().config(), "chained-request-reply-example");
return new HttpConnectorTransport
        .Builder<TwitterRequest>()
        .withName("TwitterHttp")
        .withActorSystem(actorSystem)
        .withEnricher(httpRequest -> { (1)
            HttpHeader authorization = HttpHeader.parse("Authorization", getAuthorizationHeader(httpRequest));
            HttpRequest request = httpRequest.addHeader(authorization);
            return CompletableFuture.completedFuture(request);
        })
        .withTransportConfiguration(transportConfiguration)
        .build();
1
Note the enricher here, which adds the Authorization header containing the important OAuth 1.0 user context which Twitter will authenticate.
The inputs to this method are the HTTP method (always POST) and the URL (differs depending on whether this is a media upload or a status update)
We then create the following RequestReplySendConnector.
var connector = new RequestReplySendConnector
        .Builder<TwitterRequest, TwitterRequest, TwitterResponse, TwitterResponse>("Twitter")
        .withConnectorTransport(connectorTransport)
        .withActorSystem(actorSystem)
        .withMessageLogger(logger()) (1)
        .withSendTransportMessageConverter(request -> { (2)
            var messageHeaders = new MessageHeaders(Map.of(
                    "httpUrl", getUrl(request),
                    "httpMethod", "POST"
            ));
            if (request instanceof UploadMediaRequest) {
                byte[] bytes = ((UploadMediaRequest) request).getData();
                var entity = HttpEntities.create(ContentTypes.APPLICATION_OCTET_STREAM, bytes);
                var payload = createStrictFormDataFromParts(createFormDataBodyPartStrict("media", entity)).toEntity();
                return new TransportMessage(messageHeaders, payload);
            }
            return new TransportMessage(messageHeaders, "");
        })
        .withReceiveTransportMessageConverter(transportMessage -> { (3)
            if (transportMessage.getPayload().toString().contains("media_id_string")) {
                return fromJson(UploadMediaResponse.class).convert(transportMessage);
            } else {
                return fromJson(StatusUpdateResponse.class).convert(transportMessage);
            }
        })
        .withManualStart(false)
        .build();
1
Here we are defining a MessageLogger interface.
We take both sent and received messages and log.
This can be replaced with a database implementation where all message interactions are stored relating to this message association.
2
Here we are defining a function which takes the TwitterRequest that we want to send, and creating a
TransportMessage out of it, which is the representation of the request over the wire.
In this case we need to define a different URL based on whether this is a status update or a media upload.
3
Since this is a request-reply connector, we also need to define the reverse operation, which converts the received response back into a model POJO that we understand.
We use Jackson to determine whether this is a media update response or a status update response, and map it accordingly back to the right POJO.
Call Chain
The Twitter documentation for posting media (images, GIFs, videos, etc.) states that we have to upload the media first using one of the media upload methods (simple or chunked), which will return a media_id that expires in 24 hours unless if used in a tweet before then.
We then use that media_id in our subsequent status update call, which will attach the previously uploaded media to the tweet.
connector
        .send(ProcessingContext.unknown(), new UploadMediaRequest(kittenPicBytes())) (1)
        .thenApply(twitterResponse -> (UploadMediaResponse) twitterResponse) (2)
        .thenCompose(uploadMediaResponse -> {
            String status = String.format("I am a status update at %s! Also check out this cat photo", LocalDateTime.now());
            var statusUpdateRequest = new StatusUpdateRequest(status, List.of(uploadMediaResponse.getMediaId()));
            return connector.send(ProcessingContext.unknown(), statusUpdateRequest); (3)
        })
        .toCompletableFuture()
        .join();
1
Calling the connector for the first time to upload the photo of the kitten
2
Casting the response into an UploadMediaResponse
3
Calling the connector for the second time, with the mediaId from UploadMediaResponse to post a tweet containing the media as an attachment
Exercises
Exercise 1: Retrieve a Remote Image
At the moment the sample is loading kitten.jpg from src/test/resources.
Try instead to add a call to the chain to - for example - the NASA Astronomy Picture of the Day (APOD) API to dynamically retrieve an image instead.
The APOD API returns a structure like this.
{
  "copyright": "Giancarlo Tinè",
  "date": "2021-11-15",
  "explanation": "What happening above that volcano? Something very unusual -- a volcanic light pillar. More typically, light pillars are caused by sunlight and so appear as a bright column that extends upward above a rising or setting Sun. Alternatively, other light pillars -- some quite colorful -- have been recorded above street and house lights. This light pillar, though, was illuminated by the red light emitted by the glowing magma of an erupting volcano. The volcano is Italy's Mount Etna, and the featured image was captured with a single shot a few hours after sunset in mid-June. Freezing temperatures above the volcano's ash cloud created ice-crystals either in cirrus clouds high above the volcano -- or in condensed water vapor expelled by Mount Etna. These ice crystals -- mostly flat toward the ground but fluttering -- then reflected away light from the volcano's caldera.   Explore Your Universe: Random APOD Generator",
  "hdurl": "https://apod.nasa.gov/apod/image/2111/EtnaLightPillar_Tine_5100.jpg",
  "media_type": "image",
  "service_version": "v1",
  "title": "Light Pillar over Volcanic Etna",
  "url": "https://apod.nasa.gov/apod/image/2111/EtnaLightPillar_Tine_960.jpg"
}
Try extending the current chain by adding two more calls at the beginning.
A new RequestReplySendConnector call to retrieve the link to the picture of the day
A new RequestReplySendConnector call to request the image located at url from the response (you will need to convert it into a byte array)
You will then need to change the contents of the MediaUpload to reference the byte array of the APOD image instead of the local image of the kitten.
Exercise 2: Using the chunked media upload API
The sample currently uses the "simple" API for uploading images to Twitter, which is a brittle method for uploading large media as it is not resilient to network outages and cannot be resumed.
Twitter offers a more resilient method of uploading media called the chunked media upload API which supports pausing and resuming uploads.
The documentation for that can be found here.
Try changing the existing implementation to use the chunked media upload API, and perhaps upload some larger media, like a video or GIF.
The steps would be.
Call to INIT to declare the start of a media upload
Multiple calls to APPEND chunks of the media as fragments, with an incrementing index
Call to FINALIZE the media upload and receive the all-important media_id
Call to update our status using the media_id from step 3
How to implement asynchronous request reply
How to get Connector metrics
------------------------------
Metrics
How can I receive events and metrics relating to my connector?
Connectors, by default, come with Lightbend Telemetry
for reporting metrics.
This enables the metrics to be exposed via various backends such as Prometheus, Datadog, New Relic, and so on.
The metrics example will show how to set up metrics with Prometheus, one of the most prevalent monitoring systems.
Lightbend Telemetry was formerly known as Cinnamon. You will see references to both in these samples.
This guide is also available in the separate connector-samples Git repository
here.
Cinnamon Agent
Before we can run the example, first run the following maven goal.
mvn clean compile
This is important because it will copy the cinnamon agent JAR into the target directory.
This achieved by adding the following plugin configuration in the pom.xml file.
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-dependency-plugin</artifactId>
    <version>3.1.2</version>
    <executions>
        <execution>
            <id>copy</id>
            <phase>compile</phase>
            <goals>
                <goal>copy</goal>
            </goals>
            <configuration>
                <artifactItems>
                    <artifactItem>
                        <groupId>com.lightbend.cinnamon</groupId>
                        <artifactId>cinnamon-agent</artifactId>
                        <version>${cinnamon.version}</version>
                        <overWrite>true</overWrite>
                        <destFileName>cinnamon-agent.jar</destFileName>
                    </artifactItem>
                </artifactItems>
            </configuration>
        </execution>
    </executions>
</plugin>
After running the maven compile goal, there should be a dependency/cinnamon-agent.jar file inside the target directory.
We can now add the following VM argument when running the test.
-javaagent:target/dependency/cinnamon-agent.jar
If using IntelliJ IDEA, it might be necessary to disable the IntelliJ Java agent, as it can cause issues with the Cinnamon agent.
The agent can be disabled by going selecting Help > Edit Custom Properties.. and then adding idea.no.launcher=true to the properties file.
The Lightbend Telemetry (Cinnamon) Agent can be acquired as part of a Lightbend subscription.
More info on setting that up can be found here.
This test sends a message via a SendConnector using a no-op ConnectorTransport for an hour.
Verify Metrics
If the VM argument above is set correctly the metrics should be published to a locally running prometheus server which is exposes on port 9001.
Navigate to localhost::9001.
There should be a page full of text, displaying various metrics.
Search for "application_ipf_requests_sent"
The search should show the following result.
This metric shows the total count of messages sent via connectors.
application_ipf_requests_sent{application="com.intellij.rt.junit.JUnitStarter",host="some-hostname",connector="MetricsSendConnector",type="send",} 125.0
This statistic (and others) can be scraped by Prometheus and fed into a graphing library, such as Grafana to produce interesting visualisations of the data.
Prebuilt Grafana graphs by Lightbend can be found here.
How to chain request-reply Connectors
How to filter messages
------------------------------
Filter Messages
How do I filter messages so they are not passed to my receive handler?
This Connector example shows how you can filter messages based on their header values.
Defining a Filter
Simply define a filter by defining Criteria as such:
        var propNameToFilterOn = "myprop";
        new ReceiveConnector
                .Builder<String>("OurReceiver")
                .withActorSystem(actorSystem)
                .withConnectorTransport(receivingTransport)
                .withMessageLogger(logger())
                .withFilterCriteria(new MessageHeaderCriteria(propNameToFilterOn, "1")) (1)
                .withReceiveTransportMessageConverter(a -> a.getPayload().toString())
                .withManualStart(false)
                .withProcessingContextExtractor(a -> ProcessingContext.unknown())
                .withSkipCorrelationOn(a -> true)
                .withReceiveHandler((receivingContext, response) ->
                        CompletableFuture.supplyAsync(() -> list.add(response))
                                .thenApply(a -> null))
                .build();
1
Defining the filter
In the above example, we filter out (i.e. do not accept) any messages that have a property name myprop having the
value 1.
Note that it is possible to write custom Criteria and also use logical AND and OR operators to combine operators.
See Filtering for more information on this.
How to get Connector metrics
How to accept PascalCase messages
------------------------------
PascalCase Messages
How do I accept messages that are incoming in PascalCase format?
Defining a Custom Jackson ObjectMapper
IPF currently has a utility class called the SerializationHelper which is a wrapper around a Jackson ObjectMapper that registers custom serialisers/deserialisers. This allows us to have a consistent approach when serialisers/deserialisers messages across the IPF estate.
By default Jackson support CamelCase and the SerializationHelper therefore also only support CamelCase messages by default. This can be easily changed to support PascalCase.
To support PascalCase you need to define a new instance of the ObjectMapper with the following property:
    private final ObjectMapper pascalCaseMapper = SerializationHelper.objectMapper()
            .setPropertyNamingStrategy(PropertyNamingStrategies.UPPER_CAMEL_CASE);
This can then be used when mapping messaged to/from external system within the connector code.
Below is an example ReceiveConnector and SendConnector that would receive or send messages in PascalCase respectively:
package com.iconsolutions.connector.samples.pascal;
import akka.actor.ActorSystem;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.PropertyNamingStrategies;
import com.iconsolutions.common.exception.IconRuntimeException;
import com.iconsolutions.ipf.core.connector.ReceiveConnector;
import com.iconsolutions.ipf.core.connector.ReceiveConnectorBuilderHelper;
import com.iconsolutions.ipf.core.connector.SendConnector;
import com.iconsolutions.ipf.core.connector.SendConnectorBuilderHelper;
import com.iconsolutions.ipf.core.connector.message.MessageHeaders;
import com.iconsolutions.ipf.core.connector.message.TransportMessage;
import com.iconsolutions.ipf.core.connector.transport.ConnectorTransport;
import com.iconsolutions.ipf.core.connector.transport.ReceiveConnectorTransport;
import com.iconsolutions.ipf.core.shared.correlation.CorrelationId;
import com.iconsolutions.ipf.core.shared.domain.context.ProcessingContext;
import com.iconsolutions.ipf.core.shared.domain.context.UnitOfWorkId;
import com.iconsolutions.ipf.payments.domain.clearing_and_settlement.pacs002.FIToFIPaymentStatusReport;
import com.iconsolutions.ipf.payments.domain.clearing_and_settlement.pacs008.FIToFICustomerCreditTransfer;
import com.iconsolutions.ipf.payments.domain.payment_initiation.pain001.CustomerCreditTransferInitiation;
import com.iconsolutions.samplesystems.shared.model.header.CryptoHelper;
import com.iconsolutions.simulator.api.RequestHandler;
import lombok.extern.slf4j.Slf4j;
import org.springframework.context.annotation.Bean;
@Slf4j
public class PascalConnectors {
    /**
     * Here set the UPPER_CASE_CAMEL (PascalCase) naming strategy
     * on your ObjectMapper instance
     */
    private final ObjectMapper pascalCaseMapper = SerializationHelper.objectMapper()
            .setPropertyNamingStrategy(PropertyNamingStrategies.UPPER_CAMEL_CASE);
    @Bean
    public ReceiveConnector<CustomerCreditTransferInitiation> executePaymentReceiveConnector(
            ReceiveConnectorTransport initPaymentReceiveConnectorTransport,
            RequestHandler someRequestHandler,
            ActorSystem actorSystem) {
        return ReceiveConnectorBuilderHelper.<CustomerCreditTransferInitiation>builder("InitPaymentReceive", "initpayment.receive-connector", actorSystem)
                .withMessageLogger(m -> log.debug("Receive connector has identified message: {}", m.getMessage()))
                .withProcessingContextExtractor(tm -> ProcessingContext.builder()
                        .unitOfWorkId(UnitOfWorkId.createRandom())
                        .build())
                .withConnectorTransport(initPaymentReceiveConnectorTransport)
                .withReceiveTransportMessageConverter(message -> this.convertResponse(message.getPayload().toString()))
                .withReceiveHandler((context, payload) -> someRequestHandler.process(payload))
                .build();
    }
    /**
     * Here you would convert the incoming PAIN001 JSON message in Pascal format into your PAIN001 domain object
     */
    public CustomerCreditTransferInitiation convertResponse(String messageText) {
        try {
            return pascalCaseMapper.readValue(messageText, CustomerCreditTransferInitiation.class);
        } catch (JsonProcessingException e) {
            throw new IconRuntimeException(e);
        }
    }
    @Bean
    public SendConnector<FIToFIPaymentStatusReport, FIToFIPaymentStatusReport> executePaymentSendConnector(
            ConnectorTransport<FIToFIPaymentStatusReport> executePaymentReceiveConnectorTransport,
            ActorSystem actorSystem) {
        return SendConnectorBuilderHelper
                .<FIToFIPaymentStatusReport, FIToFIPaymentStatusReport>builder("ExecutePaymentSend", "executepayment.send-connector", actorSystem)
                .withMessageLogger(m -> log.debug("Send connector has identified message: {}", m.getMessage()))
                .withCorrelationIdExtractor(it -> CorrelationId.of(it.getTxInfAndSts().get(0).getOrgnlTxId()))
                .withConnectorTransport(executePaymentReceiveConnectorTransport)
                .withSendTransportMessageConverter(this::convertToTransport)
                .build();
    }
    /**
     * Here you would convert the PACS002 domain object into Pascal formatted JSON before sending out
     */
    public TransportMessage convertToTransport(FIToFIPaymentStatusReport response) {
        try {
            return new TransportMessage(new MessageHeaders(CryptoHelper.messageHeaders()), pascalCaseMapper.writeValueAsString(response));
        } catch (JsonProcessingException e) {
            throw new IllegalStateException(e);
        }
    }
}
How to filter messages
Application Builder
------------------------------
Application Builder
This section covers the core components or modules which exist to get you started building IPF applications.
Flo Starter - this consists of all the necessary core dependencies required to wire together an IPF flow implementation.
Archetype Starter - this is the quickest way to bootstrap a new project using the IPF SDK. The Maven archetype will generate a Maven project (using the Icon BOM as its parent) containing an application skeleton, the flow domain and a test module.
How to accept PascalCase messages
Flo Starter
------------------------------
IPF Flo Starter
IPF Flo Starter bootstraps an IPF flow implementation by providing an Akka ActorSystem and the Spring Boot autoconfiguration to wire all the necessary core dependencies that are required for running a command (write) and query (read) side.
Those core dependencies are often used features documented in the Features section and you will find common implementation activities in the Getting Started.
Concepts
Flo Starter Projects
Understanding Passivation, Remembering Entities, and Schedulers
Domain Operations
Features
Application Configuration
Transaction Caching
Application Health Indicators
Flow Scheduling
Automated Retries
Monitoring and Observability
Getting Started
Create a new IPF Application Project
Defining The Read Side
How to guides…​
How to handle schema evolution in IPF events
How to troubleshoot error messages
How to secure IPF HTTP Endpoints
How to use the persistent scheduler with a flow
How to implement a duplicate check function
How to get aggregate data for use in an external domain request
Application Builder
Concepts
------------------------------
Concepts
Discover the key concepts that IPF Flo Starter provides
Flo Starter Projects
Understanding Passivation, Remembering Entities, and Schedulers
Domain Operations
Flo Starter
Flo Starter Projects
------------------------------
Flo Starter Projects
Modules
ipf-common-starter
This is the base module that provides these foundational capabilities.
It is used by the other starter projects.
Akka actor system and other sub-systems (e.g. cluster sharding, persistence)
HOCON and Spring configuration initialiser.
See IPF Application Configuration. Spring info contributors to display application properties on /actuator/info
ipf-write-starter
Provides write capabilities for persisting events to the selected persistence type.
Additionally, this provides:
BehaviourRetriesSupport for sending commands to Akka Event Sourced Behaviour in a reliable way
Metrics recorders that provide metrics for Prometheus
TransactionCacheService for detecting functional and technical duplicate checking.
See Transaction Caching
ipf-journal-processor-starter
Provides capabilities that continuously reads persisted events and then delegates the processing of those events.
See
ipf-read-starter for implementations of EventProcessor.
This currently supports processing events from three persistence types:
Mongo
Cassandra
JDBC
Error Handling
In the case the EventProcessor implementation fails
- Resiliency Settings are in place to handle recovery of messages from the event stream. Optionally a dead letter strategy can be implemented in the case of retry failures.
Deadletter Appender
The DeadletterAppender is a functional interface which is called whenever a message fails during processing of the event stream after all retries have been exhausted.
@FunctionalInterface
public interface DeadletterAppender {
    /**
     * Appends the {@link EventProcessorException} to the deadletter queue.
     *
     * @param eventProcessorException contains the failed {@link EventEnvelope message}, {@link PersistenceId id} and the cause of failure
     * @return a {@link CompletableFuture}
     */
    CompletableFuture<Void> append(EventProcessorException eventProcessorException);
}
Providing a DeadletterAppender implementation is optional and if one is not provided the journal processor will use the default implementation.
The default behaviour is to simply log both the failed message and the exception that caused the error.
All failed messages will be provided as an exception that extends EventProcessorException.
EventProcessorException wraps the original exception as the cause alongside the received event.
ipf-read-starter
Provides capabilities to process the events that were read from ipf-journal-processor-starter to construct a domain aggregate.
See ReadSideEventProcessor as an example of an EventProcessor implementation.
A static /index.html is provided out of the box for a simple view of the read aggregates that have been successfully processed.
Default Configuration
ipf-common-starter
ipf.conf
# Default IPF configuration to allow bootstrapped execution of applications that depend on ipf-common-starter and provide
# core functionalities such metrics, health, ports and cluster set-up.
# Name of the actor system that is required by Akka. There will only be a single actor system per application
actor-system-name = ipf-flow
# The name to be used as a unique identifier of the source of IPF system events
ipf.system-events.source = 0.0.0.0
ipf.system-events.source = ${?akka.remote.artery.canonical.hostname}
ipf.application.name = ${actor-system-name}
spring.application.name = ${ipf.application.name}
# The duration after which the connector event processor will
# check connector health and perform the startup logic
connector.event-processor.keep-alive-interval = 5s
# Default cinnamon to allow for monitoring
cinnamon {
  prometheus.exporters += "http-server"
}
# Exposing Spring management endpoints for further metrics
management {
  endpoint.metrics.enabled = true
  endpoints.web.exposure.include = "health,info,metrics,prometheus"
  endpoint.health.probes.enabled=true
  health.livenessState.enabled=true
  health.readinessState.enabled=true
  endpoint.prometheus.enabled = true
  endpoint.health.show-details = always
  metrics.export.prometheus.enabled = true
}
ipf-journal-processor-starter
ipf.conf
# Default IPF journal processor configuration
# Identifies that this node is on the read side when the cluster endpoint is invoked. It's added additionally with += so nodes can have more roles.
akka.cluster.roles += read-model
event-processor {
  # The ID used to create the key for the Entity Type of events.
  # The KeepAlive cluster singleton is also created with the name keepAlive-[id]
  id = EventProcessor
  # The interval at which the KeepAlive actor probes itself with a Probe.INSTANCE
  keep-alive-interval = 2 seconds
  # The tag prefix for events generated by the write side.
  # This value must match the `ipf.behaviour.event-processor.tag-prefix` setting
  # used by the write side or else the processor will not be consuming all events.
  tag-prefix = ["tag"]
  # Ensure configurations match if journal processor is running on the write side
  tag-prefix = ${?ipf.behaviour.event-processor.tag-prefix}
  # The number of partitions configured for our event journal.
  # Each partition will be processed in parallel by a dedicated Akka stream.
  # This value must match the `ipf.behaviour.event-processor.parallelism` setting
  # used by the write side - in case the value is lower than `parallelism` the processor
  # will not be consuming all events, and if the value is higher, the processor will
  # not balance the work between the nodes equally.
  number-of-partitions = 4
  # Maintaining backward compatibility with previous configuration setting.
  number-of-partitions = ${?event-processor.parallelism}
  # Ensure configurations match if journal processor is running on the write side
  number-of-partitions = ${?ipf.behaviour.event-processor.parallelism}
  # The number of events to demand per partition from upstream, and process in parallel
  upstream-event-demand = 1
  restart-settings {
    # The starting backoff interval to use when restarting event processor streams.
    min-backoff = 500 millis
    # The starting backoff interval to use when restarting event processor streams.
    max-backoff = 20 seconds
    # Maintaining backward compatibility with previous configuration.
    max-backoff = ${?event-processor.backoff}
    # The amount of restarts is capped within a timeframe of max-restarts-within.
    max-restarts = 86400000
    # The amount of restarts is capped to max-restarts within a timeframe of within.
    max-restarts-within = 1 days
    # The starting backoff interval to use when restarting event processor streams.
    jitter = 0.1
  }
  # To improve throughput, offsets of successfully processed events are
  # not checkpointed for each event but are grouped together in
  # size and time based windows and the last event offset in a window
  # is used as a checkpoint.
  # The window is considered complete when either it is filled by `size`
  # offsets or the `timeout` interval has elapsed.
  commit-offset-window {
    # The size of the window.
    size = 1000
    # The amount of time to wait for `size` events to complete.
    timeout = 1 minute
  }
}
# Enables bean creation in *ReadSideConfig Spring config. The flow domain module contains configuration for both the read and write sides. This allows the selective initiation of *ReadSideConfig or *WriteSideConfig based on which Maven modules are included.
ipf.read-side.enabled = true
ipf-write-starter
ipf.conf
ipf {
  behaviour {
    retries {
      initial-timeout = 100ms
      backoff-factor = 2
      max-attempts = 3
    }
    metrics {
      behaviour-configurations = [
        {
          behaviour = com.iconsolutions.ipf.core.platform.write.test.TestApp.Beh1
          enabled = true,
          csm-related-states = [Csm1, Csm2]
        },
        {
          behaviour = com.iconsolutions.ipf.core.platform.write.test.TestApp.Beh2
          enabled = true
          event-whitelist = [
            com.iconsolutions.ipf.core.platform.write.test.TestApp.Evt1,
            com.iconsolutions.ipf.core.platform.write.test.TestApp.Evt2
          ]
          event-blacklist = [com.iconsolutions.ipf.core.platform.write.test.TestApp.Evt1]
        },
        {
          behaviour = com.iconsolutions.ipf.core.platform.write.test.TestApp.Beh3
          enabled = false
        }
      ]
    }
  }
}
Concepts
Understanding Passivation, Remembering Entities, and Schedulers
------------------------------
Understanding Passivation, Remembering Entities, and Schedulers
When IPF flows are inflight (i.e. have not yet reached a terminal state), IPF’s default behaviour is to keep them in memory
while they are in progress until they are terminated. Flows are distributed more-or-less evenly across the cluster with
Akka Cluster Sharding.
This article explains the concepts of passivation, remembering entities, and how to restart failed flows.
Remembering Entities
If a node dies or is gracefully stopped (or if all nodes are stopped and the application is cold-started), then all
in-flight flows (shards) are loaded back into memory on
the next startup. This is called remembering entities.
The way Akka determines whether a flow is or is not completed is by also storing this state in a store of some
description. The options for this are eventsourced (the IPF default) or ddata. The eventsourced option is the IPF
default because it is more performant and is resilient to total restarts of the cluster with no additional
configuration.
Any data stored in ddata is saved to disk by default, so cannot survive a total cluster restart in a containerised
environment like Kubernetes. On such environments  you will have to configure a Persistent Volume
to store this state, and change the IPF configuration to point the application at the persistent volume.
More information on the different remember entities stores (and how to configure them) here.
Passivation and Automatic Passivation
By default, a flow is considered "stopped" when the flow determines that it has reached a terminal state and stops
itself. This is called passivation.
Other ways to stop a flow - even if it isn’t finished - are:
Manually passivating using domain operations
Auto-passivate after entities reach a certain age. This is called automatic passivation.
If remembering entities is enabled, automatic passivation cannot also be enabled
Action Revival
When a flow is loaded back into memory with remembering entites after another node leaving (or crashing), the IPF flow
is notified of this and reacts by sending itself an ActionRecoveryCommand. This special command attempts to "poke" the
flow to perform the next action that it needs to perform based on the last known state. More information on the topic is
available here.
Default IPF behaviour
The default IPF behaviour is:
Remember entities with eventsourced as the store
Automatic Passivation off (must be off if remember entities is on)
Akka Scheduler
This default behaviour is suitable for flows that match the following profile:
Short-lived
Any sort of volume (high or low)
However this approach can become problematic when there are lots of long-running flows: if there is a step that waits for
hours or days, then there might be a large number of flows that are parked and consuming memory when they will not be
needed for a long time.
If this is the case, then it might be worth changing how the cluster behaves by:
Switching off remembering entities
Enabling automatic passivation
Using IPF’s persistent Quartz Scheduler to schedule retries
If they were to be laid out in a table, the options would be:
ID
Remember entities
Auto-passivation
Scheduler implementation
Good for
How node failures handled?
1
Enabled
Disabled
Akka (not persistent)
High- or low-volume flows that complete quickly (e.g. within a minute)
Flows are always held in memory and ActionRevival process (see above) is triggered when entities are restarted on
another node after a failure or shutdown
2
Disabled
Enabled
Quartz (persistent)
Long-running flows
Retrying of tasks is managed separately to the flow itself and is also resilient against failures
Triggering for missed events in the past
If using combination 2 (remember entities disabled/auto passivation enabled/Quartz) then the Quartz Scheduler
implementation will manage the retries for any tasks relating to flows. This is resilient because it too uses a
Cluster Singleton to manage this work and in the event of the failure of the node that is holding the Cluster Singleton,
the singleton is migrated onto the next oldest node and the jobs are re-queued for retrying.
This process should only take a few seconds and so for long-running processes this is generally not a big problem.
However in the unlikely case that a job has needed to run while the Cluster Singleton was in the process of being
migrated, it is currently skipped with a warning in the logs alerting the user to this issue (i.e. a trigger
was missed while the Cluster Singleton was being moved). This functionality can be toggleable in the future if so
desired.
Database usage concerns
On startup and with remember entities enabled, IPF will read in all in-flight entities by reading in all of their events
to build the current state of that particular flow. This is a read-heavy operation, and for environments where database
usage is metered in such a way (e.g. Azure Cosmos DB) then it might be a better idea to use Combination 2 to avoid this
heavy startup cost.
Configuration
To configure the non-default Combination 2, set the following configuration keys in ipf-impl.conf or
application.conf:
# Disable remembering entities
akka.cluster.sharding.remember-entities = off
# Set the passivation strategy to be the default i.e. a maximum of active entities at any one time
akka.cluster.sharding.passivation.strategy = default-strategy
# Change the below if you want to use a separate MongoDB instance for persisting scheduled tasks
ipf-scheduler {
  mongodb = {
    uri = ${ipf.mongodb.uri}
    database-name = "scheduled"
  }
}
Flo Starter Projects
Domain Operations
------------------------------
Domain Operations
This page lists the various operations that are available on the global domain level as static method calls.
passivate: Unload a flow from memory
This will stop a flow instance (even if it’s not reached a terminal state) to save some memory, if it is known that it
has reached a long-running step, for example.
Note that messages directed at this instance will still be delivered, but IPF will reload the events of that flow from
the journal instead of having them already stored in memory.
The call is:
XxxDomain.passivate("MyBehaviour|some-id-23149082");
Where Xxx is the name of the IPF solution containing the relevant flow, and MyBehaviour is the name of the flow
itself.
getAggregate: Get a flow’s aggregate (state)
To retrieve the current state of a flow instance, you can call:
XxxDomain.getAggregate("MyBehaviour|some-id-23149082");
This will return a CompletionStage<Aggregate>
which completes when the aggregate is returned.
The Aggregate will be of the type MyBehaviourAggregate which will contain:
The current status
The last failure reason code(s)/description(s)
All business data
All events
getStatus: Get a flow’s status
This operation is similar to that above, but will only return the current status. Also returns as a
CompletionStage<AggregateStatus>.
To retrieve the status:
XxxDomain.getStatus("MyBehaviour|some-id-23149082");
abort: Stop a flow’s execution
This command takes a reason argument and:
Sets the flow’s status to ABORTED
Sets the resulting status to Aborted
Publishes an AbortedEvent with the reason specified in the call
To abort a flow:
XxxDomain.abort("MyBehaviour|some-id-23149082", "some special reason");
Once a flow has been aborted, it cannot be resumed, even with the resume function below.
resume: Continue a flow’s execution
This operation will invoke the action revival process for a flow to attempt
to move a transaction that appears to be stuck onto the next state.
Note that this operation cannot resume aborted transactions.
To resume a flow:
XxxDomain.resume("MyBehaviour|some-id-23149082");
Understanding Passivation, Remembering Entities, and Schedulers
Features
------------------------------
Features
Foundational IPF capabilities often used in IPF application and solution builds
Application Configuration
Transaction Caching
Application Health Indicators
Flow Scheduling
Automated Retries
Monitoring and Observability
Domain Operations
Application Configuration
------------------------------
Application Configuration
A convention has been introduced for an IPF application start-up configuration.
The aim was to provide a consistent and predictable approach to configuration, yet flexible enough for integration tests
that utilise various Spring annotations to inject configuration dynamically.
Config Hierarchy
When preparing the application for deployment, the configuration hierarchy will be observed as long as these rules
are followed:
Only HOCON (.conf files) are used for application configuration
Developers must populate the appropriate configuration file following this descending hierarchy of precedence:
application-override.conf
Where? Deployment config (i.e. k8s configmap) or on file-system mounted on [app-name]/conf to the container
When to use? This is to be used in an emergency that cannot be fulfilled by the existing hierarchy
application.conf
Where? Deployment config (i.e. k8s configmap) or on file-system mounted on [app-name]/conf to the container
When to use? Should contain environment specific configuration such as URLs or security config. If we had to deploy the same application
to different environments, the differences in the application.conf should mostly consist of environment specific configuration.
ipf-impl.conf
Where? src/main/resources of IPF based applications
When to use? As defaults for standing the application for local execution. Can also provide additional placeholders
as overrides to be set by application.conf for common configuration across the modules. For example, ipf-impl.conf in
sample-client provides overrides via an include file called endpoint-overrides.conf that override encryption config for
every known endpoint.
ipf.conf
Where? src/main/resources of modules used by IPF based application
When to use? Defining sensible defaults for applications that will use this module. Modules should not set the same
configuration since they are at the same level. The problem is that it will not be predictable as to which config will
"win". The exception is if += is used on lists (e.g. akka.cluster.roles). Placeholders should be avoided unless
necessary or recommended by the underlying library. For example, Alpakka Kafka recommend using
config inheritance to assign defaults
for Kafka configuration.
This can be seen as IPFs version of Akka’s reference.conf for the various modules.
Further Overriding
If configuration via the file hierarchy is not enough, further overriding is possible with environment variables and
JVM properties.
Environment Variables
By setting the JVM property -Dconfig.override_with_env_vars=true (via IPF_JAVA_ARGS on IPF containers) it is possible to override any configuration value using environment variables even if an explicit substitution is not specified.
The environment variable value will override any pre-existing value and also any value provided as Java property.
With this option enabled only environment variables starting with CONFIG_FORCE_ are considered, and the name is mangled as follows:
the prefix CONFIG_FORCE_ is stripped
single underscore(_) is converted into a dot(.)
double underscore(__) is converted into a dash(-)
triple underscore(_) is converted into a single underscore(_)
i.e. The environment variable CONFIG_FORCE_a_bc_d set the configuration key a.b-c_d
This is only supported from com.typesafe:config:1.4.0 onwards. If this does not work, it’s likely because your
application is pulling in an older version as a transitive dependency.
System Properties
Any configuration value can be overridden as a Java -D property (again via IPF_JAVA_ARGS on IPF containers). These
will override configuration properties set in the file hierarchy. However, environment variables set with CONFIG_FORCE_
still take precedence.
Lightbend HOCON + Config
The process is powered by the combination of HOCON and
the Lightbend Config library. While the Config library also supports YAML and
.properties files, adopting only HOCON provides consistency and flexibility as it has additional capabilities over
the other two file formats.
Noteworthy are the block inheritance and substitution functionalities but also that Akka serves as the foundation to IPF.
Many standard configuration options would be difficult or awkward to do such as defining seed node lists.
The readability of HOCON makes it suitable as documentation as well as default configuration. In this way
much effort is saved from writing additional support documentation when the default configuration can be commented on.
There is also added impetus for developers to keep configuration tidy as it can be used as customer facing collateral.
Implementation Detail
The config hierarchy is used as fallback to Spring PropertySource, loaded before the application context is initialised.
This is purposely to maintain the usage of the various Spring annotations that can be used to dynamically inject properties
into integration tests. As we avoid using any Spring configuration mechanisms (e.g. application.* or Spring profiles)
HOCON and the Config hierarchy should become the only source of configuration in actual deployments.
This means that it is still possible to use Spring config when deployed. However, this is strongly discouraged to maintain
consistency in configuring our expanding landscape of components.
    /**
     * Combine the two configurations in Spring through the environment and HOCON through config.
     * <p>
     * .defaultOverrides() is required so when -Dconfig.override_with_env_vars=true you can set environment variables
     * with CONFIG_FORCE_ to override any configuration.
     *
     * @param applicationContext Combined configuration from Spring and HOCON
     */
    @Override
    public void initialize(ConfigurableApplicationContext applicationContext) {
        ConfigurableEnvironment env = applicationContext.getEnvironment();
        // Skip the Spring Cloud context and set the configuration of the main application context instead
        if (!"bootstrap".equals(env.getProperty("spring.config.name")) && shouldInitialiseConfig(applicationContext)) {
            env.getPropertySources().remove(COMBINED_CONFIG);
            Config springConfig = new ConfigBuilder(env).buildHoconConfigFromEnvironment();
            Config hoconConfig = parseWithFallback(getConfigFileHierarchy());
            Config allConfig = ConfigFactory.defaultOverrides()
                    .withFallback(springConfig)
                    .withFallback(hoconConfig);
            var additionalConfig = getAdditionalConfig(allConfig);
            if (additionalConfig.isPresent()) {
                allConfig = allConfig.withFallback(additionalConfig.get());
            }
            allConfig = allConfig.resolve();
            PropertySource<?> allProperties = ConfigPropertySourceConverter.createPropertySource(COMBINED_CONFIG, allConfig);
            env.getPropertySources().addFirst(allProperties);
            Object singleton = applicationContext.getBeanFactory().getSingleton(DEFAULT_CONFIG);
            if(singleton != null) {
                ((DefaultListableBeanFactory)applicationContext.getBeanFactory()).destroySingleton(DEFAULT_CONFIG);
            }
            applicationContext.getBeanFactory().registerSingleton(DEFAULT_CONFIG, allConfig);
        }
    }
Masking Sensitive Data in Logs
Masking is done with our MaskPatternLayout class, which extends logback’s PatternLayout.
Basically, we have replaced the default layouts with the new one in the DefaultLogbackConfigInitializer, which gets triggered after HoconConfigInitialiser.
Hocon configuration needs to be initialized first since masking configuration exists there.
Masking configuration contains two parts:
masking.enabled - boolean value that says if the feature itself is enabled or not
masking.mask-config-objects - that represents an array of objects. Each object contains reg-ex and strategy information for the value we want to mask
We have introduced the following masking strategies:
MASK_ALL - mask all characters
MASK_FIRST_N_CHARS - mask first n characters
MASK_LAST_N_CHARS - mask last n characters
MASK_M_TO_N_CHARS - mask chars from position m to n
NO_MASK_FIRST_N_CHARS - do not mask first n characters
NO_MASK_LAST_N_CHARS - do not mask last n characters
NO_MASK_M_TO_N_CHARS - do not mask chars from position m to n
Here’s an example how masking configuration looks like:
masking {
  enabled = true
  mask-config-objects = [
    //objects matching json fields
    {
      pattern = "\"Nm\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"Id\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_FIRST_N_CHARS",
        args = {
            n = 3
        }
      }
    },
    {
      pattern = "\"Dept\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "NO_MASK_M_TO_N_CHARS",
        args = {
            m = 3,
            n = 5
        }
      }
    }
  ]
}
So as mentioned above, there is a regex for each value we want to mask. This will catch the whole line with the key/tag and the value,
but only the group that represents the value is matched for masking.
Here are some examples of masking xml tags, showing how each strategy should work:
MASK_ALL -> <Dept>****</Dept>,
MASK_FIRST_N_CHARS(3) -> <SubDept>***Dept</SubDept>,
MASK_LAST_N_CHARS(2) -> <StrtNm>Strt**</StrtNm>,
NO_MASK_FIRST_N_CHARS(4) -> <BldgNb>Bldg**</BldgNb>,
NO_MASK_LAST_N_CHARS(2) -> <BldgNm>****Nm</BldgNm>.
MASK_M_TO_N_CHARS(4,7) -> <TwnLctnNm>Twn****Nm</TwnLctnNm>,
NO_MASK_M_TO_N_CHARS(5,7) -> <CtrySubDvsn>****Sub****</CtrySubDvsn>
Already defined masking objects
  mask-config-objects = [
    //objects matching json fields
    {
      pattern = "\"Nm\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"Id\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"Dept\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"SubDept\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"StrtNm\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"BldgNb\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"BldgNm\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"Flr\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"PstBx\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"Room\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"PstCd\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"TwnNm\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"TwnLctnNm\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"DstrctNm\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"CtrySubDvsn\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"Ctry\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"Adrline\"\\s*:\\s*\\[\\s*\"([^\"]+)\\s*\\]\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"BirthDt\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"PrvcOfBirth\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"CityOfBirth\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"CtryOfRes\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"CtryOfBirth\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"NmPrfx\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"PhneNb\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"MobNb\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"FaxNb\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"TaxId\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"RegnId\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"EmailAdr\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"EmailPurp\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"JobTitl\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"Rspnsblty\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "\"Titl\"\\s*:\\s*\"([^\"]+)\"",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    //objects matching xml tags
    {
      pattern = "<(?:\\w+:)?Nm>(.+)</(?:\\w+:)?Nm>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?Id>(.+)</(?:\\w+:)?Id>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?Dept>(.+)</(?:\\w+:)?Dept>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?SubDept>(.+)</(?:\\w+:)?SubDept>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?StrtNm>(.+)</(?:\\w+:)?StrtNm>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?BldgNb>(.+)</(?:\\w+:)?BldgNb>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?BldgNm>(.+)</(?:\\w+:)?BldgNm>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?Flr>(.+)</(?:\\w+:)?Flr>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?PstBx>(.+)</(?:\\w+:)?PstBx>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?Room>(.+)</(?:\\w+:)?Room>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?PstCd>(.+)</(?:\\w+:)?PstCd>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?TwnNm>(.+)</(?:\\w+:)?TwnNm>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?TwnLctnNm>(.+)</(?:\\w+:)?TwnLctnNm>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?DstrctNm>(.+)</(?:\\w+:)?DstrctNm>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?CtrySubDvsn>(.+)</(?:\\w+:)?CtrySubDvsn>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?Ctry>(.+)</(?:\\w+:)?Ctry>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?Adrline>(.+)</(?:\\w+:)?Adrline>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?BirthDt>(.+)</(?:\\w+:)?BirthDt>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?PrvcOfBirth>(.+)</(?:\\w+:)?PrvcOfBirth>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?CityOfBirth>(.+)</(?:\\w+:)?CityOfBirth>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?CtryOfRes>(.+)</(?:\\w+:)?CtryOfRes>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?CtryOfBirth>(.+)</(?:\\w+:)?CtryOfBirth>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?NmPrfx>(.+)</(?:\\w+:)?NmPrfx>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?PhneNb>(.+)</(?:\\w+:)?PhneNb>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?MobNb>(.+)</(?:\\w+:)?MobNb>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?FaxNb>(.+)</(?:\\w+:)?FaxNb>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?TaxId>(.+)</(?:\\w+:)?TaxId>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?RegnId>(.+)</(?:\\w+:)?RegnId>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?EmailAdr>(.+)</(?:\\w+:)?EmailAdr>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?EmailPurp>(.+)</(?:\\w+:)?EmailPurp>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?JobTitl>(.+)</(?:\\w+:)?JobTitl>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?Rspnsblty>(.+)</(?:\\w+:)?Rspnsblty>",
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    },
    {
      pattern = "<(?:\\w+:)?Titl>(.+)</(?:\\w+:)?Titl>"
      strategy {
        name = "MASK_ALL",
        args = {}
      }
    }
  ]
Important thing to mention is, because this feature’s implementation depends on logback library, in order for it to work, there can be used no other logger implementation.
Features
Transaction Caching
------------------------------
Transaction Caching
IPF is an event-sourced application and as a result the read ("query") side is eventually consistent.
This means that for cases where we need to look into the application’s very recent history, it might not be representative to look into the read side, as it may not yet have caught up with all events that have taken place on the write side.
As a result, users have the ability to implement a transaction cache, using the supplied transaction cache service.
It can be used to satisfy business requirements such as:
Functional duplicate checks
Technical duplicate checks
Populating the Transaction Cache
The transaction cache requires some assembly,
but it’s not that complicated to get started!
Here’s what you need to do:
1. Identify the transaction types to persist
You must identify what you wish to persist into the cache.
The PersistentTransactionCacheService has a generic type T
which you can use to insert any sort of MongoDB persist-able POJO.
In our example we want to persist this payment object:
    public static class Payment {
        private final String from;
        private final String to;
        private final BigDecimal amount;
        private final LocalDate paymentDate;
    }
2. Select business data that’s important
We need to implement an "ID extractor" to determine which fields are important when determining if we’ve seen this transaction before.
Some examples might be:
Amount
End-to-end ID
From Identifier
To Identifier
Unstructured data
This is implemented as a Function<T, List<String>>.
This gives the service a way to extract the relevant fields and hash them together to efficiently attempt to look them up later.
Here’s an example of how to initialise the transaction cache service:
        var transactionCacheService = new PersistentTransactionCacheService<>(
                payment -> List.of(payment.getFrom(), payment.getTo(), payment.getAmount().toString())
                , repo, repositoryRetryProvider);
The first argument consists of the list of fields (which must be protected against nulls!) which form part of the hash.
The particular implementation of cache service we have selected here is MongoDB-based, so the second argument takes a repository for storing cache entries.
Different implementations may have different signatures depending on their requirements.
1. Create an entry type enum
The cache can potentially contain different types of transactions.
For that reason we need to be able to enumerate the different types.
This is represented by the TransactionCacheEntryType interface.
Here’s its definition:
public interface TransactionCacheEntryType {
    String getName();
}
We can see that it’s really just a way to be able to differentiate between different types of transactions being cached.
We need this because some transaction flows can share the same root message type (think incoming and outgoing messages of the same type e.g. pacs.008).
Here’s an example implementation of a TransactionCacheEntryType:
    public enum ExampleTransactionCacheEntryType implements TransactionCacheEntryType {
        TYPE_ONE,
        TYPE_TWO;
        @Override
        public String getName() {
            return name();
        }
    }
This is an enum which implements the TransactionCacheEntryType interface and can support two different types of cache entries: TYPE_ONE and TYPE_TWO.
We can then use the service to persist our types to persist.
3. Wrap and save
We can now call the transaction cache service to save our Payment with its type like this:
        var payment = new Payment("Me", "You", new BigDecimal("4.20"), LocalDate.now());
        transactionCacheService.saveToCache(TYPE_ONE, payment)
                .toCompletableFuture()
                .join();
In case we want to prevent saving two entries for the same physical message (e.g. in case of a retry), we should call saveToCache method with messageId parameter.
MessageId should be a unique identifier for a content we are storing in cache.
New entry will not be saved if entry with the same hash and messageId already exists in the cache for the given type, and it will return existing one instead.
        var payment = new Payment("Me", "You", new BigDecimal("4.20"), LocalDate.now());
        transactionCacheService.saveToCache(TYPE_ONE, payment, "messageId")
                .toCompletableFuture()
                .join();
We save it with the transactionCacheService.
The join() call is for testing; we should not block while this operation completes.
Checking the Transaction Cache
The cache service has the following method for retrieving data from the cache:
    CompletionStage<List<TransactionCacheEntry<T>>> findInCache(TransactionCacheEntryType type, T content);
It needs the type of entry you wish to find, followed by the T type you wish to check to see if it’s a functional duplicate.
It returns a future containing a list of matching cache entries.
You may wish to inspect their creationDate to check for functional duplicates within some window of time.
Implementation Considerations
Housekeeping
For the MongoDB implementation, consider using a MongoDB TTL index
on the creationDate field to expire entries.
An index for searching by hash is created by default, but you may wish to add this TTL index to expire (delete) entries after a specific period of time if they are no longer required.
Application Configuration
Application Health Indicators
------------------------------
Application Health Indicators
IPF flow starter provides a spring backed health actuator, this is responsible for updating the health endpoint of the application.
The health of the app is built up from existing spring health indicators, i.e. mongo, diskspace, etc, plus the health of the connector transports.
Connector Transports
The IPF connectors emit one of TransportAvailable, TransportUnavailable, TopicUnavailable or CircuitBreakerStateChanged which are then converted into a health state by the ConnectorEventProcessor.
Health Reports
Reports the general health of the app by aggregating all health indicators.
localhost:8081/actuator/health
{
   "status":"UP",
   "components":{
      "akkaClusterMembership":{
         "status":"UP"
      },
      "connectors":{
         "status":"UP",
         "details":{
            "FraudSendKAFKA":{
               "connectorName":"Fraud",
               "status":{
                  "status":"UP"
               },
               "circuitBreakerState":"CLOSED",
               "circuitBreakerClosed":true
            },
            "KafkaReceiveConnectorTransport":{
               "connectorName":"ExecutePaymentRequestReceive",
               "status":{
                  "status":"UP"
               },
               "circuitBreakerState":"CLOSED",
               "circuitBreakerClosed":true
            },
            "KafkaSendConnectorTransport":{
               "connectorName":"ExecutePaymentSend",
               "status":{
                  "status":"UP"
               },
               "circuitBreakerState":"CLOSED",
               "circuitBreakerClosed":true
            },
            "FraudReceiveConnectorTransport":{
               "connectorName":"FraudReceive",
               "status":{
                  "status":"UP"
               },
               "circuitBreakerState":"CLOSED",
               "circuitBreakerClosed":true
            }
         }
      },
      "diskSpace":{
         "status":"UP",
         "details":{
            "total":368110161920,
            "free":78524358656,
            "threshold":10485760,
            "exists":true
         }
      },
      "livenessState":{
         "status":"UP"
      },
      "mongo":{
         "status":"UP",
         "details":{
            "version":"4.4.4"
         }
      },
      "ping":{
         "status":"UP"
      },
      "readinessState":{
         "status":"UP"
      }
   },
   "groups":[
      "liveness",
      "readiness"
   ]
}
If we don’t enable health-check it will raise TransportAvailable by default.
Application Configuration
To enable spring endpoints add the following enablers to your application config.
management.endpoint.health.probes.enabled=true
management.health.livenessState.enabled=true
management.health.readinessState.enabled=true
Liveness
Reports whether the app is alive.
localhost:8081/actuator/health/liveness
{
"status": "UP"
}
Readiness
Reports whether application is ready to process data.
This relies on the transports being in the spring context otherwise the readiness of those transports is not checked.
localhost:8081/actuator/health/readiness
{
"status": "OUT_OF_SERVICE"
}
Transaction Caching
Flow Scheduling
------------------------------
Flow Scheduling
The ipf-flo-scheduler module provides the ability to interact with IPF flows and activate the scheduling capabilities.  There are three types of scheduling features currently available within a flow:
Action Timeouts: provides the ability for the flow to continue processing if a response from an action is not provided in a defined timeframe.
Retries: this provides a mechanism for the flow to be able to invoke retries of actions when responses are not returned in defined timeframe.
Processing Timeouts: this provides a mechanism to define that a section of flow (between any two states) has to be completed within a defined timeframe.
There are then two different types of scheduler to provide the implementations of these depending on requirements:
Akka scheduler
Persistent scheduler
To use a scheduler, it is simply necessary to import the relevant scheduler as defined below and then inject the scheduler port when instantiating the domain as such:
@Bean
public QuickStartModelDomain initialiseDomain(ActorSystem actorSystem, SchedulerPort schedulerAdapter) {
    // All adapters should be added to the domain model
    return new QuickStartModelDomain.Builder(actorSystem)
                .withSchedulerAdapter(schedulerAdapter)
                .build();
}
The relevant scheduler implementation will then provide the required implementation.
Akka Scheduler
All documentation on akka scheduling can be found here.
The scheduler in Akka is designed for high-throughput of thousands up to millions of triggers.  The prime use-case being triggering Actor receive timeouts, Future timeouts, circuit breakers and other time dependent events which happen all-the-time and in many instances at the same time.
The Akka scheduler is not designed for long-term scheduling and for that you should use Quartz scheduler.
Nor is it to be used for highly precise firing of the events.
The maximum amount of time into the future you can schedule an event to trigger is around 8 months, which in practice is too much to be useful since this would assume the system never went down during that period.
If you need long-term scheduling we highly recommend looking into alternative schedulers, as this is not the use-case the Akka scheduler is implemented for.
To use the akka scheduler implementation, all you need to do is provide the dependency:
<dependency>
  <groupId>com.iconsolutions.ipf.core.platform</groupId>
  <artifactId>ipf-flo-scheduler-akka</artifactId>
</dependency>
Persistent Scheduler
The persistent scheduler implementation uses Icon’s ipf-persistent-scheduler as it’s underlying scheduler implementation.
All documentation on Quartz scheduling can be found here.
Quartz is a richly featured, open source job scheduling library that can be integrated within virtually any Java application.
If your application has tasks that need to occur at given moments in time, or if your system has recurring maintenance jobs then Quartz may be your ideal solution.
In addition to Quartz scheduling, IPF also supports persisting scheduled jobs.
In case of system crash, when it restarts, the system is able to rehydrate still active jobs.
To use the quartz scheduler, simply add its dependency:
<dependency>
  <groupId>com.iconsolutions.ipf.core.platform</groupId>
  <artifactId>ipf-flo-scheduler-persistent</artifactId>
</dependency>
In addition, for quartz scheduling it is necessary to provide a datastore for persistance.  The following configuration show’s how to do this:
ipf-scheduler {
  mongodb = {
    uri = "mongodb://localhost/ipf"
    database-name = "scheduled"
  }
}
Akka Scheduler Vs Quartz Scheduler
Scheduler Type
Pros
Cons
Akka
Short-term events that should last seconds to minutes
- Perhaps the name "Scheduler" was unfortunate, "Deferer" is probably more indicative of what it does.
- The Akka Scheduler is designed to setup events that happen based on durations from the current moment: You can say "fire this job in 15 minutes, every 30 minutes thereafter" but not "fire a job every day at 3pm".
- Akka’s default scheduler is executed around a HashedWheelTimer – a potential precision loss for jobs, as it does not provide strong guarantees on the timeliness of execution.
- Scheduled jobs get lost when the system restarts
Quartz
- Jobs are scheduled to run when a given Trigger occurs. Triggers can be created with nearly any combination of the following directives.
At a certain time of day (to the second), on certain days of the week, and so on.
- Jobs are given names by their creator and can also be organized into named groups. Triggers may also be given names and placed into groups, in order to easily organize them within the scheduler.
Jobs can be added to the scheduler once, but registered with multiple Triggers.
More complex for short "schedule once" jobs
Configuration
Action Timeouts
HOCON configuration can be provided (usually in Akka application.conf) to configure the time-out duration of each action.
When the duration has expired, the flow will receive an Action Timeout event for that configured Action.
The format of the config items are currently in flux and subject to change
The current format for the configuration is
FlowName.STATE_NAME.ActionName.Type=[Duration|Integer]
FlowName : flow identifier or Any for wildcard
STATE_NAME : State identifier or Any for wildcard
ActionName : Action identifier or Any for wildcard
Type : one of timeout-duration, initial-retry-interval, or max-retries.
Where Duration is any format supported by HOCON
Specific example
OBCreditTransfer.VALIDATING_SCHEME_RULES.ValidateAgainstSchemeRules.timeout-duration=2s
This equates to:
The Validate Against Scheme Rules action in the VALIDATING_SCHEME_RULES state in the O B Credit Transfer flow will time-out if not responded to in 2 seconds
Each part in the config also supports the Any keyword which will match on anything for that given part. It is
applicable to flows, states and actions.
Any example
Any.Any.ValidateAgainstSchemeRules.timeout-duration=10s (1)
Any.CheckingFraud.CheckFraud.timeout-duration=20s  (2)
1
The Validate Against Scheme Rules action in the any state in any flow will time-out if not responded to in 10 seconds
2
The Check Fraud action in the Checking Fraud state in any flow will time-out if not responded to in 20 seconds
Backoff Types and Jitter
The configuration allows for determining different backoff types:
EXPONENTIAL: 2^n scaling (where n is the initial delay). This is the default type.
LINEAR: 2n scaling (where n is the initial delay)
USER_DEFINED: Custom intervals that are defined in configuration
Jitter is enabled by default, but the configuration also allows for disabling jitter in the case that retries are so large that jitter would add a
significant amount of delta. Imagine a retry happening in a day’s time, the jitter for that would be +/- 5 hours each way.
This might not be desirable.
So - for example - to configure 5 attempts of the CheckFraud action with no jitter, initially retrying after 10 seconds but then every 30 minutes, the configuration would be:
Any.CheckingFraud.CheckFraud.max-retries = 4
Any.CheckingFraud.CheckFraud.backoff-type = "USER_DEFINED"
Any.CheckingFraud.CheckFraud.custom-backoffs = [10s,30m] (1)
Any.CheckingFraud.CheckFraud.jitter-enabled = false
1
Using the HOCON duration format
Another example here shows a linear retry (with jitter present so omitted because on by default)
Any.CheckingFraud.CheckFraud.initial-retry-interval = 1000
Any.CheckingFraud.CheckFraud.max-retries = 4
Any.CheckingFraud.CheckFraud.backoff-type = "LINEAR"
This example will retry at 1/2/3/4 seconds.
Precedence
The most specific configuration takes precedence i.e. if it matches on all 3 parts (flow, state and action).
For actions, when there are multiple configurations items that might apply, the more specific state will override the
more specific flow configuration.
Example action:
Flow: Flow1
State: State1
Action: Action1
This would be the order of precedence of all the possible configurations that might apply to this action
1. Flow1.State1.Action1.timeout-duration
2. Any.State1.Action1.timeout-duration
3. Flow1.Any.Action1.timeout-duration
4. Any.Any.Action1.timeout-duration
5. Flow1.State1.Any.timeout-duration
6. Any.State1.Any.timeout-duration
7. Flow1.Any.Any.timeout-duration
8. Any.Any.Any.timeout-duration
Processing Time
Durations
HOCON configuration can also be provided (usually in Akka application.conf) to configure how much time is allowed to be spent in passing between a pair of states, irrespective of the journey taken to reach the destination.
When the duration has expired, the flow will receive an ProcessingTimeElapsedEvent for the destination state.
The current format for the configuration is
ipf.flow.<flow-name>.processing-time.<source-state>.<destination-state>.timeout-duration=Duration|Integer]
flow-name : flow identifier
source-state : State identifier
destination-state : The state the flow should reach in the allotted time
Where Duration is any format supported by HOCON
Specific example
ipf.flow.OBCreditTransfer.processing-time.ValidatingAgainstSchemeRules.Complete.timeout-duration=2s
This equates to:
The flow will produce a processing time elapsed event if the time taken from the ValidatingAgainstSchemeRules state to the Complete state exceeds 2 seconds
Offsets
Offsets provide an enhanced mechanism of defining the timeout duration.  The basic processing time looks simply at the time between two states.  However, it may be necessary for the timeout to consider for example a specific customer defined start time.  In this case we are able to use offsets as a way of enriching the duration.
For example, suppose that the client provides an accepted timestamp and that the flow must complete within 5 seconds of that time - rather than 5 seconds after the flow initiating.  In this case, IPF can be provided with the accepted timestamp as an offset and then define a 5 second duration, but the actual timeout will be fired by allowing for the offset.
There are two types of offset:
System Offsets: each time IPF reaches a new state, a new offset is created.
Custom Offsets: these are user defined and can be provided to IPF.
Each offset has two attributes - a unique offset-id and the offset time itself.
To use an offset in the scheduling configuration, it is simply necessary to define in the configuration the offset-id to use. This is done by:
ipf.flow.OBCreditTransfer.processing-time.ValidatingAgainstSchemeRules.Complete.offset-id=anOffsetId
To provide a custom offset to any flow, we simply need to provide it on the input via the withFlowOffset method.
It is also possible to provide the offsets from one flow to another.  This is useful in parent-child relationships where there is a requirement for an offset to span multiple flows.  To do this, on all actions the offset map of the current flow is provided as a parameter and this can simply be passed to the new flow via it’s withOffsetMap method.
Application Health Indicators
Automated Retries
------------------------------
Automated Retries
Recovery of the 'client implementation' can take shape in one of two ways;
Action retries - retry action after if transaction’s state has not changed in X seconds
Action revivals - retry action on newly started cluster shard, and using an exponential backoff starting with the initial duration of X seconds any transactions in a non-terminal state will be retried.
Action Retries
Action retries are used to prevent transaction’s remaining stuck in a state, by issuing retries if an action does not change state within an acceptable (configurable) duration.
Interaction with Action timeouts
For action timeouts see Scheduling.
Timeouts would usually result in a new state (Terminal) and therefore would not be subjected to retrying.
When timeouts cause a new state (non-terminal) then a retry would be attempted on the ActionTimeout if it remains stuck in its state.
Action Retry Configuration
The configuration utilises the configuration policy of the ipf-scheduler (see Scheduling for configuration and action timeouts).
There are 2 configuration items necessary for action retries;
initial-retry-interval - the initial duration between retries, subsequent retries multiplied by a backoff factor of 2, i.e. if duration is 1 then 1,2,4,8.
max-retries - the maximum number of retries to attempt, i.e. [initial] + [max-retries].  0 retries will effectively turn this functionality off.
jitter-factor - The percentage of randomness to use when retrying actions, default is 0.2.
For all Actions
application.conf
Any.Any.Any.timeout-duration=10s
Any.Any.Any.initial-retry-interval=3s
Any.Any.Any.max-retries=2
Any.Any.Any.jitter-factor=0.2
For Specific Action
application.conf
Any.Any.Any.timeout-duration=10s
Flow1.State1.Action1.initial-retry-interval=3s
Flow1.State1.Action1.max-retries=2
Using the above configuration would create the following effect for Action1;
The following assumes ActionTimeout will lead to a terminal state, or at least a change of state.
Time (t+seconds)
State
Action
0
State1
Action1
3
State1
ActionRetry (Action1)
6
State1
ActionRetry (Action1)
10
Timeout (or whatever state ActionTimeout causes)
ActionTimeout
Action Revival
Action revival is designed to recover transactions on a failed node.  They differ from Action retries in the fact that they only fire when the cluster is started or re-started, a scenario not covered by the Action retries.
Revival will utilise action retries and continue from any retry attempt history, i.e. if a behaviour had already attempted 1 of say the configured 2 attempts then only 1 retry will be attempted.
The revival process will not attempt to recover a transaction in INITIAL or any terminal states.  This was to protect the system from attempting to recover on all newly started shards.
The revival process will not attempt revival if the state has changed before the actionRevivalOffset (see configuration) has complete, as the transaction will no longer be deemed stuck.
Action revival is based upon Akka recovery signals this means that a recovery of state will occur when any of the following happen;
An Event Sourced Behaviour (ESB) is initialised for the first time
An ESB is revived after having been passivated (happens automatically after 120s by default)
An ESB was killed by an exception
An ESB is rebalanced and therefore restarted on another node
Action Revival Configuration
There are 2 important configurations required to activate revival;
remember-entities - an Akka configuration item which causes Akka to automatically restart shards upon a shard restart.  see akka docs
action-recovery-delay - an offset configured as duration which is imposed upon the system to allow any actions to change state before sending additional requests.  An offset of 0 will turn this functionality off.
appliation.conf
akka.cluster.sharding.remember-entities = on
application.properties
ipf.behaviour.config.action-recovery-delay=3s
Flow Scheduling
Monitoring and Observability
------------------------------
Monitoring and Observability
The strategy on IPF for monitoring and observability is achieved by utilising an event-driven paradigm, strict categorising of application behaviour, and extensibility for exposing this data through appropriate channels, with the best tooling.
Monitoring of IPF application services can be done in three primary ways:
HTTP APIs
Time-series metrics via Prometheus and Grafana
Application logging
Out of Scope:
Application performance monitoring (APM) - This is something
to consider if such software is available at the customer’s site. APM software such as Dynatrace or AppDynamics can help
diagnose potential problems before they materialise.
Infrastructure Monitoring e.g. Brokers, containers
Definitions
The "IPF application" is actually comprised of multiple runtime software packages. This section will describe the
terminology that will be used herein:
Component
Sometimes known as
Description
Needs to be in Akka cluster with other similar nodes?
Payment Services (Customised)
Write side
The set of payment flows that the client has defined.
There can be multiple of these representing different sets of
flows (e.g. credit transfer, recall, value-added service).
Yes
CSM Service
CSM Pack, Scheme Pack
An adapter for a payment provider such as an payment scheme, wallet, etc.
No
Support Services
Notification Service
Additional processing of events to third party systems.
Yes
Individual Service APIs (HTTP)
There are certain HTTP APIs that are enabled by default which can be interrogated when the application is running, each
serving a specific purpose.
Here’s a summary of those APIs and the configuration items that set their hostnames and ports:
What is it?
What does it do?
Example use case
API reference (or similar)
Default hostname
Default port
Host override env var
Port override env var
Available on
Spring Boot Actuator API
Spring Boot style metrics for the JVM, Spring config properties, beans, health
To verify the version of active libraries
Click here
0.0.0.0
8080
MANAGEMENT_SERVER_ADDRESS
MANAGEMENT_SERVER_PORT (set to -1 to disable)
Client Implementation(s)
Scheme Connector
Akka Management Cluster HTTP API
Information on the currently running Akka cluster for debug purposes
To verify cluster state and manually manage nodes
Click here
(the result of InetAddress.getLocalHost().getHostAddress() which is usually - but not always - 127.0.0.1)
8558
AKKA_MANAGEMENT_HTTP_HOSTNAME
AKKA_MANAGEMENT_HTTP_PORT
Client Implementation(s)
Spring Boot Actuator API
Use this API for inquiring on the Spring ApplicationContext that’s currently running on this particular node. Some
interesting Spring Actuator endpoints for IPF:
conditions: Check that the write and read side of the application have been configured on the relevant nodes
correctly
env: Display environment variables to check overrides are correct
health: Useful for liveness probes and the like
info: General application info (also useful for liveness probes)
More endpoints are available. Visit the Spring Boot Actuator link in the table above to see all the details. Also
please note this
particular section on how to enable and disable particular Actuator endpoints (MANAGEMENT_ENDPOINT_X_ENABLED where X
is the relevant Actuator part).
For information on how to configure TLS for the Actuator endpoints, see this section.
Akka Cluster HTTP Management
This API allows the user to interact with IPF’s underlying Akka cluster using an HTTP interface.
The primary use for this API is to verify the cluster state. IPF client implementation components require that all
"write" nodes serving the same set of flows (e.g. credit transfer, recall, digital wallet payout) are in an Akka cluster
together. If this is not the case, no new work will be taken to avoid losing transactions.
The cluster finds other similar nodes by itself, but in case the application appears to not be consuming any new work,
this should be the first port of call to ensure that the cluster is in a valid state and that the application is not in
a "split brain" (multiple nodes being created in separate clusters).
Split brain situations can be resolved using the Akka Split Brain Resolver. More information is available on this topic
on the Akka website.
The Akka Cluster HTTP Management endpoint also allows for update operations. If this behaviour is not desired, set the
AKKA_MANAGEMENT_READ_ONLY environment variable to true to enable read-only mode where cluster information can only
be retrieved but not updated.
TLS settings for Akka Management are the same as those for Spring Boot, but the server or management.server prefixes
are replaced with akka.management. So for example to set the keystore path, the property would be
akka.management.ssl.keystore or AKKA_MANAGEMENT_SSL_KEYSTORE (the Spring Boot equivalent being server.ssl.keystore
or SERVER_SSL_KEYSTORE).
Time-Series Metrics
Metrics are exposed via a Prometheus HTTP server which is interrogated over some set interval by Prometheus, and
visualised using tools like Grafana and Kibana.
The AlertManager component performs configurable aggregation of events based on thresholds and conditions to translate this system behaviour into something that may need action for an Operator.
Spring Boot Dashboards
Basic JVM metrics via the prometheus Spring Boot Actuator endpoint. There are several dashboards
that can be used to visualise this data, but we recommend this one from the
Grafana dashboard collection.
Akka Dashboards
There are some out-of-the-box Grafana metrics that are available for Akka, documented and available for download
here.
The interesting Akka dashboards to look out for - as far as IPF is concerned - are:
Event Sourced Behaviours: Metrics on events and command being processed by IPF flows
Akka Streams and Akka Streams (extended): Connector stream processing performance metrics
IPF Specific Dashboards
There are also some custom IPF-specific dashboards which are available:
Name
Description
Required Data
IPF Connectors
Per-connector statistics on numbers of requests sent, received, and average response times per connector.
IPF Flow Timings
Per-flow statistics on how long flows are taking to be processed from initiation until they reach a final state.
IPF Resilience4j
Reports statistics on the connector circuitbreakers using metrics defined in Resilience4J.
IPF JVM Overview
Provides a few statistics on the JVM, for example heap memory, per pod in the cluster. For a more detailed dashboard see IPF JVM (Actuator)
IPF CSM Simulator
Reports on any the request/response metrics of CSM simulators that are deployed in the cluster.
Requires a namespace label in Prometheus
IPF Node Degradation
Reports on a wide range of application service metrics (MongoDB/Akka…​) to determine the overall health of the IPF ecosystem, and help identify if there has been any degradation of performance over time.
IPF ODS Ingestion
Statistics relating to the ODS ingestion service reporting on any lag between ingestion of data as well as metrics relating to Kafka/MongoDB.
Pod name is assumed to be ods-ingestion.
Kafka consumer group for ingress is assumed to be called ipf-processing-data-ingress-consumer-group.
Kafka egress topic is assumed to be called IPF_PROCESSING_DATA.
IPF Transaction Status
Reports on statistics based on flow metrics to show transactions in completed or non-completed states
Completion state name is assumed to end with the word Completed or else all events will end in the Rejected panel
IPF ESB Actions & Inputs
Reports on Event Sourced Behaviours (ESBs) action and input metrics providing the rate of execution between these functions.
IPF JVM (Actuator)
Provides detailed statistics on the JVM from metrics provided by the Spring actuator.
IPF Requests Info
Provides overall and per-connector metrics on request and response timings.
Requires a namespace label in Prometheus
Exporter Configuration
Spring Boot Actuator metrics are exposed via the same Actuator HTTP server documented above. See above for how to change
the Actuator host and port.
Both Akka and IPF-specific Prometheus metrics are available on the same Prometheus exporter web server, which by default
is configured to listen on all interfaces (0.0.0.0) and port 9001. To change these details, the relevant environment
variables are:
CINNAMON_PROMETHEUS_HTTP_SERVER_HOST
CINNAMON_PROMETHEUS_HTTP_SERVER_PORT
If these properties are changed, remember to also change the Prometheus side so that Prometheus can collect time-series
data from the correct address(es).
Logging
IPF uses Logback for logging configuration. This allows the user to configure a logging setup which can mirror that of
other applications developed inside the organisation, and make IPF report log data in the same way.
This document will explain some typical logging setups which can be used to output log data in various ways.
For all setups, the Logback configuration file needs to be mounted on the app’s classpath. For an Icon-built
image, this is always available at /[name of container]/conf. So if the container is named credit-transfer, then the
Logback configuration can be mounted at /credit-transfer/conf/logback.xml.
Option 1: Elasticsearch/Logstash/Kibana
A popular stack in the industry is ELK: a combination of Elasticsearch, Logstash (and/or Beats) and
Kibana. It was formerly known as the ELK stack, but with the introduction of Beats, Elastic have been pushing the more
generic "Elastic Stack" naming for this setup.
Either way, the setup looks like this:
First we need to configure Logstash to listen to a TCP port. Here’s an example of how to configure that in Logstash:
input {
    tcp {
        port => 4560
        codec => json_lines
    }
}
This makes Logstash listen on port 4560 for lines of JSON separated by the newline character \n. The Logstash Logback
appender does this for us, and can be configured like so:
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
  <appender name="stash" class="net.logstash.logback.appender.LogstashAccessTcpSocketAppender">
      <destination>127.0.0.1:4560</destination>
      <encoder class="net.logstash.logback.encoder.LogstashAccessEncoder" />
  </appender>
  <appender-ref ref="stash" />
</configuration>
This will configure IPF application logs to be sent to Logstash. Configuring Logstash to connect to Elasticsearch
(and Kibana to Elasticsearch) is out of scope of this document but can be found on the Elastic website here.
More examples, including:
Configuring TLS over the TCP connection, and;
A UDP variant of the appender
Can be found on the Logstash Logback appender’s GitHub page here.
Configuring a Logstash Appender for System Events
If you wish to aggregate IPF system events, consider using the com.iconsolutions.payments.systemevents.utils.Slf4jEventLogger
which forwards all received system events to an appender. This can be used in conjunction with this Logstash appender
to push system events to an aggregator such as Elasticsearch as mentioned above.
Here’s an example Logback config which takes Slf4jEventLogger events and sends them to our STASH appender:
<logger name="com.iconsolutions.payments.systemevents.utils.Slf4jEventLogger" level="DEBUG" additivity="false">
    <appender-ref ref="STASH"/>
</logger>
Option 2: Splunk
Aside from a specific setup such as the one above, a true twelve-factor app should output its logs - unbuffered - to
stdout, and this can be analysed by software such as Splunk.
Splunk provides an HTTP appender for Logback. This is documented here.
That document also outlines some performance considerations for logging with HTTP, and also a TCP appender which can be
used instead of HTTP.
The Logback template referred to in that document can be found here.
There are three mandatory fields:
url: The Splunk URL to forward to
token: The token provided by Splunk for authentication and authorisation
index: The Splunk index (repository) for storing this log data
Option 3: Files (Not Recommended)
Logging to file breaks cloud-native principles about not making assumptions about an underlying file system.
Logs should be treated as data streams instead of files that need maintaining.
Only use this approach as a last resort when it’s absolutely not possible to use a more modern approach for logging.
For more information please see XI. Logs on the Twelve-Factor App.
It is possible to specify a normal Logback file appender. A typical logback.xml might look like:
logback.xml
<?xml version="1.0" encoding="UTF-8" scan="true"?>
<configuration>
    <jmxConfigurator />
    <appender name="FILE"
              class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>/opt/ipf/logs/credit-transfer-service.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy">
            <fileNamePattern>/opt/ipf/logs/credit-transfer-service.log.%i</fileNamePattern>
            <minIndex>1</minIndex>
            <maxIndex>20</maxIndex>
        </rollingPolicy>
        <triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy">
            <maxFileSize>50MB</maxFileSize>
        </triggeringPolicy>
        <encoder>
            <pattern>%date{yyyy-MM-dd} %d{HH:mm:ss.SSS} %-5level %X{traceId} %logger{36} %X{sourceThread} %X{akkaSource} - %msg%n</pattern>
        </encoder>
    </appender>
    <root level="INFO">
        <appender-ref ref="FILE" />
    </root>
</configuration>
This file creates the following configuration:
The logfile is parsed for live changes every minute (scan="true")
Logging configuration can be modified live with JMX. More information on this is available here
File logging to /opt/ipf/logs/credit-transfer-service.log
Files rolled over to /opt/ipf/logs/credit-transfer-service.log.n, where n is a number between 1-20, and is rolled
over when credit-transfer-service.log reaches 50 MB. Note that only 20 such files (i.e. a total of 1 GB of log
data) will be kept
This file can also be scraped by a framework such as Splunk forwarder or
Beats as
shown in the diagram below:
System Events
What are IPF System Events
IPF applications are underpinned by a System Event framework, providing pub/sub capabilities between application components.
Hierarchical, extensible, versioned, catalogued. All application areas: Technical, Functional & Lifecycle. All events include common properties: Source location, Creation time, Level, Type, association context, simple to get all System Events for a given payment
The fundamental data framework for capturing application behaviour, providing a place to build extensible event-driven functionality, which can then be feed into supporting tasks such as providing data for monitoring and alerting.
System Event processors subscription pattern : "Act on all WARN level Infrastructure System Events”
Configure multiple processors, functional style, store /transform – emit over a Connector
All the System Events that an IPF Service may emit are catalogued and have a schema, they are versioned against the deployed software
New client-specific events are encouraged to be added for a given Solution. Providing a very clean extension for leverage the existing framework and providing additional data insights
Where they are Defined
The full list of system events that IPF produces are listed here
Troubleshooting Error Messages
The events in this table are ERROR events logged by IPF and describe
recommended remediation steps to follow if encountered.
Automated Retries
Getting Started
------------------------------
Getting Started
Create a new IPF Application Project
Defining The Read Side
Monitoring and Observability
Create a new IPF Application Project
------------------------------
Create a new IPF Application Project
To create a brand new IPF Application project we advise using the Archetype starter as this is the quickest way to bootstrap a new project using the IPF Toolkit. The Maven archetype will generate a Maven project (using the Icon BOM as its parent) containing;
an application skeleton (home for the application runtime code)
a flow domain (artefacts related to the DSL and flow definitions)
a test module (containing the docker application configuration)
From this base you can then wire in any additional capability you need, either from the Flo Starter Projects or other IPF component libraries.
Getting Started
Defining The Read Side
------------------------------
Defining The Read Side
The ipf-read-starter provides a base mongo setup that can be extended for use in implementations.
The read side supports multiple different flows running with a single read side.
The key to the read side setup is the abstract 'MongoReadModelEntity' class.
It provides:
persistenceId - the persistence id (the IPF ID e.g. Payoutflow|ABC)
originatingId - the originating id of the flow (e.g. UPRID)
originatingMessageType - the originating message type (e.g. ExecutePaymentRequest)
firstEventTime - the time the first event was record on the transaction
lastProcessedEventTime - the time the most recent event was record on the transaction
lastEventName - the most recent event name
initiatingTimestamp - when the flow as initiated.
alternativeIds - a list of alternative ids that the transaction may be known by
eventSummaries - a list of event summary details for all the events on the transaction
payload - the payload for the transaction, as per the api (for example could be CreditTransferPayload, ReturnsPayload etc.)
The implementation of this class, if application specific as it depends on being populated by data received on custom events.
For this reason, it is necessary for the application to provide the concrete implementation.
To do this, the following methods will need to be implemented:
    protected abstract Supplier<T> payloadCreator();
    protected abstract BiFunction<Event, T, T> payloadUpdater();
payloadCreator - this is required to provide a new instance of the payload type.
payloadUpdater - this is required to determine how the given event should update the current payload.
originatingMessageTypeSupplier - this is required to specify the name (from the message log) of the initiating message for the flow.
In addition to the above 3 methods, there are a number of methods that have default implementeds that can be overriden:
    protected Set<String> getAlternativeIdsFrom(Event event) {
        return Collections.emptySet();
    }
getAlternativeIdsFrom - by defaulting returning an empty collection, this method should extract any id’s that should be used for searching on.
getInitiatingTimestampFrom - by default this uses the createdAt time of the first event received, but can be overriden to use the data from any event.
getOriginatingIdFrom - by default this uses the commandId of the first event received, but can be overriden to use the data from any event.
Once the concrete implementation of the MongoReadModelEntity has been created, it needs to be registered by creating a factory that will determine which implementation should be used based on the IPF ID that is received.
public interface ReadModelEntityFactory {
    ReadModelEntity create(String persistenceId);
}
com.iconsolutions.ipf.platform.read.transaction This needs to be registered as a spring bean.
Example
The following shows an example implementation of the MongoReadModelEntity.
@Data
@AllArgsConstructor
public class TestCreditTransferTransactionReadModel extends MongoReadModelEntity<CreditTransferPayload> {
    @Override
    protected Supplier<CreditTransferPayload> payloadCreator() {
        return () -> CreditTransferPayload.builder().build();
    }
    @Override
    protected BiFunction<Event, CreditTransferPayload, CreditTransferPayload> payloadUpdater() {
        return (evt, returnPayload) -> {
            if (evt instanceof TestFIToFICustomerCreditTransferEvent) {
                returnPayload.setCreditTransfer(((TestFIToFICustomerCreditTransferEvent) evt).getCreditTransfer());
            }
            return returnPayload;
        };
    }
}
To register this we create the factory:
@Slf4j
public class ReadModelEntityTestFactory implements ReadModelEntityFactory {
    @Override
    public ReadModelEntity create(String persistenceId) {
        log.debug("PersistenceId found on read side: {}", persistenceId);
        return new ReadSideFlowTestModel();
    }
}
Create a new IPF Application Project
How to handle schema evolution in IPF events
------------------------------
Handle Schema Evolution in IPF Events
Over the life of an IPF flow, changes will be made to the events that are persisted by that flow. Changes to flows
can happen for many reasons, such as:
Better business understanding of the flow leading to a change in requirements
Design-time assumptions not being correct and therefore requiring a change to the initially agreed-upon events
Regulatory reasons for needing to capture more data in an event
The technical nature of these changes can vary, but they are generally variants on the below patterns, which will also serve as the subheadings for this guide.
The patterns for schema evolution are:
Adding a data field to an event
Changing the fully-qualified class name (FQCN) of an event
Changing the name of a data element in an event
Changing the data type of an element in an event
Splitting an event into multiple, smaller events
Removing an event
The process for updating events is out of scope of this document. Follow the DSL portion of the tutorial for event
definition and modification.
These event migrations should only be implemented if the flow itself is not changing.
If the flow itself is also changing in addition to its events, then consider creating a new version of the flow
altogether. See the DSL portion of the tutorial for how to define multiple versions of flows
Adding a Data Field to an Event
Consider v1 of an event being as follows:
public class UserRegistered extends DomainEvent {
    private final String name;
}
This would be serialised in JSON as:
{"name": "jeff"}
If the new data being added is simply a new optional field, like this:
package com.myorg.events;
import java.time.LocalDate;
public class UserRegistered extends DomainEvent {
    private final String name;
    private final LocalDate dob;
}
The old serialised version will still be successfully parsed as a UserRegistered, and no change is required. But of course the dob field will be null.
However, business rules may not allow the dob field to be null, but there may be a special placeholder date of birth
0001-01-01 that you can use to indicate that the lack of availability of a date of birth for this user. In which case,
this JacksonMigration will check for dob, and - if absent - will set it to the default value before it is
deserialised as UserRegistered:
package com.myorg.migrations;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.node.ObjectNode;
public class DobFillerMigration extends JacksonMigration {
    private static final String DOB_FIELD_NAME = "dob";
    @Override
    public int currentVersion() {
        return 2;
    }
    @Override
    public JsonNode transform(int fromVersion, JsonNode json) {
        if (!json.has(DOB_FIELD_NAME)) {
            ((ObjectNode) json).set(DOB_FIELD_NAME, "0001-01-01");
        }
    }
}
And then bound to the UserRegistered event by adding it to the IPF configuration like so:
akka.serialization.jackson.migrations {
  "com.myorg.events.UserRegistered" = "com.myorg.migrations.DobFillerMigration"
}
Remember, if the new field is allowed to be null, no migration needs to be written.
Changing the fully-qualified class name (FQCN) of an event, or renaming an event
This cannot be remedied by fixing a JacksonMigration, since JacksonMigration works on the body and not the type.
However, if nothing else has changed apart from the fully qualified classname (or just the name) of the event, and the Icon
MongoDB Akka Persistence plugin is being used, this can be remedied using a MongoDB update statement.
As an example, if the package name of the events was misspelt as com.myorg.evnets and we want to correct the typo, the
following update statement will change the package name of all com.myorg.evnets events to com.myorg.events:
const updates = [];
db.journal.find({"eventPayloads.payload.type":/com.iconsolutions.instantpayments/})
    .forEach(doc => {
        doc.eventPayloads.forEach(pld => {
            const oldFQCN = pld.payload.type;
            const newFQCN = oldFQCN.replace("com.iconsolutions", "com.monkey");
            updates.push({"updateOne": {
                "filter": {$and: [{"_id": doc._id}, {"eventPayloads.payload.type":oldFQCN}]},
                "update": {$set: {"eventPayloads.$.payload.type": newFQCN}}
            }})
        })
    });
print(updates);
// uncomment after this line to actually run the update
// const result = db.journal.bulkWrite(updates);
// print(JSON.stringify(result));
Some things to note about the snippet above:
The default name of the journal is journal but this may be overridden with iconsolutions.akka.persistence.mongodb.journal-collection
Changing the Name of a Data Element in an Event
This can also be resolved by writing a JacksonMigration.
Consider v1 of the event being:
import java.time.LocalDate;
public class UserRegistered extends DomainEvent {
    private final String name;
    private final LocalDate dob;
}
This would be serialised in JSON as:
{"name": "jeff", "dob": "1985-01-01"}
But you decide that dob might not be so clear and decide to rename it to dateOfBirth:
package com.myorg.events;
import java.time.LocalDate;
public class UserRegistered extends DomainEvent {
    private final String name;
    private final LocalDate dateOfBirth;
}
Write the following migration to rename dob to dateOfBirth:
package com.myorg.migrations;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.node.ObjectNode;
public class DobRenameMigration extends JacksonMigration {
    private static final String OLD_FIELD_NAME = "dob";
    private static final String NEW_FIELD_NAME = "dateOfBirth";
    @Override
    public int currentVersion() {
        return 2;
    }
    @Override
    public JsonNode transform(int fromVersion, JsonNode json) {
        if (json.has(OLD_FIELD_NAME)) {
            //get value of dob
            var seqValue = json.get(OLD_FIELD_NAME);
            //set it to new field
            ((ObjectNode) json).set(NEW_FIELD_NAME, seqValue);
            //remove old field
            ((ObjectNode) json).remove(OLD_FIELD_NAME);
        }
        return json;
    }
}
And then bound to the UserRegistered event by adding it to the IPF configuration like so:
akka.serialization.jackson.migrations {
  "com.myorg.events.UserRegistered" = "com.myorg.migrations.DobRenameMigration"
}
Changing the Data Type of an Element in an Event
Changing a type can mean multiple things, such as:
A data element splitting into multiple elements
Moving from a simple type to a complex type
But both are handled in more-or-less the same way, which is writing a JacksonMigration to map the values from the old
version of the event to its new representation.
Continuing with the previous example v1 event:
package com.myorg.events;
import java.time.LocalDate;
public class UserRegistered extends DomainEvent {
    private final String name;
    private final LocalDate dateOfBirth;
}
Imagine the business wants to devolve the name details into a separate Name object:
package com.myorg.model;
import java.time.LocalDate;
public class Name {
    private final String firstName;
    private final String middleName;
    private final String lastName;
}
So the new event looks like this:
package com.myorg.events;
import com.myorg.model.Name;
import java.time.LocalDate;
public class UserRegistered extends DomainEvent {
    private final Name name;
    private final LocalDate dateOfBirth;
}
Assuming the following imaginary migration rule from the business:
If a name contains two tokens, split into first and last name in that order
If a name contains three tokens, split into first, middle and last name in that order
This is not a good way to devolve someone’s name into constituent parts!
Under these rules, the migration would be:
package com.myorg.migrations;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.node.JsonNodeFactory;import com.fasterxml.jackson.databind.node.ObjectNode;
public class NameMigration extends JacksonMigration {
    @Override
    public int currentVersion() {
        return 2;
    }
    @Override
    public JsonNode transform(int fromVersion, JsonNode json) {
        var name = json.get("name").asText();
        var nameNode = JsonNodeFactory.instance.objectNode();
        var nameSplit = name.split("\s");
        if (nameSplit.length == 2) {
            nameNode.set("firstName", nameSplit[0]);
            nameNode.set("lastName", nameSplit[1]);
        } else if (nameSplit.length == 3) {
            nameNode.set("firstName", nameSplit[0]);
            nameNode.set("middleName", nameSplit[1]);
            nameNode.set("lastName", nameSplit[2]);
        }
        ((ObjectNode) json).set("name", nameNode);
        return json;
    }
}
And in the IPF configuration:
akka.serialization.jackson.migrations {
  "com.myorg.events.UserRegistered" = "com.myorg.migrations.NameMigration"
}
Splitting an Event Into Multiple, Smaller Events
This can be implemented using an EventAdapter. The approach is similar to writing a JacksonMigration, but is
implemented after the events have been deserialised.
Consult the Akka docs for more information on this topic: Split large event into fine-grained events
To implement an EventAdapter that will be passed into the EventSourcedBehaviour for the flow, when initiating the domain,
use withBehaviourExtensions to supply a BehaviourExtensions implementation like this:
import java.util.Optional;
public class MyBehaviourExtensions implements BehaviourExtensions {
    @Override
    public Optional<EventAdapter<Event, Event>> eventAdapter() {
        return Optional.of(new EventAdapter<>() {
            @Override
            public Event toJournal(Event event) {
                return event;
            }
            @Override
            public String manifest(Event event) {
                return "";
            }
            @Override
            public EventSeq<Event> fromJournal(Event event, String manifest) {
                //if it's MySuperEvent, devolve it into two smaller events
                if (event instanceof MySuperEvent) {
                    var mse = (MySuperEvent) event;
                    return EventSeq.create(List.of(
                            new MySmallerEvent1(mse.data1()),
                            new MySmallerEvent2(mse.data2())
                    ));
                }
                //otherwise return any other event as-is
                return EventSeq.single(event);
            }
        });
    }
}
and then…​
@EventListener
public void init(ContextRefreshedEvent event, MyBehaviourExtensions myBehaviourExtensions) {
    new MyDomain.Builder(actorSystem)
            .withEventBus(eventBus)
            .withSchedulerAdapter(schedulerAdapter)
            .withSystemAActionAdapter(new SampleSanctionsActionAdapter(sanctionsAdapter))
            .withSystemBActionAdapter(new SampleSanctionsActionAdapter(sanctionsAdapter))
            .withBehaviourExtensions(myBehaviourExtensions) (1)
            .build();
}
1
How to add the BehaviourExtensions implementation
Removing an Event
This is the same as above, but instead emit an EventSeq.empty (the rest of the implementation is the same):
@Override
public EventSeq<Event> fromJournal(Event event, String manifest) {
    //if it's MyUnwantedEvent, pretend we didn't see it
    if (event instanceof MyUnwantedEvent) {
        return EventSeq.empty();
    }
    //otherwise return any other event as-is
    return EventSeq.single(event);
}
Defining The Read Side
How to troubleshoot error messages
------------------------------
Troubleshooting Error Messages
The events in the following table are ERROR events logged by IPF and describe
recommended remediation steps to follow if encountered.
Error events
Name
Description
Level
Remediation
AuthenticationFailed
Published when authentication fails
ERROR
If an Authentication error occurs please check the following:
the logs of the service the connector belongs to for any errors
CorrelationNotFound
Published when an CorrelationId cannot be matched with a UnitOfWorkId
ERROR
If an CorrelationId cannot be found please check the following:
the correlation service is available
the mapping of fields is correct
the field is available in the input message
InboundMessageFailed
Event published when any aspect of the message receive process fails (mapping, receiver function,etc.) - contains the exception that was raised
ERROR
If an Inbound Message Failure occurs please check the following:
the logs of the service the connector belongs to for any errors
OutboundMessageFailed
Published when any error relating to sending of messages is raised (mapping, enqueue, etc.)
ERROR
If an Outbound Message Failure occurs please check the following:
the logs of the service the connector belongs to for any errors
OutboundMessageValidationFailed
Published in case of outgoing message validation failure
ERROR
How to handle schema evolution in IPF events
How to secure IPF HTTP Endpoints
------------------------------
Securing HTTP Endpoints
By default, the IPF APIs do not authenticate inbound requests.
This can be configured in the client implementation by including the spring-boot-starter-security dependency to the project.
The full documentation on Spring Security can be found here.
Example
This section will demonstrate how a client implementation might add validation of JWT access tokens on the endpoints exposed by the ipf-read-starter.
Prerequisites
The read-side client implementation should be implemented and working with all valid requests being accepted.
Add Maven Dependencies
For this example, two dependencies are required.
The first is the aforementioned spring-boot-starter-security module provided by the Spring framework.
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-security</artifactId>
</dependency>
The second is a JWT library that will be used to validate tokens on incoming requests.
<dependency>
    <groupId>com.auth0</groupId>
    <artifactId>java-jwt</artifactId>
</dependency>
Service to Validate JWTs
Next, a service will be created to validate and decode JWTs.
The service requires the secret used to sign the token so that it can verify the token’s signature.
The secret is autowired using the @Value annotation provided by Spring.
@Service
public class JWTService {
    private final String secret;
    @Autowired
    public JWTService(@Value("${ipf.security.jwt.secret}") String secret) {
        this.secret = secret;
    }
    public Optional<DecodedJWT> decodeAccessToken(String accessToken) {
        return decode(secret, accessToken);
    }
    public List<GrantedAuthority> getRoles(DecodedJWT decodedJWT) {
        return decodedJWT.getClaim("role").asList(String.class).stream()
                .map(SimpleGrantedAuthority::new)
                .collect(Collectors.toList());
    }
    private Optional<DecodedJWT> decode(String signature, String token) {
        try {
            return Optional.of(JWT.require(Algorithm.HMAC512(signature.getBytes(StandardCharsets.UTF_8)))
                    .build()
                    .verify(token.replace("Bearer ", "")));
        } catch (Exception ex) {
            return Optional.empty();
        }
    }
}
Authorization Filter Component
When a request is made, it is passed through a chain of WebFilter classes.
Most of the chain is provided by Spring out of the box, though additional filters can be provided by the client.
In this example one such WebFilter will be implemented to validate that requests made to secure endpoints have set the Authorization header with a valid token, otherwise the request should fail (unless another authentication filter is provided to allow requests to be authenticated a different way).
@Component
public class JWTAuthorizationFilter implements WebFilter {
    private final JWTService jwtService;
    public JWTAuthorizationFilter(JWTService jwtService) {
        this.jwtService = jwtService;
    }
    @Override
    public Mono<Void> filter(ServerWebExchange exchange, @NotNull WebFilterChain chain) {
        String header = exchange.getRequest().getHeaders().getFirst(HttpHeaders.AUTHORIZATION);
        if (header == null || !header.startsWith("Bearer ")) {
            return chain.filter(exchange);
        }
        return jwtService.decodeAccessToken(header)
                .map(token -> new UsernamePasswordAuthenticationToken(
                        token.getSubject(),
                        null,
                        jwtService.getRoles(token)))
                .map(token -> chain.filter(exchange)
                        .subscriberContext(context -> ReactiveSecurityContextHolder.withAuthentication(token)))
                .orElse(chain.filter(exchange)
                        .subscriberContext(context -> ReactiveSecurityContextHolder.clearContext().apply(context)));
    }
}
The JWTAuthorizationFilter implements the WebFilter class' single method filter.
It checks that the request’s Authorization header for is prefixed with "Bearer " followed by a valid JWT.
If the header is not set, the filter simply passes the exchange to the next filter without doing anything.
If the token is valid, the filter sets the authentication into the security context.
Otherwise, the filter clears the security context.
Configure the SecurityWebFilterChain
Finally, the SecurityWebFilterChain must be configured as a Spring bean.
@Configuration
@EnableWebFluxSecurity
public class WebSecurityConfig {
    @Bean
    public SecurityWebFilterChain secureFilterChain(ServerHttpSecurity http,
                                                    JWTAuthorizationFilter jwtAuthorizationFilter) {
        return http
                .formLogin(ServerHttpSecurity.FormLoginSpec::disable)
                .httpBasic(ServerHttpSecurity.HttpBasicSpec::disable)
                .csrf(ServerHttpSecurity.CsrfSpec::disable)
                .logout(ServerHttpSecurity.LogoutSpec::disable)
                .authorizeExchange(spec -> spec
                        .pathMatchers("/actuator/health").permitAll()
                        .pathMatchers("/actuator/**").hasAnyRole("ADMIN")
                        .anyExchange().authenticated()
                )
                .addFilterAt(jwtAuthorizationFilter, SecurityWebFiltersOrder.AUTHORIZATION)
                .securityContextRepository(NoOpServerSecurityContextRepository.getInstance())
                .build();
    }
    @Bean
    public AbstractJackson2Decoder jacksonDecoder(ObjectMapper mapper) {
        return new Jackson2JsonDecoder(mapper);
    }
}
This configuration is set up to disable some features which aren’t required (form login, http basic, csrf and logout).
The two most important aspect are the authorizeExchange specification and addFilterAt method.
The authorizeExchange block specifies a number of path matchers that will determine what shall be done with requests that match those matchers.
The first matcher ensures any requests made to "/actuator/health" will be permitted, that is, no authorization token is required.
The second matcher will only allow requests made to "/actuator/**" which have the role "ADMIN".
This implicitly requires the token to be valid too.
The last is a catch-all matcher which will ensure that any unspecified requests are authenticated, i.e. they have a valid token.
The addFilterAt method simply adds the JWTAuthorizationFilter that was created previously into the filter chain and places it at the AUTHORIZATION stage within the chain.
This ensures the request is passed to the filter at the right time.
Testing
To test the newly added configuration the ipf.security.jwt.secret property must be set.
In this example the value secret will be used for simplicity.
ipf.security.jwt.secret=secret
A tool to create valid JWTs is required, the tool used in this demonstration can be found in this GitHub repository.
Set the following environment variables.
JWT_SECRET=secret
JWT_EXPIRY_SECONDS=180 # 3 minutes
JWT_ROLE=USER
Then create the JWT with the following command.
JWT=$(jwt encode \
  --alg HS512 \
  --sub user \
  --exp $(expr $(date +%s) + ${JWT_EXPIRY_SECONDS}) \
  --secret ${JWT_SECRET} \
  '{"role": ["ROLE_${ROLE}"]}')
At this point the service should be started, and the configuration can be tested using curl.
# Verify the "/actuator/health" endpoint does not need to be authenticated
curl localhost:8080/actuator/health # should return 200
# Verify the "/transactions" endpoint needs to be authenticated
curl localhost:8080/transactions -i # should return 401
curl -H "Authorization: Bearer $JWT" localhost:8080/transactions # Should return 200
# Verify the "/actuator/info" endpoint needs to be authenticated and have role ADMIN
curl localhost:8080/actuator/info -i # should return 401
curl -H "Authorization: Bearer $JWT" localhost:8080/actuator/info -i # should return 401
JWT_ROLE=ADMIN
JWT=<same as before>
curl -H "Authorization: Bearer $JWT" localhost:8080/actuator/info -i # should return 200
How to troubleshoot error messages
How to use the persistent scheduler with a flow
------------------------------
Using the persistent scheduler to support flows
The ipf-flo-scheduler module provides an out of the box mechanism to support the usage of the ipf-flo-scheduler with a flo without having to add any additional configuration.
To use, it is simply necessary to add the dependency to the module:
 <dependency>
    <groupId>com.iconsolutions.ipf.core.platform</groupId>
    <artifactId>ipf-flo-scheduler</artifactId>
</dependency>
This will inject all the necessary components via spring into your project.  All that is then required is the standard configuration for determining which actions and states require scheduling information.
How to secure IPF HTTP Endpoints
How to implement a duplicate check function
------------------------------
Sample Duplicate Check Function Implementation
Duplicate check via transaction cache is modeled as a Domain Function implementation calling the configured Transaction Cache Service.
This uses PersistentTransactionCacheService backed by MongoDB, so the necessary data to populate the cache and perform duplicate checks survives a service restart.
The definition of the Domain Function is (currently) done within the MPS project as the data points may be solution specific. See screenshots of an example Domain Function definition and its usage within a flow (calling the function and handling the response events) from a reference solution:
The corresponding implementation is as follows:
package com.iconsolutions.instantpayments.credittransfer.sample.config;
import com.iconsolutions.ipf.core.platform.txcache.service.PersistentTransactionCacheService;
import com.iconsolutions.ipf.core.platform.txcache.repository.TransactionCacheRepository;
import com.iconsolutions.ipf.core.platform.txcache.service.TransactionCacheService;
import com.iconsolutions.ipf.core.shared.retry.RepositoryRetryProvider;
import com.iconsolutions.ipf.payments.domain.clearing_and_settlement.pacs008.FIToFICustomerCreditTransfer;
import io.vavr.control.Try;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import java.util.List;
import java.util.Optional;
@Configuration
public class TransactionCacheServiceConfig {
    /**
     *
     * This definition builds a TransactionCacheService instances that handles entries for pacs.008 objects.
     * The following fields are being used to determine if the transaction is a duplicate and forms the key:
     *  EndToEndId, DbtrAcct.Id, LclInstrm.Prtry/Cd, IntrBkSttlmAmt.Value, IntrBkSttlmAmt.Ccy
     *
     * @param transactionCacheRepository - by default this bean is provide by spring-data repository through the
     *                                   com.icon.ipf.core.platform:ipf-transaction-cache dependency
     *
     * @param repositoryRetryProvider    - by default this bean is provided by the SharedRepositoryConfiguration config
     *                                   from the com.iconsolutions.ipf.core.shared:shared-application-common dependency.
     *                                   Added below for completeness
     *
     *                                   <pre>
     *                                       @Bean
     *                                       @ConditionalOnMissingBean
     *                                       public RepositoryRetryProvider repositoryRetryProvider() {
     *                                           return new RepositoryRetryProvider(
     *                                                   0,
     *                                                   t -> false,
     *                                                   null);
     *                                       }</pre>
     */
    @Bean
    public TransactionCacheService<FIToFICustomerCreditTransfer> debtorCTTransactionCacheService(
            TransactionCacheRepository transactionCacheRepository, RepositoryRetryProvider repositoryRetryProvider) {
        return new PersistentTransactionCacheService<FIToFICustomerCreditTransfer>(
                fi2fi -> {
                    var cdtTrfTxInf = fi2fi.getCdtTrfTxInf().get(0);
                    return List.of(
                            cdtTrfTxInf.getPmtId().getEndToEndId(),
                            Try.of(() -> cdtTrfTxInf.getDbtrAcct().getId().getOthr().getId()).getOrElseTry(() -> cdtTrfTxInf.getDbtrAcct().getId().getIBAN()),
                            Try.of(() -> Optional.ofNullable(cdtTrfTxInf.getPmtTpInf().getLclInstrm().getPrtry()).orElseThrow()).getOrElseTry(() -> cdtTrfTxInf.getPmtTpInf().getLclInstrm().getCd()),
                            cdtTrfTxInf.getIntrBkSttlmAmt().getValue().toString(),
                            cdtTrfTxInf.getIntrBkSttlmAmt().getCcy());
                },
                transactionCacheRepository,
                repositoryRetryProvider
        );
    }
}
package com.iconsolutions.instantpayments.credittransfer.sample.adapter.action;
import com.iconsolutions.instantpayments.domain.credittransfer.actions.CheckFunctionalDuplicateAction;
import com.iconsolutions.instantpayments.domain.credittransfer.adapters.DuplicateCheckingActionPort;
import com.iconsolutions.instantpayments.domain.credittransfer.domain.CredittransferDomain;
import com.iconsolutions.instantpayments.domain.credittransfer.inputs.CheckFunctionalDuplicateResponseInput;
import com.iconsolutions.instantpayments.domain.credittransfer.inputs.responsecodes.AcceptOrRejectCodes;
import com.iconsolutions.instantpayments.domain.credittransfer.reasoncodes.ISOReasonCodes;
import com.iconsolutions.ipf.core.platform.txcache.service.TransactionCacheService;
import com.iconsolutions.ipf.payments.domain.clearing_and_settlement.pacs008.FIToFICustomerCreditTransfer;
import lombok.AllArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import java.util.concurrent.CompletionStage;
/**
 * This class is the implementation of an external domain action adapter that calls the transaction cache in order to perform
 * a duplicate check.
 *
 * It depends on an instance of TransactionCacheService that has been defined in TransactionCacheServiceConfig
 */
@Slf4j
@AllArgsConstructor
public class SampleDuplicateCheckingActionAdapter implements DuplicateCheckingActionPort {
    private final TransactionCacheService<FIToFICustomerCreditTransfer> transactionCacheService;
    @Override
    public CompletionStage<Void> execute(CheckFunctionalDuplicateAction action) {
        return saveAndVerify(transactionCacheService, action)
                .thenCompose(CredittransferDomain.duplicateChecking()::handle)
                .thenAccept(result -> log.debug("DuplicateCheckingActionAdapter completed with {}", result.getResult()));
    }
    /**
     * In this example we eagerly save the payment and then verify if it has any duplicates
     * - save it to the cache
     * - re-read it from the cache using the derived key
     * - if more than one entry is found then at least one previously existed and therefore it IS a duplicate
     * - return the appropriate response to the process response Input
     *
     * Note:
     * This "eager" save is a preferable alternative to the process of:
     * - read from cache with derived key
     * - if there is a result then flag a duplicate else save to the cache
     *
     * As it reduces the window for concurrent duplicates slipping through, at the cost of an extra record being stored
     *
     * @param cacheService
     * @param action
     */
    public CompletionStage<CheckFunctionalDuplicateResponseInput> saveAndVerify(TransactionCacheService<FIToFICustomerCreditTransfer> cacheService,
                                                                               CheckFunctionalDuplicateAction action) {
        return cacheService.saveToCache(action::getFlowType, action.getCustomerCreditTransfer())
                .thenCompose(entry -> cacheService.findInCache(action::getFlowType, action.getCustomerCreditTransfer()))
                .thenApply(entries -> entries.size() == 1 ? accepted(action.getId()) : rejected(action.getId()));
    }
    private CheckFunctionalDuplicateResponseInput accepted(String aggregateId) {
        return new CheckFunctionalDuplicateResponseInput.Builder(aggregateId, AcceptOrRejectCodes.Accepted)
                .build();
    }
    private CheckFunctionalDuplicateResponseInput rejected(String aggregateId) {
        return new CheckFunctionalDuplicateResponseInput.Builder(aggregateId, AcceptOrRejectCodes.Rejected)
                .withReasonCode(ISOReasonCodes.AM05)
                .build();
    }
}
How to use the persistent scheduler with a flow
How to get aggregate data for use in an external domain request
------------------------------
Get Aggregate Data for Use in an External Call
Getting the aggregate data for use in an external domain function can be done with the help of the domain functions as mentioned in Domain Operations.
All domain operations are asynchronous and return a CompletionStage. So if we want to use the result of the getAggregate we need to chain calls with thenCompose/thenApply to ensure we are not blocking anywhere.
Below is an example of sending some data out to a SendConnector after we have retrieved the aggregate data, as it is required to populate some data in the request message of the SendConnector.
public class SampleFraudActionAdapter implements FraudActionPort {
    private final SendConnector<FraudRequest, OlafRequest> fraudSendConnector;
    @Override
    public CompletionStage<Void> execute(CheckFraudAction action) {
        FraudRequest fraudRequest = new FraudRequest();
        fraudRequest.fiToFICustomerCreditTransfer = action.getCustomerCreditTransfer();
        return CredittransferDomain.getAggregate(action.getId())
                .thenCompose(
                        aggregate -> {
                            fraudRequest.customFields.put("mykey", aggregate.getStatus().getStatus());
                            return fraudSendConnector.send(action.getProcessingContext(), fraudRequest)
                                    .thenAccept(deliveryOutcome -> log.debug("FraudAdapter completed with {}", deliveryOutcome.getDeliveryReport().getOutcome()));
                        }
                );
    }
    private class FraudRequest {
        public FIToFICustomerCreditTransfer fiToFICustomerCreditTransfer;
        public Map<String, String> customFields;
    }
}
How to implement a duplicate check function
Archetype Starter
------------------------------
IPF Archetype
The IPF archetype is a Maven Archetype that provides engineers with a quick and efficient way to boostrap a new project using the IPF SDK.  The archetype will generate a new IPF project that includes all the necessary dependencies and configuration to be able to run as a standalone project.
To understand more about the key concepts and configuration options, we suggest you start with Archetype Concepts.  Alternatively you can just get going and create a quick IPF project using the Getting Started guides.
How to get aggregate data for use in an external domain request
Concepts
------------------------------
Archetype Concepts
Output
Once the archetype has run, it will generate a new project.  This key configuration option to consider here is the 'artifactId'. This will be used to determine the folder name of the generated project.  So if running with an 'artifactId' for example of 'my-test' then this will lead to a new folder being generated called my-test relative to the directory the archetype is run from.
Inside this folder will be a fully functional IPF application which can be used as the basis from which to build your IPF application"
The Icon BOM
The generated project uses the Icon BOM (a Maven Bill of Materials) as its parent.  This provides all the dependencies and their versions for the generated application to use. If the project you are building needs to extend for your own parent, this can be updated post generation.
Run Modes
The archetype can be run in either:
interactive mode - this is where the user specifies a minimal set of configuration on initial creation command and then the archetype will prompt the user to complete each of the remaining config items in turn.
batch mode - this is where the user is responsible for supplying all the configuration options to the archetype on the initial creation command.  There is then no ability to further enrich or edit the commands.
Configuration Options
The following table defines the different configuration options available for use within the archetype.
Parameter
Description
Type
Example
archetypeGroupId
Group of the archetype.
String
com.iconsolutions
archetypeArtifactId
The archetype id.
String
icon-archetype
archetypeVersion
Version to use of the archetype.
String
1.3.1
ipfVersion
Version of IPF to use
String
2022.3
groupId
Group id, also used as package.
String
my.test
artifactId
The name of the new archetype.
String
my-test
version
The version  to start with.
String
1.0.0-RELEASE
solutionName
Name of the solution.
Alpha first uppercase, rest lower
Solution
modelName
Name of the model being used.
Alpha all lowercase
model
flowName
Name of the initial flow.
Alpha first uppercase, rest lower
MyFlow
includeApplication
Include application modules.
(yn)
y
includeTest
Include test modules
(yn)
y
useDocker
Generate project using docker
(yn)
y
dockerRegistry
Docker registry for docker images
String
registry.ipf.iconsolutions.com
You must supply the target version of both the archetype (archetypeVersion) and the IPF version (ipfVersion).
The versions must be compatible, please check and replace the numbers based on IPF’s release documentation and/or the details for your target environment!
Archetype Starter
Getting Started
------------------------------
Getting Started
This guide is aimed to get a user from never having used IPF before to having a running application in under 5 minutes! It assumes that you already have access to the appropriate maven repository containing an IPF release.
We’re going to build the simplest IPF application possible:
Let’s look a little deeper at what is actually making up the IPF application here:
Here you can see that the application is built using a number of IPF components.
We’ll use the IPF Archetype to build a skeleton application that contains a single processing flow.  We can then use a simple http controller to invoke the flow and see our results on the IPF Developer Application.
Running the Archetype
Firstly navigate to the directory that you want your generated project to be installed to.  Whilst this directory does not need to be empty it must not contain a folder with the name of the project to generate and most not have a pom.xml file within it.
mvn archetype:generate -B -DarchetypeGroupId=com.iconsolutions.ipf.core.archetype -DarchetypeArtifactId=icon-archetype -DarchetypeVersion=1.*.* -DgroupId=quick.start -DartifactId=quick-start -Dversion=0.0.1-SNAPSHOT -DipfVersion=202*.*.* -DsolutionName=Quickstartsolution -DmodelName=quickstartmodel -DflowName=Quickstartflow -DincludeApplication=y -DincludeTest=y
You must supply the target version of both the archetype (archetypeVersion) and the IPF version (ipfVersion).
The versions must be compatible, please check and replace the numbers based on IPF’s release documentation and/or the details for your target environment!
After a few seconds, the project will be generated. You can see this in the output:
[INFO] Project created from Archetype in dir: /build/archdocs/quick-start
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  8.914 s
[INFO] Finished at: 2023-01-05T17:01:08Z
[INFO] ------------------------------------------------------------------------
Here you can see that our project now resides in /build/archdocs/quick-start.  Note that in this case, the archetype was run from the /build/archdocs directory.
Here you need to supply the target version of both the archetype and the IPF version.  At the time of writing, the latest versions are 2022.2.17.172 and 1.5.0 but please check this for your target environment!
If you want to learn more about the config options provided and their meaning please check here.
Let’s now look at the /build/archdocs/quick-start directory and you’ll see:
➜  mytest ls -ltr
total 44
drwxrwxr-x 3 bensavage bensavage  4096 Jan  5 17:01 quick-start-app (1)
drwxrwxr-x 8 bensavage bensavage  4096 Jan  5 17:01 quick-start-domain (2)
drwxrwxr-x 3 bensavage bensavage  4096 Jan  5 17:01 quick-start-e2e (3)
-rw-rw-r-- 1 bensavage bensavage  2490 Jan  5 17:01 Readme.md (4)
-rw-rw-r-- 1 bensavage bensavage  3893 Jan  5 17:01 pom.xml
The key folders to note here are the:
1
quick-start-app - this is the home for the application runtime code, the code that will be deployed and use the IPF generated domain to process transactions.
2
quick-start-domain - this contains all the artefacts relating to the DSL and flow definitions.
3
quick-start-e2e - this contains all the docker application code
4
Readme.md - this contains information on how to run the generated project, have a look at that now.
Having generated the project, you can build it. Move to the base directory of the project and run:
mvn clean install
This will build the project, generating all the artifacts and run both the DSL and application tests.  Note that in the application tests, the base implementation uses TestContainers to provide the mongo database.  If you’re not using docker, you can update this and use your own mongo database.
Let’s now run the application in it’s containerised form.  To do this, open the quick-start-e2e module and try running the E2EComposeLauncher java class.  This will spin up a docker environment containing:
An instance of our application code.
An instance of a mongo database.
An instance of the ipf-developer-app, which can be used to quickly view payments.
It should take a minute or so to fully start the environment, you can see when it’s ready from the logs:
22-01-2023 17:39:52.889 [main] INFO  c.i.t.c.c.runner.env.ComposeLauncher - background compose started successfully
Once it’s ready, we can fire a dummy payment through by calling the "submit" endpoint of the provided controller.  A simple way to do this using curl is:
curl -X POST localhost:8080/submit
If successful this should return you the payment identification details:
{
  "requestId": "4cece8f7-1a7f-4eb8-a189-a0de5523175e",
  "uowId": "478a35ce-5e34-4e7b-a58b-060368fab758",
  "aggregateId": "QuickStartFlow|39437cdb-4612-42a8-b503-8963a211391a"
}
The key id to consider now is the uowId - this is the unique identifier for our transaction.
Now let’s open the Developer Application.  Once open, click search and we should see:
From here you can click the 'View' button to start digging into the details of your transaction!
Concepts
How do you build a project without docker dependencies?
------------------------------
How do you build a project without docker dependencies?
When the archetype runs, it’s default mode will generate a set of docker scripts that can be used to run the application.  These docker scripts provide containers for mongodb.  The docker based mongo containers are required in order for the generated tests to work.  To stop them from being generated entirely, we simply run the archetype with the "includeTests" config option set to "N".
Getting Started
How do you run the archetype in the different modes?
------------------------------
How do you run the archetype in the different modes?
To run the archetype, we simply run in one of the two modes as detailed below.  This will generate a new IPF project in a directory relative to that the command is run from.  The project name will be set to the artifactId provided.
In order to run the application, you’ll need to supply the IPF version you want to run against.
Running in interactive mode
The application can be run interactively, as follows;
 mvn archetype:generate \
-Preleases \
-DarchetypeGroupId=com.iconsolutions.ipf.core.archetype \
-DarchetypeArtifactId=icon-archetype \
-DipfVersion=__IPF_VERSION__ \
-DarchetypeVersion=${project.version}
This will then prompt the user to complete the remaining configuration options.
Running in batch (non-interactive) mode
 mvn archetype:generate -B \
-DarchetypeGroupId=com.iconsolutions.ipf.core.archetype \
-DarchetypeArtifactId=icon-archetype \
-DarchetypeVersion=${project.version} \
-DipfVersion=__IPF_VERSION__ \
-DgroupId=my.test \
-DartifactId=my-test \
-Dversion=1.0.0 \
-DsolutionName=Mytest \
-DmodelName=model \
-DflowName=Myflow \
-DincludeApplication=y
-DincludeTest=y
How do you build a project without docker dependencies?
How do you use your own parent dependency?
------------------------------
How do you use your own parent dependency?
Projects created by this archetype will use the IPF Bill Of Materials (BOM) as a parent, if you wish to use a custom
parent POM, then make the following changes after project creation to ensure that the correct ipf dependencies are included:
<dependencyManagement>
     <dependencies>
        <dependency>
            <groupId>com.iconsolutions.ipf</groupId>
            <artifactId>ipf-release-bom</artifactId>
            <version>__IPF_VERSION__</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
How do you run the archetype in the different modes?
How do you create a reusable flow module?
------------------------------
How do you create a reusable flow module?
Most IPF projects are traditional applications that contain one or more flows together with the implementation details to make them executable and able to process payments.  This is done through the many different types of component available with the Icon DSL.
However, you may wish to build a project that only includes the DSL code, i.e. to create a re-usable pieces of DSL that is not implemented directly within the generated project but is expected to be included within other projects, for example see: Using shared models.
To do this, you simply run the archetype with the 'includeApplication' flag set to "N".  This will mean the archetype will only build the project in a reusable component format.  Note there’s no reason why you can’t subsequently turn it into a standard self-contained application project by adding the relevant application modules yourself subsequently.
How do you use your own parent dependency?
Processing
------------------------------
Processing
IPF contains a number of features within the core which are optionally used depending on your solution requirements. This section covers those which are related to core processing.
IPF Cache - this module provides a simple Caffeine backed caching interface that IPF solutions can use for in memory caching (also see Transaction Caching for transaction specific caching, especially where a long lived persistent cache is required).
Persistent Scheduler - scheduling future processing and jobs is covered by this component and works with cron expressions in combination with calendars to execute one-time or recurrent jobs. It also features a persistence layer whose main role is to keep single source of truth.
Dynamic Settings - Dynamic Processing Settings provides a platform to manage configurable settings and their lifecycle.
Bulk File Processing - the modules here provide the capability to process bulks of transactions or records, streamed from or to files.
Message Logger - module and interface to allow the logging and/or publishing of IPF processing data.
How do you create a reusable flow module?
IPF Cache
------------------------------
IPF Cache
The ipf-cache module provides a simple caching interface that IPF products can use for caching.
The current offering is a Caffeine backed cache baked into Spring or InfiniSpan.
Concepts
Features
Caffeine
InfiniSpan
Getting Started
Processing
Concepts
------------------------------
Concepts
The ipf-cache module provides a simple caching interface that IPF products can use for caching.
The current offering is a Caffeine backed cache baked into Spring or Infinispan.
API
The API is based upon common API interactions and comes in 2 flavours, asynchronous and synchronous.
package com.iconsolutions.ipf.core.platform.cache.api;
import com.iconsolutions.ipf.core.shared.domain.context.ProcessingContext;
import java.util.concurrent.CompletionStage;
import java.util.function.Function;
/**
 * Asynchronous IPF cache adapter
 *
 * @param <K> key
 * @param <V> Value
 */
public interface AsyncCacheAdapter<K,V> {
    /**
     * Retrieve a future value from cache
     * @return future containing nullable value
     */
    CompletionStage<V> get(K key);
    /**
     * Retrieve a cache entry or if missing provide a future to make the value
     * @return future containing value cached
     */
    CompletionStage<V> getOrDefault(K key, Function<K, CompletionStage<V>> callback);
    /**
     * Retrieve a cache entry, add to messageLog or if missing provide a future to make the value
     * @return future containing value cached
     */
    CompletionStage<V> getThenLogOrDefault(ProcessingContext processingContext, K key, Function<K, CompletionStage<V>> callback);
    /**
     * Pass a future which upon completion will add the cache entry
     */
    CompletionStage<Void> put(K key, CompletionStage<V> value);
    /**
     * Manually evict an entry
     */
    CompletionStage<Void> evict(K key);
    /**
     * Evict all entries
     */
    CompletionStage<Void> clear();
}
package com.iconsolutions.ipf.core.platform.cache.api;
import com.iconsolutions.ipf.core.shared.domain.context.ProcessingContext;
import java.util.Optional;
/**
 * Synchronous IPF cache adapter
 *
 * @param <K> key
 * @param <V> Value
 */
public interface CacheAdapter<K, V> {
    /**
     * Retrieve from cache
     *
     * @param key
     * @return Optional<value>
     */
    Optional<V> get(K key);
    /**
     * Put an entry into cache
     *
     * @param key
     * @param value
     */
    void put(K key, V value);
    /**
     * Manually evict the cached item for this key
     *
     * @param key
     */
    void evict(K key);
    /**
     * Clear all cached entries.
     */
    void clear();
    /**
     * Retrieve an entry or if absent use the provided value
     *
     * @param key
     * @param defaultValue
     * @return value cached
     */
    default V getOrDefault(K key, V defaultValue) {
        return get(key).orElseGet(() -> {
            put(key, defaultValue);
            return defaultValue;
        });
    }
    /**
     * Retrieve an entry or if absent use the provided value
     *
     *
     * @param processingContext
     * @param key
     * @param defaultValue
     * @return value cached
     */
    V getThenLogOrDefault(ProcessingContext processingContext, K key, V defaultValue);
}
Both implementations are created based on the Cache Factory
package com.iconsolutions.ipf.core.platform.cache.api;
/**
 * Factory to provide the cache implementation by name
 *
 * @param <K> key
 * @param <V> value
 */
public interface CacheFactory<K, V> {
    CacheAdapter<K, V> createCacheAdapter(String name);
    AsyncCacheAdapter<K, V> asyncCreateCacheAdapter(String name);
}
IPF Cache
Features
------------------------------
Features
The current IPF-Cache offering is a Caffeine backed cache baked into Spring or InfiniSpan.
Here are the key features:
Caffeine
InfiniSpan
Concepts
Caffeine
------------------------------
Caffeine
Introduction
Caffeine is a high-performance caching library for Java.
One fundamental difference between a cache and a Map is that a cache evicts stored items.
An eviction policy decides which objects should be deleted at any given time. This policy directly affects the cache’s hit rate — a crucial characteristic of caching libraries.
Caffeine uses the Window TinyLfu eviction policy, which provides a near-optimal hit rate.
Caffeine Configuration
First, let’s create a Caffeine bean. This is the main configuration that will control caching behavior such as expiration, cache size limits, and more:
    private CaffeineCache buildCaffeineCache(String name, CaffeineCacheSetting cacheSpec) {
        log.info("Cache {} specified timeout of {} min, max of {}", name, cacheSpec.getTimeout(), cacheSpec.getMaxSize());
        final Caffeine<Object, Object> caffeineBuilder =
            Caffeine.newBuilder()
                .expireAfterWrite(cacheSpec.getTimeout())
                .maximumSize(cacheSpec.getMaxSize())
                .recordStats();
        return new CaffeineCache(name, caffeineBuilder.build());
    }
Next, we need  another bean using the Spring CacheManager interface. Caffeine provides its implementation of this interface, which requires the Caffeine object we created above:
    @Bean(name = "ipfCacheManager")
    CacheManager ipfCaffeineCacheManager() {
        SimpleCacheManager manager = new SimpleCacheManager();
        if (Objects.nonNull(settings)) {
            List<CaffeineCache> caches = settings.entrySet().stream()
                .map(entry -> buildCaffeineCache(entry.getKey(), entry.getValue()))
                .collect(Collectors.toList());
            manager.setCaches(caches);
        }
        return manager;
    }
All the beans mentioned above we get for free when adding the maven dependency mentioned before.
However, the Caffeine cache requires the following configuration values for each cache that is provided:
ipf.caching.caffeine.settings."${cache_name}".timeout=[Duration]
ipf.caching.caffeine.settings."${cache_name}".max-size=[Long]
cache_name - name of the cache being used
timeout - duration cache will remain in memory active before being evicted.
max-size - maximum cache size before the cache evicts entries that are less likely to be used again see Caffeine
An Example:
ipf.caching.caffeine.settings.cache1.timeout=10m
ipf.caching.caffeine.settings.cache1.max-size=10000
ipf.caching.caffeine.settings.cache2.timeout=20s
ipf.caching.caffeine.settings.cache2.max-size=100000
Caffeine Implementation
Implementation is simple, as this module is spring boot aware it will wire up all the necessary beans and hook them into the Spring CacheManager providing metrics.
Just add the maven dependency and then retrieve any caches by name.
We get the CacheFactory bean for free from ipf-cache-caffeine module and by enabling caffeine caching.
    @Bean(name = "caffeineCacheFactory")
    CacheFactory<?, ?> cacheFactory(CacheManager cacheManager, CacheLogger<Object, Object> cacheLogger) {
        return new CaffeineCacheFactory(cacheManager, cacheLogger);
    }
Then, you just need to use the CacheFactory to create either an AsyncCacheAdapter:
        @Bean
        AsyncCacheAdapter<String, String> asyncCacheAdapter3(CacheFactory<String, String> cacheFactory) {
            return cacheFactory.asyncCreateCacheAdapter(CACHE_3);
        }
Or a CacheAdapter:
        @Bean
        CacheAdapter<?, ?> cacheAdapter1(CacheFactory<?, ?> cacheFactory) {
            return cacheFactory.createCacheAdapter(CACHE_1);
        }
Dependencies
The dependency on ipf-cache-api module requires some supplied dependencies to read and write to the message log.
These can be added to your application (if not present) as follows:
        @Bean
        ObjectMapper objectMapper() {
            return new ObjectMapper();
        }
        @Bean
        MessageLogger messageLogger() {
            return messageLogEntry -> log.info("log entry: {}", messageLogEntry);
        }
Features
InfiniSpan
------------------------------
InfiniSpan
Introduction
Infinispan is an in-memory key/value data store that ships with a more robust set of features than other tools of the same niche.
It provides a flexible, in-memory data stores that you can configure to suit use cases such as:
Boosting application performance with high-speed local caches.
Optimising databases by decreasing the volume of write operations.
Providing resiliency and durability for consistent data across clusters.
Infinispan Configuration
The CacheManager is the foundation of the majority of features that we’ll use. It acts as a container for all declared caches, controlling their lifecycle, and is responsible for the global configuration.
Infinispan ships with a really easy way to build the CacheManager:
    @Bean
    InfinispanCacheProvider infinispanCacheProvider(final Marshaller marshaller) {
        var configuration = kubernetesStack
                ? buildClusteredConfigurationForKubernetes(marshaller)
                : buildDefaultClusteredConfiguration(marshaller);
        var cacheManager = new DefaultCacheManager(configuration);
        return new InfinispanCacheProvider(cacheManager, settings);
    }
    private GlobalConfiguration buildClusteredConfigurationForKubernetes(Marshaller marshaller) {
        return GlobalConfigurationBuilder.defaultClusteredBuilder()
                .cacheManagerName(cacheManagerName)
                .transport()
                .addProperty("stack", "kubernetes")
                .addProperty("configurationFile", "default-configs/default-jgroups-kubernetes.xml")
                .initialClusterSize(initialClusterSize)
                .initialClusterTimeout(initialClusterTimeout.getSeconds(), TimeUnit.SECONDS)
                .serialization()
                .marshaller(marshaller)
                .build();
    }
    private GlobalConfiguration buildDefaultClusteredConfiguration(Marshaller marshaller) {
        return GlobalConfigurationBuilder.defaultClusteredBuilder()
                .cacheManagerName(cacheManagerName)
                .transport()
                .initialClusterSize(initialClusterSize)
                .initialClusterTimeout(initialClusterTimeout.getSeconds(), TimeUnit.SECONDS)
                .serialization()
                .marshaller(marshaller)
                .build();
    }
A cache is defined by a name and a configuration. The necessary configuration can be built using the class ConfigurationBuilder, already available in our classpath.
The ConfigurationBuilder is provided with the following method:
    private Cache<Object, Object> buildInfinispanCache(final String name, final InfinispanCacheSetting infinispanCacheSetting) {
        log.info("Cache {} specified timeout of {} min, max of {}", name, infinispanCacheSetting.getTimeout(),
                infinispanCacheSetting.getMaxSize());
        var configBuilder = new ConfigurationBuilder();
        var cacheMode = CacheMode.valueOf(infinispanCacheSetting.getCacheMode());
        configBuilder.clustering()
                .cacheMode(cacheMode)
                .encoding().mediaType("application/json")
                .memory()
                .maxCount(infinispanCacheSetting.getMaxSize())
                .whenFull(EvictionStrategy.REMOVE)
                .expiration()
                .lifespan(infinispanCacheSetting.getTimeout().toMillis(), TimeUnit.MILLISECONDS);
        if (isRemote(cacheMode)) {
            configBuilder.clustering()
                    .stateTransfer().fetchInMemoryState(infinispanCacheSetting.getFetchInMemoryState())
                    .awaitInitialTransfer(infinispanCacheSetting.getAwaitInitialStateTransfer())
                    .timeout(infinispanCacheSetting.getStateTransferTimeout().toMillis());
        }
        final Cache<Object, Object> cache = cacheManager.administration().withFlags(CacheContainerAdmin.AdminFlag.VOLATILE)
                .getOrCreateCache(name, configBuilder.build());
        setCacheLevelLogging(cache, name, cacheMode, infinispanCacheSetting);
        setClusterLevelLogging(cacheMode, infinispanCacheSetting);
        return cache;
    }
    private void setCacheLevelLogging(final Cache<Object, Object> cache, final String cacheName, final CacheMode cacheMode,
                                      final InfinispanCacheSetting infinispanCacheSetting) {
        if (isRemote(cacheMode) && infinispanCacheSetting.getClusterLogging()) {
            cache.addListener(new ClusterCacheLoggingListener(cacheName));
        }
        if (infinispanCacheSetting.getLocalLogging()) {
            cache.addListener(new LocalCacheLoggingListener(cacheName));
        }
    }
    private void setClusterLevelLogging(final CacheMode cacheMode, final InfinispanCacheSetting infinispanCacheSetting) {
        if (isRemote(cacheMode) && isLoggingEnabled(infinispanCacheSetting)) {
            cacheManager.addListener(new ClusterLoggingListener());
        }
    }
    private boolean isLoggingEnabled(final InfinispanCacheSetting infinispanCacheSetting) {
        return infinispanCacheSetting.getClusterLogging() || infinispanCacheSetting.getLocalLogging();
    }
    private boolean isRemote(final CacheMode cacheMode) {
        return cacheMode.isDistributed() || cacheMode.isReplicated();
    }
All the documentation on how to configure an Infinispan cache is available here.
All the beans mentioned above we get for free when adding the maven dependency mentioned before.
However, the Infinispan cache requires the following configuration values for each cache that is provided;
ipf.caching.infinispan.settings."${cache_name}".cache-mode=[CacheMode]
ipf.caching.infinispan.settings."${cache_name}".timeout=[Duration]
ipf.caching.infinispan.settings."${cache_name}".max-size=[Long]
ipf.caching.infinispan.settings."${cache_name}".cluster-logging=[Boolean]
ipf.caching.infinispan.settings."${cache_name}".local-logging=[Boolean]
cache_name - name of the cache being used
cache-mode - Infinispan cache managers can create and control multiple caches that use different modes. For example, you can use the same cache manager for local caches, distributed caches, and caches with invalidation mode.
timeout - duration cache will remain in memory active before being evicted.
max-size - maximum cache size before the cache evicts entries that are less likely to be used again
cluster-logging - instantiates a ClusterCacheLoggingListener
local-logging - instantiates a LocalCacheLoggingListener
An Example:
ipf.caching.infinispan.settings.cache1.cache-mode=REPL_ASYNC
ipf.caching.infinispan.settings.cache1.timeout=15m
ipf.caching.infinispan.settings.cache1.max-size=15000
ipf.caching.infinispan.settings.payment-data.cluster-logging=true
ipf.caching.infinispan.settings.payment-data.local-logging=true
cache-mode - can be set to one of following:
LOCAL - Data is not replicated
REPL_ASYNC - Data replicated asynchronously
REPL_SYNC - Data replicated synchronously
DIST_SYNC
DIST_ASYNC
In case the cache mode is distributed or replicated, the following additional configuration is required:
ipf.caching.infinispan.settings."${cache_name}".fetch-in-memory-state=[Boolean]
ipf.caching.infinispan.settings."${cache_name}".await-initial-state-transfer=[Boolean]
ipf.caching.infinispan.settings."${cache_name}".state-transfer-timeout=[Duration]
More detail on the fields:
fetch-in-memory-state - If true, the cache will fetch data from the neighboring caches when it starts up, so the cache starts 'warm', although it will impact startup time. In distributed mode, state is transferred between running caches as well, as the ownership of keys changes (e.g. because a cache left the cluster). Disabling this setting means a key will sometimes have less than numOwner owners.
await-initial-state-transfer - If true, this will cause the first call to method CacheManager.getCache() on the joiner node to block and wait until the joining is complete and the cache has finished receiving state from neighboring caches (if fetchInMemoryState is enabled). This option applies to distributed and replicated caches only and is enabled by default. Please note that setting this to false will make the cache object available immediately but any access to keys that should be available locally but are not yet transferred will actually cause a (transparent) remote access. While this will not have any impact on the logic of your application it might impact performance.
state-transfer-timeout - This is the maximum amount of time - in milliseconds - to wait for state from neighboring caches, before throwing an exception and aborting startup.
An Example:
ipf.caching.infinispan.settings.cache1.fetch-in-memory-state=true
ipf.caching.infinispan.settings.cache1.await-initial-state-transfer=true
ipf.caching.infinispan.settings.cache1.state-transfer-timeout=6m
Infinispan Implementation
We get the CacheFactory bean for free from ipf-cache-infinispan module and by enabling infinispan caching.
    @Bean(name = "infinispanCacheFactory")
    CacheFactory<?, ?> infinispanCacheFactory(InfinispanCacheProvider infinispanCacheProvider, CacheLogger<Object, Object> cacheLogger) {
        return new InfinispanCacheFactory(infinispanCacheProvider, cacheLogger);
    }
Then, you just need to use the CacheFactory to create either an AsyncCacheAdapter:
        @Bean(name = "paymentInfinispanDataCacheAdapter1")
        AsyncCacheAdapter<Object, Object> paymentInfinispanDataCacheAdapter1(CacheFactory<Object, Object> infinispanCacheFactory) {
            return infinispanCacheFactory.asyncCreateCacheAdapter("cache1");
        }
Caffeine
Getting Started
------------------------------
Getting Started
Modules
There are 3 separate modules:
ipf-cache-api - all the API contracts
ipf-cache-caffeine - using spring backed cache provides caffeine backed implementation
ipf-cache-infinispan - provides our implementation around famous InfiniSpan caching technology.
Dependency
Declare the api dependency if a custom implementation is to be used.
<dependency>
    <groupId>com.iconsolutions.ipf.core.platform</groupId>
    <artifactId>ipf-cache-api</artifactId>
    <version>${project-version}</version>
</dependency>
If you want a pre-prepared Caffeine flavour then simply add the following;
<dependency>
    <groupId>com.iconsolutions.ipf.core.platform</groupId>
    <artifactId>ipf-cache-caffeine</artifactId>
    <version>${project-version}</version>
</dependency>
If you want a pre-prepared InfiniSpan flavour then simply add the following;
<dependency>
    <groupId>com.iconsolutions.ipf.core.platform</groupId>
    <artifactId>ipf-cache-infinispan</artifactId>
    <version>${project-version}</version>
</dependency>
InfiniSpan
Persistent Scheduler
------------------------------
IPF Persistent Scheduler
Introduction
IPF’s Persistent Scheduler allows you to schedule jobs of any kind.
It is based on the Quartz scheduler, and works with cron expressions in combination
with calendars in order to execute one-time or recurrent jobs.
It also features a persistence layer whose main role is to keep single source of truth with regards to the definition
of tasks and also persists job history in an append-only journal.
The Scheduler also features failsafes such as a rescheduling module which runs at startup and restores all the scheduled
jobs back into Quartz after a previous failure. To use the Scheduler, start with SchedulingModuleInterface.
Finally, it is designed to be run in a cluster. It uses Akka Cluster
and Cluster Singleton to ensure
that jobs are only scheduled in one place in the cluster and can survive any number of node failures (including a total outage).
The architecture for the scheduler is shown below:
Getting Started
Concepts
------------------------------
Concepts
The Action Helper
Calendars
Failed Jobs
Job Specification and Status
Scheduling Module
Scheduling Status
Persistent Scheduler
The Action Helper
------------------------------
The Action Helper
This can be thought of as the engine that receives the scheduled commands to run. It awaits messages from the scheduler
which - when the time comes to run a job as per the cron expression - will hand the command to the SchedulingHelper and
being executing
Interface
The SchedulingHelper has two methods:
execute(String, Command): Run a task
This is what the scheduler will call at the right time. The "String" is the triggerIdentifier from the
JobSpecification, and the Command is the triggerCommand from the same specification.
supports(Command): Does this SchedulingHelper support this command?
When defining multiple SchedulingHelpers, the scheduler needs to know which SchedulingHelper can support which command.
Concepts
Calendars
------------------------------
Calendars
A Calendar is a mechanism that is used for excluding blocks of time regardless of the time specification
that was supplied. For example, if a job is set to run once a day, it is possible to augment this specification with a
calendar to say that it should only run once a day on weekdays, for example. More information is available here.
If a calendar is not provided as part of the JobSpecification, then the default calendar is used.
To define a default calendar, the configuration file has to contain ipf.persistent-scheduler.quartz.calendars.default
and ipf.persistent-scheduler.quartz.calendars as in example below.
If no default calendar is configured, then the scheduler will not use calendars and the cron expression will be honoured
without exclusions.
ipf.persistent-scheduler.quartz.calendars.default = "DefaultCalendar"
ipf.persistent-scheduler {
  quartz {
    calendars {
      DefaultCalendar {
        type = Annual
        description = "Default calendar"
        exclude-dates = ["12-25", "01-01"]
      }
    }
  }
}
Calendar Types
Quartz calendars can be of the following types:
Calendar type name
Description
Example
Daily
Exclude blocks of time from a day with a timezone. Use UTC or the list of timezones here.
exclude {
    start-time = "03:00"
    end-time = "05:00"
}
timezone = UTC
Monthly
Exclude days from a month
exclude-days = [1, 3, 5, 7]
Weekly
Exclude days of the week from a week.  Days are 1-indexed and start on Sunday, i.e. Sunday = 1, Monday = 2, etc.
exclude-days = [1, 7] //will exclude the weekend
Cron
Exclude by a custom cron expression
exclude-expression = * * 0-7,18-23 ? * *
Holiday
Exclude explicit dates (useful for moveable feasts like Easter or UK bank holidays) in ISO 8601 yyyy-MM-dd format
exclude-dates = ["2024-03-31", "2025-04-20"] //excludes Easter 2024 and 2025
Annual
Exclude calendar dates from each year in MM-DD format
exclude-dates = ["01-01", "25-12"] //excludes New Year’s Day and Christmas Day every year
A job can only use one calendar.
The Action Helper
Failed Jobs
------------------------------
Failed Jobs
A failed job is defined as a job that matches the following criteria:
It has missed its regular execution slot with its specification unchanged, OR;
It was scheduled to be in the past
Its specification was updated in a way to stop any further execution
The Failed Jobs Processor
When the system starts, it schedules an internal job to find other failed scheduled jobs.
This job behaves like any other scheduled job, but has a special task of identifying other jobs that have failed
according to the above criteria, and processes them (see below for details).
The frequency of execution is configurable, and it can be set using the following properties file:
package com.iconsolutions.ipf.core.platform.scheduler.persistent.job;
import lombok.Data;
import org.springframework.boot.context.properties.ConfigurationProperties;
@Data
@ConfigurationProperties(prefix = "ipf.persistent.scheduler.process-failed-jobs")
public class ProcessFailedJobsProperties {
    private Boolean active;
    private String cronExpression;
}
active: a flag to set if the job is active or not
cronExpression: a cron expression to describe the frequency the job will run. For help with building a cron
expression, use an online cron expression builder such as this one.
The example below sets it to run once a day at 0:15:59:
ipf.persistent.scheduler.process-failed-jobs {
  active = true
  cron-expression = "59 15 0 */1 ? *"
}
Being notified of failed jobs
When configuring a JobSpecification, it is possible to be notified of failures for that specific job by specifying
the failure identifier and command:
JobSpecificationDto.builder()
    .jobRequestor(JOB_REQUESTOR)
    .triggerCommand(TEST_COMMAND)
    .schedulingSpecification(cronExpression)
    .failureIdentifier(failureIdentifier) (1)
    .failureCommand(failureCommand) (2)
    .build();
If this job has failed since the last checkpoint, then the relevant SchedulingHelper will be notified of this. Note that the
SchedulingHelper will need to have the supports method updated to support the failure command too.
Calendars
Job Specification and Status
------------------------------
Job Specification and Status
This page explains the job specification and status:
Job Specification
The job specification is the way to tell the IPF Persistent Scheduler how to run a scheduled job. It consists of the
following parts:
Name
Mandatory?
Description
jobSpecificationKey
Yes
a unique identifier for this job
jobRequestor
Yes
A unique identifier for the requestor (a requestor can have multiple jobs)
schedulingSpecification
Yes
The cron expression for this job
triggerCommand
Yes
The Command to send to the SchedulingHelper at the scheduled time(s)
triggerIdentifier
Yes
An ID to use to send to the SchedulingHelper at the scheduled time(s)
failureCommand
Yes
When a failed execution is detected, the SchedulingHelper will receive this message.
failureIdentifier
Yes
An ID to use to send to the SchedulingHelper at the scheduled time(s)
calendar
No
Calendar to use for this job (see Calendars)
Execution Status
The JobExecutionStatus collection is linked to the JobSpecification collection by the jobSpecificationId field.
The JobExecutionStatus collection is going to change more frequently, because the status of a job changes more frequently than its specification.
It contains the jobSpecificationId, the executionStatus and the updatedTime.
It is an append only collection and in order to get the current job execution status we just need to order by updatedTime and get the latest more recent entry.
Status definitions
The different statuses are:
SCHEDULED: Is scheduled to run (and may have run in the past)
TRIGGERED: Has already run and will not run again
CANCELED: Was previously SCHEDULED but was not triggred
FAILED: Failed to execute
Failed Jobs
Scheduling Module
------------------------------
Scheduling Module
The scheduling module is how to interact with the IPF Persistent scheduler. It offers typical scheduling operations that
you would expect to see on such a tool. They are documented below.
scheduleJob and updateJob - [re]schedule a job
This method takes a JobSpecificationDto, persists the job specification and enqueues the job to run in Quartz with the
relevant scheduler cron expression ("specification") and calendar if present.
Note that if the key for this job already exists, then the existing job will be updated to this new specification.
cancelJob - cancel a job
Takes the JobSpecificationKey only, and any future executions of this job - if any - will not be executed. Please note
that the execution history of the job will be retained.
findJobById - get a job’s latest status
Takes the JobSpecificationKey of a job and returns its specification and latest run status.
Job Specification and Status
Scheduling Status
------------------------------
Scheduling Status
The purpose of this page is to clarify the possible execution statuses for the two different types of jobs as well how these statuses
should be handled in the context of rehydration as well as failure handling. Execution Statuses are stored in a separate collection
which is keyed on JobId and Timestamp, with the JobId corresponding to a valid Job Specification in the Job Specification collection.
Determining if a job is one time or recurrent, can be done looking at the schedulingSpecification (in Job Specification)
One Time Scheduled Job
For a one time scheduled job, the following Execution Statuses apply for a one time scheduled job and the transition between the statuses is as follows:
Rehydration - for one time scheduled job
Rehydration occurs in the event of a failure which causes Quartz to restart. Since there is no persistence in Quartz, it needs to be rehydrated with jobs which have been persisted in the Job Repository.
Candidate statuses for rehydration would be SCHEDULED
Job Failure Handling - for one time scheduled job
When determining if jobs have failed we would need to check for the following statuses: SCHEDULED
Recurrent Scheduled Job
The following Execution Statuses apply for a recurrent scheduled job, the following Execution Statuses apply for
a recurrent between the statuses is as follows:
Rehydration - for recurrent scheduled job
Rehydration occurs in the event of a failure which causes Quartz to restart. Since there is no persistence in Quartz, it needs to be rehydrated with jobs which have been persisted in the Job Repository.
Candidate statuses for rehydration would be :
SCHEDULED
TRIGGERED
FAILED
Job Failure Handling - for recurrent scheduled job
When determining if jobs have failed we would need to check for the following statuses:
SCHEDULED
TRIGGERED
And also the updatedTime from the JobExecutionStatus collection
In addition to considering the Execution Status we also need to consider the following from JobSpecification
latestUpdate
schedulingSpecification
The latestUpdate should be compared to the updatedTime in the JobExecutionStatus if latestUpdate > updatedTime then the
schedulingSpecification  should be considered as pending execution, you can use the getNextValidTimeAfter using
latestUpdate comparing to see if this is consistent with the expected execution.
Otherwise, it should be considered as an existing job i.e. the updateTime  should be compared to the cronExpression - getNextValidTimeAfter
this in conjunction with specifying calling the method against the current time can be used to determine failed job execution by comparing the two dates.
For example:
//Successful job
  //Daily expression with last job run time of 2022-11-18T08:24:00
  // "Now": 2022-11-18T08:26:00
  @Test
  void shouldHaveAfterNowAndAfterLastJobRunResultSame() throws ParseException {
      String dailyExpressionString = "0 24 08 * * ? *";
      CronExpression dailyExpresssionJobRun = new CronExpression(dailyExpressionString);
      LocalDateTime jobLastRunTime = LocalDateTime.parse("2022-11-18T08:24:00");
      LocalDateTime nowTime = LocalDateTime.parse("2022-11-18T08:26:00");
      Date nowDate = Date.from(nowTime.atZone(ZoneId.systemDefault()).toInstant());
      Date nextValidTimeAfterNow = dailyExpresssionJobRun.getNextValidTimeAfter(nowDate);
      Date jobLastRunDate = Date.from(jobLastRunTime.atZone(ZoneId.systemDefault()).toInstant());
      Date nextValidTimeAfterJobRun = dailyExpresssionJobRun.getNextValidTimeAfter(jobLastRunDate);
      String nextValidTimeAfterJobRunString = nextValidTimeAfterJobRun.toString();
      System.out.println("next valid time after last run: " + nextValidTimeAfterJobRunString);
      String nextValidTimeAfterNowString = nextValidTimeAfterNow.toString();
      System.out.println("next valid time after now: " + nextValidTimeAfterNowString);
      assertThat(nextValidTimeAfterJobRunString, is(nextValidTimeAfterNowString));
      // next valid time after last run: Sat Nov 19 08:24:00 GMT 2022
      // next valid time after now: Sat Nov 19 08:24:00 GMT 2022
  }
  //Failed job
  //Daily expression with last job run time of 2022-11-17T08:24:00
  // "Now": 2022-11-18T08:26:00
  @Test
  void shouldHaveAfterNowAndAfterLastJobRunResultDifferent() throws ParseException {
      String dailyExpressionString = "0 24 08 * * ? *";
      CronExpression dailyExpresssionJobRun = new CronExpression(dailyExpressionString);
      LocalDateTime jobLastRunTime = LocalDateTime.parse("2022-11-17T08:24:00");
      LocalDateTime nowTime = LocalDateTime.parse("2022-11-18T08:26:00");
      Date nowDate = Date.from(nowTime.atZone(ZoneId.systemDefault()).toInstant());
      Date nextValidTimeAfterNow = dailyExpresssionJobRun.getNextValidTimeAfter(nowDate);
      Date jobLastRunDate = Date.from(jobLastRunTime.atZone(ZoneId.systemDefault()).toInstant());
      Date nextValidTimeAfterJobRun = dailyExpresssionJobRun.getNextValidTimeAfter(jobLastRunDate);
      String nextValidTimeAfterJobRunString = nextValidTimeAfterJobRun.toString();
      System.out.println("next valid time after last run: " + nextValidTimeAfterJobRunString);
      String nextValidTimeAfterNowString = nextValidTimeAfterNow.toString();
      System.out.println("next valid time after now: " + nextValidTimeAfterNowString);
      assertThat(nextValidTimeAfterJobRunString, not(is(nextValidTimeAfterNowString)));
      // next valid time after last run: Fri Nov 18 08:24:00 GMT 2022
      // next valid time after now: Sat Nov 19 08:24:00 GMT 2022
}
Scheduling Module
Features
------------------------------
Features
Scheduling Metrics
Scheduling Status
Scheduling Metrics
------------------------------
Scheduling Metrics
This page discusses the metrics exposed by IPF’s Persistent Scheduler:
Metric Reference
Metric
Type
Description
scheduling_registered_jobs_total
Counter
Total number of new scheduled jobs registered with the scheduling module. Each unique job ID counts only once irrespective of number of updates or number of re-hydrations
scheduling_updated_jobs_total
Counter
Total number of updates to existing scheduled jobs. Each update to the same job ID increments the counter
scheduling_cancelled_jobs_total
Counter
Total number of explicitly cancelled jobs
scheduling_failed_jobs_total
Counter
Total number of failed jobs due to expiration
scheduling_rehydrations_total
Counter
Total number of re-hydrations due to crash of scheduling module
Enabling Persistent Scheduler Metrics
Metrics are enabled by default and will be present alongside other IPF metrics for connectors, flows, etc. No extra work
is required to enable Persistent Scheduler metrics.
Features
Getting Started
------------------------------
Getting Started
Getting started guides for starting with the IPF Persistent Scheduler.
Scheduling Your First Job
Scheduling Metrics
Scheduling Your First Job
------------------------------
Scheduling Your First Job
There are a few things to set up to start scheduling your first IPF Scheduler job. We’ll do them in the most sensible
order possible.
You will create a command, a JobSpecification that uses that command to define the job to run, and an SchedulingHelper
which gets given that command at the scheduled run time(s).
Step 0: Add dependency
You will need to add this to pom.xml:
<dependency>
    <groupId>com.iconsolutions.ipf.core.platform</groupId>
    <artifactId>scheduler-core</artifactId>
    <version>${ipf-persistent-scheduler.version}</version>
</dependency>
To find the latest version, you can use this Nexus query.
Step 1: Create a command
This is a command that will be sent to your  that has to extend Command.
Here’s an example of one:
    public static class MyCommand implements Command {
        @Override
        public CommandId getCommandId() {
            return CommandId.from("A|B|C");
        }
        @Override
        public Instant getCreatedAt() {
            return Instant.now();
        }
    }
Step 2: Create an SchedulingHelper and define it as a Spring bean
This is the thing that will run your job at a specific time, with the given command from step 1:
    public static class MySchedulingHelper implements SchedulingHelper {
        @Override
        public CompletionStage<Void> execute(String id, Command command) {
            //do some really important work here that can possibly take a long time...or not?
            log.info("Look I'm being scheduled! The ID was: {}", id);
            return CompletableFuture.completedFuture(null);
        }
        @Override
        public boolean supports(Command command) {
            return command instanceof MyCommand;
        }
    }
You will also need to define it as a bean:
        @Bean
        public SchedulingHelper mySchedulingHelper() {
            return new MySchedulingHelper();
        }
Step 3: Schedule the job
Now we tell the SchedulingModuleInterface to schedule our job with our command at a specific time.
In the below example we are running our job every 5 seconds.
    public void scheduleJob() {
        schedulingModuleInterface.scheduleJob(JobSpecificationDto.builder()
                .jobRequestor("test-requestor")
                .jobSpecificationKey(new JobSpecificationKeyDto("my-special-job-wow"))
                .triggerCommand(new MyCommand())
                .triggerIdentifier("my-trigger-id")
                .schedulingSpecification("*/5 * * ? * *")
                .build());
    }
Note that if you want a non-repeating job, you can use singleSchedule and pass in a Calendar instance representing
the desired trigger time, instead of supplying a cron-style schedulingSpecification.
Step 4: Run it!
If we run this application we can see that every 5 seconds our log message is printed out:
28-02-2023 15:29:00.002 [DefaultQuartzScheduler_Worker-1] INFO  c.i.i.c.p.s.persistent.DocsExamples.execute - Look I'm being scheduled! The ID was: my-trigger-id
28-02-2023 15:29:05.001 [DefaultQuartzScheduler_Worker-2] INFO  c.i.i.c.p.s.persistent.DocsExamples.execute - Look I'm being scheduled! The ID was: my-trigger-id
28-02-2023 15:29:10.000 [DefaultQuartzScheduler_Worker-3] INFO  c.i.i.c.p.s.persistent.DocsExamples.execute - Look I'm being scheduled! The ID was: my-trigger-id
28-02-2023 15:29:15.001 [DefaultQuartzScheduler_Worker-4] INFO  c.i.i.c.p.s.persistent.DocsExamples.execute - Look I'm being scheduled! The ID was: my-trigger-id
28-02-2023 15:29:20.000 [DefaultQuartzScheduler_Worker-5] INFO  c.i.i.c.p.s.persistent.DocsExamples.execute - Look I'm being scheduled! The ID was: my-trigger-id
Getting Started
Dynamic Settings
------------------------------
Dynamic Processing Settings
Background
Dynamic Processing Settings provides a platform to manage configurable settings and their lifecycle. This platform provides for defining, maintaining and exposing configurable settings that can be referenced from
an external application such as IPF.
The initial use case which has been used to prove out the framework in the first instance is CSM Reachability.
High Level Project Structure
The following diagram shows an example of the high level structure of components involved in Dynamic Processing Settings.
Platform Projects:
Project
Description
file-ingestion-service
Contains the required infrastructure to consume settings from a source e.g. a file and propagate the consumed settings through to the setting management API via a Process Manager.
setting-domain
Contains generic setting domain and model objects e.g. CreateSetting, UpdateSetting, Setting
setting-management
Contains the API framework which is used to manage the settings (CRUD API)
setting-workflow
Contains the MPS generated artefacts to support lifecycle management via Event Sourced Behaviours
setting-catalogue
Specific settings which have been defined to be managed by the Dynamic Processing Settings framework. Contains supporting infrastructure such as read side models and settings definitions
CSM Reachability Solution Projects:
The CSM Reachability Solution leverages the Platform Projects and defines additional solution specific projects
Project
Description
csm-reachability
Pulls in the relevant platform projects, and contains the validate csm reachability business service, which invokes the setting management APIs in order to determine CSM reachability given a specific input
csm-reachability-app
Wrapper project which pulls in all the relevant modules and runs as a single sprint boot application. It also builds a docker image of the same.
csm-reachability-service
Contains the orchestration service - csm-reachability-service
csm-reachability-setting-management
Leverages setting-management and specific setting definitions relevant to the CSM Reachability Solution
participant-file-handling
Leverages file-ingestion-service and specific setting definitions relevant to the CSM Reachability Solution
csm-reachability-e2e-test
Verifies the CSM Reachability App assembles and runs successfully as a docker image
Documentation
How to create a project based on Dynamic Settings -
An overview of modules needed to assemble an application built on top of dynamic-settings-workflow
Creating a Setting - Example of adding a setting to the dynamic processing settings framework
Workflow Documentation - An overview of all the concepts relating to the dynamic settings workflow
Scheduling Your First Job
Features
------------------------------
Settings Model
This section describes the Dynamic settings flow and its attributes.
Flows
Dynamic Settings
Flow Properties
Flow Name:
Dynamic Settings
Version:
0
Description:
This is a placeholder for the flow solution.
Global State Set:
Default Global States
Flow Graph
States
Name
Description
Global State
Is Terminal
Inactive Approval Pending
Approval Pending for Setting currently inactive
none
No
Active
Active Setting state
none
No
Active Approval Pending
Approval pending state for active setting after update
none
No
Delete Approval Pending
Delete Approval Pending for setting deletion
none
No
Terminal State
State added because it’s required to have terminal state
none
Yes
Events
Name
Description
Business Data
Flow Initiated
The flow has been successfully started.
Provisional Setting
Requires Approval
Approval Received
Raised after setting approved
Active Setting
Provisional Setting
Requires Approval
Approval Rejected
Emited after approval rejected
Provisional Setting
Requires Approval
Setting Deactivated
Setting moved to Inactive state
Active Setting
Requires Approval
Setting Created
Setting has been created
Active Setting
Requires Approval
Setting Created Needs Approval
Setting has been created but needs approval
Provisional Setting
Requires Approval
Setting Updated
Setting has been updated
Provisional Setting
Active Setting
Setting Updated Requires Approval
Update requested requires approval
Provisional Setting
Setting Deactivated Requires Approval
Deactivation request requires approval
Active Setting
Functions
No aggregate functions defined.
Input Behaviour
Input
Response Code
Event Selection
Initiate Dynamic Settings
none
Flow Initiated
Update Setting
none
Decision: Needs Approval
On
On*YES*raiseSetting Updated Requires Approval
On*NO*raiseSetting Updated
Approval Response
Accepted
Approval Received
Approval Response
Rejected
Approval Rejected
Deactivate Setting
none
Decision: Needs Approval
On
On*YES*raiseSetting Deactivated Requires Approval
On*NO*raiseSetting Deactivated
Create Setting
none
Decision: Needs Approval
On
On*NO*raiseSetting Created
On*YES*raiseSetting Created Needs Approval
Event Behaviour
Given State
Criteria
Events
New State
Perform Actions
Initial
On
Flow Initiated
Inactive Approval Pending
Initial
On
Setting Created
Active
Initial
On
Setting Created Needs Approval
Inactive Approval Pending
Call Request: Approve Setting
Inactive Approval Pending
On
Approval Received
Active
Inactive Approval Pending
On
Approval Rejected
Initial
Active
On
Setting Updated Requires Approval
Active Approval Pending
Call Request: Approve Setting
Active
On
Setting Updated
Active
Active Approval Pending
On
Approval Received
Active
Active Approval Pending
On
Approval Rejected
Active
Delete Approval Pending
On
Approval Rejected
Active
Delete Approval Pending
On
Approval Received
Initial
Active
On
Setting Deactivated
Initial
Active
On
Setting Deactivated Requires Approval
Delete Approval Pending
Call Request: Approve Setting
Terminal State
On
Setting Deactivated
Terminal State
Initial
On
Setting Updated
Active
Initial
On
Setting Updated Requires Approval
Inactive Approval Pending
Call Request: Approve Setting
Flow BDD
DynamicSettings-Aborted.story
Meta:
Narrative:
Dynamic Settings
This is a placeholder for the flow solution.
Paths ending in state: Aborted
DynamicSettings-TerminalState.story
Meta:
Narrative:
Dynamic Settings
This is a placeholder for the flow solution.
Paths ending in state: Terminal State
External Domains
Settings
Settings workflow domain
Requests
No requests defined.
Notifications
No notifications defined.
Instructions
Name
Description
Business Data
Deactivate Setting
deactivate
Provisional Setting
Active Setting
Requires Approval
Update Setting
Update setting
Active Setting
Provisional Setting
Requires Approval
Create Setting
Initial command
Provisional Setting
Active Setting
Requires Approval
Approver
This domain is used to handle the approval process
Requests
Name
Description
Business Data
Response
Approve Setting
Submit this setting for approval
Active Setting
Provisional Setting
Name:
Approval Response
Description:
description
Business Data:
Active Setting
 Provisional Setting
 Requires Approval
ResponseCodes:
AcceptOrReject
ReasonCodes:
none
Completing:
Yes
Notifications
No notifications defined.
Instructions
No instructions defined.
Initiation
A system generation domain representation of this domain to allow initiate from external sources.
Requests
No requests defined.
Notifications
No notifications defined.
Instructions
Name
Description
Business Data
Initiate Dynamic Settings
Flow Initiation
Provisional Setting
Requires Approval
Supporting Libraries
Business Data Libraries
Business Data Library
Business Data Library for Dynamic Processing Settings
Name
Description
Data Type
Data Category
Active Setting
Currently Active Setting
com.iconsolutions.ipf.dynamicsettings.domain.Setting
Provisional Setting
Setting to be approved
com.iconsolutions.ipf.dynamicsettings.domain.Setting
Requires Approval
Requires Approval
java.lang.Boolean
Common Event Libraries
No event libraries have been defined
Decision Libraries
Decision Library
No description provided.
Name
Description
Business Data
Outcomes
Needs Approval
Does this action need approval
Requires Approval
YES
NO
Domain Function Libraries
No domain functions have been defined.
Response Code Libraries
No response codes have been defined.
Reason Code Libraries
No reason codes have been defined.
Placeholder Libraries
No placeholders have been defined.
Global States
No global states have been defined.
Features
How to guides
------------------------------
Create a Project Based on Dynamic Settings Workflow
Dynamic settings workflow contains the building blocks that are meant to be used as a starting point to create an application that suits your needs.
Such an application will manage the lifecycle of the settings which you will then use depending on your use case.
Two typical reasons to manage a setting in your application are:
to expose the setting to other services
to use it as part of the business API that you want to expose (e.g. CSM Reachability)
Creating a Setting covers details around adding a new setting.
The structure of your Application
Your application will typically contain the following modules:
file-ingestion module
data-management module
one or more modules that expose business APIs
an application module that assembles the application from the previous modules
File ingestion module
File ingestion module is used to feed the setting data from a source (local directory, REST API…​) through a data management API exposed by the data-management module.
This module is built on top of the generic file-ingestion-service framework.
The framework expects you to configure connectors that would ingest the settings from its source (currently supported transports are local directory/REST API).
Depending on the specification of the source, specific mappers are needed in order to convert the settings into the canonical format.
Data management module
Data management module manages the lifecycle of each supported type of the setting. It encapsulates the write-side of the application.
Module should be built on top of the setting-management framework. Setting-management framework is a generic API framework that exposes the data management API for each of the settings found on the classpath.
You need to provide the relevant settings from the settings-catalogue as dependencies.
Business API module(s)
Depending on the use case, you may need one or more modules that expose business APIs. Such a module would typically query read side collections for one or more settings in order to provide a specific functionality.
CSM Reachability is a good starting point.
How to guides
Create Settings
------------------------------
Create Settings
The following is an example of how to add a setting, in this case a CsmAgent Setting that will be managed by the platform. You need to configure a domain project and a repository project
Domain Project Setup
In order to add a setting to be managed by the Dynamic Processing Settings Platform you need to create the following:
Setting Definition
Domain Object
Search Fields for the setting
Setting Definition
Specifies how to calculate the logical unique key for the setting and associates all the other components (domain object and search fields) to the setting concept
    @Bean
    SettingDefinition csmAgentSettingDefinition(final Notifier systemEventSender) {
        return SettingDefinition.<CsmAgent>builder()
                .name("csmagent")
                .clazz(CsmAgent.class)
                .idFunction(setting -> setting.getProcessingEntity() + "-" + setting.getPayload().getCsmAgentId())
                .approvalFunction((requiresApproval, persistanceId, inputSetting) -> CompletableFuture.completedStage(requiresApproval))
                .searchableFields(CsmAgentSearchableFields.class)
                .notificationFunction(systemEventSender::notify)
                .build();
    }
Domain Object
This will be the payload of a setting object and should contain all the relevant attributes for the setting you wish to define
@Data
@Builder(toBuilder = true)
public class CsmAgent {
    @NotNull
    private String csmAgentId;
    private String csmAgentBic;
    @Size(max = 70)
    private String csmAgentName;
    @NotNull
    @Size(max = 35)
    private String csmAgentType;
    @NotNull
    @Size(max = 15)
    private String csmParticipantIdentifierType;
    @NotNull
    @Size(max = 35)
    private String csmAgentConnector;
    @Size(max = 70)
    private String csmAgentConnectorAddress;
    @Size(min=1)
    @Valid
    private List<CsmAgentMessageStandard> csmAgentMessageStandards;
    private Boolean onUsCSM;
    private Boolean higherParticipantLimitNotAllowed;
    private Boolean instantPayments;
    @Data
    @Builder
    public static class CsmAgentMessageStandard {
        @NotNull
        @Size(max = 35)
        private String messageStandard;
        @NotNull
        @Size(max = 35)
        private String messageStandardVersion;
        @NotNull
        private Instant activeFrom;
    }
    public boolean isHigherParticipantLimitNotAllowed() {
        return BooleanUtils.isTrue(higherParticipantLimitNotAllowed);
    }
    public boolean isOnUsCSM() {
        return BooleanUtils.isTrue(onUsCSM);
    }
    public boolean isInstantPayments() {
        return BooleanUtils.isTrue(instantPayments);
    }
}
Setting class:
@Data
@AllArgsConstructor
@NoArgsConstructor
@Builder
public class Setting <T> implements Serializable {
    private String id;
    @Size(max = 15, min = 1)
    private String processingEntity;
    private Instant activeFromDate;
    private String source;
    private String status;
    private int version;
    private String createdBy;
    private String rejectedBy;
    private String approvedBy;
    @JsonTypeInfo(use = JsonTypeInfo.Id.CLASS, property = "className")
    private T payload;
    @JsonIgnore
    public boolean isActive() {
        return "ACTIVE".equalsIgnoreCase(status);
    }
}
Search Fields
Define the fields which are searchable on the setting, in this case the CsmAgent can be searched by CsmAgentId.
package com.iconsolutions.ipf.dynamicsettings.search;
public enum CsmAgentSettingSearchFields implements SearchField {
    CSM_AGENT_ID;
    @Override
    public String getName() {
        return this.name();
    }
}
In addition to the search fields you define for the setting, all settings are searchable via CommonSearchFields (status, processingEntity, activeFrom and source)
@Data
public class CommonSearchableFields implements SearchableFields {
    private String status;
    private String processingEntity;
    private Instant activeFrom;
    private List<String> idList;
    @Pattern(regexp = "import|manual", flags = Pattern.Flag.CASE_INSENSITIVE)
    private String source;
    public CommonSearchableFields populateFromRequest(ServerRequest serverRequest) {
        CommonSearchableFields commonSearchableFields = newInstance();
        serverRequest.queryParam("status").ifPresent(commonSearchableFields::setStatus);
        serverRequest.queryParam("processingEntity").ifPresent(commonSearchableFields::setProcessingEntity);
        serverRequest.queryParam("source").ifPresent(commonSearchableFields::setSource);
        serverRequest.queryParam("activeFrom").ifPresent(activeFrom1 -> commonSearchableFields.setActiveFrom(Instant.parse(activeFrom1)));
        return commonSearchableFields;
    }
    public CommonSearchableFields newInstance() {
        return new CommonSearchableFields();
    }
    @Override
    public List<Criterion> criteria() {
        final List<Criterion> criteria = new ArrayList<>();
        if (status != null) {
            criteria.add(Criterion.equalTo(SettingSearchFields.STATUS, status));
        } else {
            criteria.add(Criterion.notEqualTo(SettingSearchFields.STATUS, "INITIAL"));
        }
        if (activeFrom != null) {
            criteria.add(Criterion.gte(SettingSearchFields.ACTIVE_FROM, activeFrom));
        }
        if (source != null) {
            criteria.add(Criterion.equalTo(SettingSearchFields.SOURCE, source));
        }
        if (processingEntity != null) {
            criteria.add(Criterion.equalTo(SettingSearchFields.PROCESSING_ENTITY, processingEntity));
        }
        if(idList != null) {
            criteria.add(Criterion.in(SettingSearchFields.ID, idList));
        }
        return criteria;
    }
}
The below tells the framework how to extract the search fields from the requests received
@Data
public class CsmAgentSearchableFields extends CommonSearchableFields {
    private String csmAgentId;
    @Override
    public CsmAgentSearchableFields populateFromRequest(ServerRequest serverRequest) {
        CsmAgentSearchableFields searchableFields = (CsmAgentSearchableFields) super.populateFromRequest(serverRequest);
        serverRequest.queryParam("csmAgentId").ifPresent(searchableFields::setCsmAgentId);
        return searchableFields;
    }
    @Override
    public CsmAgentSearchableFields newInstance() {
        return new CsmAgentSearchableFields();
    }
    @Override
    public List<Criterion> criteria() {
        final List<Criterion> criteria = new ArrayList<>(super.criteria());
        if (csmAgentId != null) {
            criteria.add(equalTo(CsmAgentSettingSearchFields.CSM_AGENT_ID, csmAgentId));
        }
        return criteria;
    }
}
You also need to update the search fields map which specifies the path to the searchable field from the perspective of a setting
    @PostConstruct
    void updateSearchFieldsMap() {
        settingSearchFieldsMapper.putMapping(CsmAgentSettingSearchFields.CSM_AGENT_ID.getName(), "payload.csmAgentId");
    }
Repository Project Setup
Additionally, the following read side infrastructure needs to be defined:
Repository
ModelEntity
ModelEntityProvider
IndexInitialiser
Repository
Repository, which extends ReactiveCRUDRepository and exposes the query functionality of the setting stored in the database
public interface CsmAgentSettingsRepository extends SettingRepository<CsmAgentSettings> {
    String CSMAGENT = "csmagent-";
    Flux<CsmAgentSettings> findAll(Sort sort);
    @Override
    default boolean supports(String id) {
        return id.toLowerCase().contains(CSMAGENT);
    }
}
ModelEntity
ModelEntity, defines how the setting will be represented in the DB and also defines how the payload for the settings is created/updated
@Document(collection = "settings-csm-agent")
@Data
public class CsmAgentSettings extends MongoSettingReadModelEntity<CsmAgent> {
    @Override
    protected Supplier<CsmAgent> payloadCreator() {
        return () -> CsmAgent.builder().build();
    }
    @Override
    protected BiFunction<Event, CsmAgent, CsmAgent> payloadUpdater() {
        return (event, csmAgent) -> csmAgent;
    }
}
ModelEntityProvider
ModelEntityProvider, is responsible for creating the appropriate ModelEntity, based on the identifier that is input
@Component
public class CsmAgentMongoSettingModelEntityProvider implements MongoSettingModelEntityProvider {
    // "-" suffix added to avoid partial match e.g. csmagent matching csmagentcurrency
    private static final String CSMAGENT = "csmagent-";
    @Override
    public MongoSettingReadModelEntity provide() {
        return new CsmAgentSettings();
    }
    @Override
    public Class<? extends MongoSettingReadModelEntity> getEntityClazz() {
        return CsmAgentSettings.class;
    }
    @Override
    public boolean supports(String id) {
        return id.toLowerCase().contains(CSMAGENT);
    }
}
IndexInitialiser
Index Initialiser, is responsible for creating indexes on the collection
@Slf4j
@AllArgsConstructor
public class CsmAgentMongoSettingRecordIndexInitialiser {
    private static final String STATUS = "status";
    private static final String PROCESSING_ENTITY = "processingEntity";
    private static final String PAYLOAD_CSM_AGENT_ID = "payload.csmAgentId";
    private static final String COLLECTION_NAME = "CsmAgentSettings";
    private final ReactiveMongoTemplate reactiveMongoTemplate;
    private final RepositoryRetryProvider repositoryRetryProvider;
    @EventListener(ContextRefreshedEvent.class)
    public void initialise() {
        log.info("creating indexes");
        final ReactiveIndexOperations indexOperations = reactiveMongoTemplate
                .indexOps(CsmAgentSettings.class);
        createIndex(indexOperations, STATUS, COLLECTION_NAME, repositoryRetryProvider);
        createIndex(indexOperations, PROCESSING_ENTITY, COLLECTION_NAME, repositoryRetryProvider);
        createIndex(indexOperations, PAYLOAD_CSM_AGENT_ID, COLLECTION_NAME, repositoryRetryProvider);
    }
}
Create a Project Based on Dynamic Settings Workflow
Bulker
------------------------------
IPF Bulker
"The ability to aggregate payment instructions into one or more bulk files, each containing multiple transactions, potentially between multiple debtors and creditors."
If an IPF implementation needs to store a collection of data elements in a structured file format, the Bulker will provide this functionality.
The individual features and characteristics of the Bulker are covered in the Concepts page.
Create Settings
Concepts
------------------------------
Concepts
The Bulker is responsible for bringing together individual transactions or components, and acts initially as a temporary storage area where the main IPF flow can store elements that will eventually end up in the structured file. When instructed, the Bulker will stream each item, in a preconfigured order, to a file at a predefined location.
Along with the items themselves, the Bulker can use pre-configured templates to include headers, footers and intermediary constructs in the final file.
The trigger for streaming the stored elements to file can be a "manual" command sent from the IPF Implementation that set-up the Bulk and provided the elements, or, it can come automatically from a variety of sources. Automatic finalisation can be time based in Scheduled or periodic intervals, or it could be based on the characteristics of the Bulk such as number of elements or  estimated total size of the output file. The method of finalisation is defined at the point the Bulk is first created.
In a situation when elements will continue to be generated and need to be stored, even after a Bulk has been finalised and a file created, the Bulk can be given the "Recurrance" characteristic. In this situation the closing of one Bulk will automatically trigger the creation of a new Bulk, with identical configuration, for subsequent elements to be added to.
The structure of the output file needs to be provided in a template in the configuration of the Bulk. This configuration will instruct the Bulker where elements need to end up in the final documentation and any ordering that may be necessary.
As well as the ability to build Bulk files, the Bulker provides housekeeping features to ensure memory and storage is freed after files are produced and also a host of enquiry API’s in order to be able to find out the size, status and structure of the Bulk while it grows.
Bulker
Features
------------------------------
Features
This section outlines all the features that are needed, and should be considered, when creating and configuring a Bulk
Akka Bulker Aggregate
Bulk Aggregate
Bulk Element Adder* Bulk Initiation
Bulk Initiation
Bulker Finalisation
Component Parser
New Bulk Notification
Default
Bulk Outputstream Provider
Delivered Bulk Notification
Bulk Auto Closed Notification
Concepts
Akka Bulker Aggregate
------------------------------
Akka Bulker Aggregate
Akka Bulker Aggregate is a BulkAggregate implementation which uses akka event sourcing for ingesting bulk components and perform validation.
Maven Dependency
To use the Akka Bulk Aggregate, the following dependency must be provided, with a version matching ipf-bulker to ensure compatibility.
    <dependency>
        <groupId>com.iconsolutions.ipf.bulk</groupId>
        <artifactId>ipf-bulker-aggregate-akka</artifactId>
        <version>${ipf-bulker.version}</version>
    </dependency>
Features
Bulk Aggregate
------------------------------
Bulk Aggregate
Purpose of Bulker Aggregate is to create single or recurring bulks, validating and aggregating components that will be used by the Bulk Producer to create the bulk file.
Single Bulk Aggregate
Single Bulk aggregate is a component whose purpose is to create a new bulk, aggregate bulk components and make sure those components are valid. After all bulk components are collected by the aggregate, bulking can be started.
Interface
The BulkAggregate interface is defined as follows.
public interface BulkAggregate {
    CompletionStage<BulkIdResponse> createBulk(CreateBulkCommand command); (1)
    CompletionStage<BulkComponentIdResponse> addComponent(AddComponentCommand command); (2)
    CompletionStage<Response> updateComponent(UpdateComponentCommand command); (3)
    CompletionStage<Response> removeComponent(RemoveComponentCommand command); (4)
    CompletionStage<Response> closeBulk(CloseBulkCommand command); (5)
    CompletionStage<Response> openBulk(OpenBulkCommand command); (6)
    CompletionStage<Response> finaliseBulk(FinaliseBulkCommand command); (7)
    CompletionStage<BulkReportResponse> getBulkReport(GetBulkReportCommand command); (8)
    CompletionStage<Response> terminateBulk(TerminateBulkCommand command); (8)
}
1
createBulk is used to create a new bulk.
Returns BulkIdResponse with BulkId when bulk creation was successful
2
addComponent is used to add new component to the bulk aggregate, first component needs to be root component.
Returns BulkComponentIdResponse with SUCCESS result and BulkComponentId when adding component is successful
3
updateComponent updates the component that which is present in the aggregate with the new content.
Returns Response with SUCCESS result if the component update is successful
4
removeComponent deletes the component which is present in the aggregate.
Returns Response with SUCCESS result if the component removal is successful
5
closeBulk Closes the aggregate, preventing client to remove and add new components to the aggregate, but allows updating components which are in the aggregate.
Returns Response with SUCCESS result if the bulk is closed
6
openBulk Re-opens the aggregate which is closed.
Returns Response with SUCCESS result if the bulk is opened
7
finaliseBulk The point of no return, this is the signal to start processing data in the aggregate and perform bulking.
Returns Response with SUCCESS result if the bulk is finalised
8
terminateBulk remove a whole bulk which is present in aggregate.
Returns Response with SUCCESS result if the bulk removal is successful
Recurring Bulk Aggregate
Recurring Bulk Aggregate is a component for creating recurring bulk. It is responsible for delegating components to currently open Bulk Aggregate, and when that Bulk Aggregate gets finalised for any reason, it will create new Bulk Aggregate with root component and forward components to it.
Interface
The RecurringBulkAggregate interface is defined as follows.
public interface RecurringBulkAggregate {
    CompletionStage<CurrentOpenBulkResponse> createBulk(CreateBulkCommand command); (1)
    CompletionStage<RecurringBulkComponentIdResponse> addComponent(AddComponentCommand command); (2)
}
1
createBulk is used to create a new recurring bulk.
Returns CurrentOpenBulkResponse with BulkId of the current single bulk and root component id to which components will be sent via addComponent method
2
addComponent is used to add new component to the current open bulk.
Returns RecurringBulkComponentIdResponse with SUCCESS result and BulkId of the single bulk aggregate to which component was added, and the BulkComponentId of the added component.
Limited bulk specification support
Since recurring bulk aggregate is responsible for creating a new single bulk, it is also responsible for generating the root component for created single bulk, and the client can send only components which will be added as child components of that root component. Client will need to create an implementation of ComponentGenerator which will be responsible for generation of single bulk root components created by recurring bulk aggregate.
public interface ComponentGenerator {
    String generateComponent(String componentName);
}
1
generateComponent is used to create a root component for the single bulk created by the recurring bulk.
It should return generated component based on the passed componentName. componentName which is being passed by the recurring bulk aggregate is the recurring bulk id.
ERROR Codes
When client sends invalid commands Response with FAILURE result will be returned, and it will also contain the Error.
List of error codes:
Error
Message
Description
For Command
Aggregate type
AC01
Parent id present for root component
Root component shouldn’t have parent
AddComponentCommand
Single
AC02
Parent doesn’t exist
Returned if the parent is not present in the bulk aggregate (doesn’t apply to root component)
AddComponentCommand
Single
AC03
Path not valid or not present in the BulkSpecification
Returned if component path is not defined in the BulkSpecification
AddComponentCommand
Single
AC04
Content not present
Returned if content is null or empty
AddComponentCommand
Single
UC01
Component doesn’t exist
Component with BulkComponentId not present in the Bulk Aggregate
UpdateComponentCommand
Single
UC02
Content not present
Returned if content is null or empty
UpdateComponentCommand
Single
RC01
Component doesn’t exist
Component with BulkComponentId not present in the Bulk Aggregate
RemoveComponentCommand
Single
RC02
Root component can’t be deleted
Returned if component can’t be removed because it is a root component
RemoveComponentCommand
Single
RC03
Component has child components
Returned when component can’t be removed because it has child components
RemoveComponentCommand
Single
RB01
"Recurring bulk id not valid"
Returned if recurring bulk id is not provided
CreateBulkCommand
Recurring
RB02
Recurring bulk specification not valid
Returned if BulkSpecification is not valid.
CreateBulkCommand
Recurring
RB03
Unknown bulk marked as closed
The command is not supported in the current state
On any other commands
Recurring
NSC03
Commmand is not supported
The command is not supported in the current state
On any other commands
Implementations
As with other IPF libraries, default implementations for the most common use-cases are already provided.
Links to the documentation for each implementation are listed below.
Akka
Akka Bulker Aggregate
Bulk Element Adder
------------------------------
Bulk Element Adder
This provides a receive connector that consumes from a kafka topic a bulk element information containing the content, path, bulkId and parentId,
When the bulk element is received, the receiver is passing the information to an adapter interface in order to create the new bulk element.
Config
Type
Comment
Default
bulk-element-adder.transport
String
Property which defines which transport will be used.
kafka
bulk-element-adder.kafka.consumer.topics.new-bulk-request
String
The topic from which messages will be consumed.
BULK_ELEMENT_ADDER_REQUEST
Bulk Aggregate
Bulk Initiation
------------------------------
Bulk Initiation
This provides a receive connector that consumes from a kafka topic a bulk initiation request  information containing the bulkId and config.
When the bulk information is received, the receiver is passing the information to an adapter interface in order to initialise a new bulk.
Config
Type
Comment
Default
bulk-initiation.transport
String
Property which defines which transport will be used.
kafka
bulk-initiation.kafka.consumer.topics.new-bulk-request
String
The topic from which messages will be consumed.
BULK_INITIATION_REQUEST
Bulk Element Adder
Bulker Finalisation
------------------------------
Bulker Finalisation
Initiate finalisation of the bulk (identified by its ID). During finalisation all the necessary supporting structures within the Bulk are finished and validated. Any amendments which need to be made to a parent level element are made at this time. For example, in a three-level hierarchy, both the top level message component ("grandparent") and the mid-level message component ("parent") can be modified, but the bottom level message component ("child") cannot be modified because it has no children of its own.
Finalisation does not edit the content of individual child elements, only the headers (ie counts and cumulative totals) and parents elements that ensure the children are accounted for and accessible.
Finalisation itself does not create an output File, but could call the "Produce Bulk" method.
Config
Type
Comment
Default
bulk-finalisation.transport
String
Property which defines which transport will be used.
kafka
new-bulk.kafka.consumer.topics.bulk-finalisation
String
The topic from which messages will be consumed.
BULK_FINALISATION_REQUEST
Bulk Initiation
Automatic Finalisation of a Bulk
------------------------------
Automatic Finalisation of a Bulk
Automatic finalisation can be time based in Scheduled or periodic intervals, or it could be based on the characteristics of the Bulk(Auto Close Triggers) such as number of elements or  estimated total size of the output file.
The method of finalisation is defined at the point the Bulk is first created. Currently, we support three types for automatic finalisation.
A bulk may be configured to automatically enter the finalisation state when the following criteria is met:
periodic (i.e. 20 seconds after bulk is created)
scheduled time (e.g. at midnight)
auto close triggers (e.g. fullnessAutoCloseTrigger (finalise the bulk after reaching the maximum number of components))
Example:
scheduled-auto-close = {
auto-close-by-age = 20s schedule-at = "*/10 * * ?
* *" }
auto-close-triggers = ["fullnessAutoCloseTrigger"]
Config
Type
Default
Comment
scheduled-auto-close.auto-close-by-age
Duration
0
Duration value that defines when the automatic finalization of the bulk will be scheduled with the help of the Scheduler. The values can be anything supported by the java.time.
Duration class, if we want to exclude this option from the function, we need to specify 0s for the value.
scheduled-auto-close.schedule-at
String
" " Empty string
A CRON expression is entered for the value, which is parsed and based on which the automatic closing of the bulk is scheduled.
If we want to turn off this function, we need to specify "" for the value
auto-close-triggers
List<String>
[] Empty array
auto-close-triggers specify list of triggers that send the bulk to the finalized state after the certain criteria have been met.
The values can be string name for concrete implementation of AutoCloseTrigger interface.
If we want to exclude this option from the function, we need to specify [] for the value.
finalise-on-auto-close
boolean
true
boolean value that describes whether the bulk should be finalized after automatically closed by any trigger(AutoClose) default value true.
Important: scheduled-auto-close.auto-close-by-age, scheduled-auto-close.schedule-at and auto-close-triggers are configured at the bulk level and are part of the overall configuration for that bulk
Example of configuration:
output
ipf.bulker {
  configurations = [
    {
      name = "pain.001.001.09"
      file-name-prefix = "bulk-"
      component-hierarchy {
        component-parser-name = "xml"
        marker = "Document"
        children = [
          {
            marker = "CstmrCdtTrfInitn.PmtInf"
            children = [
              {
                before-elements = ["SplmtryData"]
                marker = "CdtTrfTxInf"
              }
            ]
          }
        ]
      }
      auto-close-triggers = ["fullnessAutoCloseTrigger"]
      scheduled-auto-close = {
        auto-close-by-age = 30s
        schedule-at = "*/10 * * ? * *"
      }
      finalise-on-auto-close = true
    }
]
}
Choice of which time to use
If auto-close-by-age is specified and schedule-at is turned off(empty string "") as an option, the time from auto-close-by-age will be used
If schedule-at is specified and auto-close-by-age is turned off(0s) as an option, the time from schedule-at will be used
If both values are specified for scheduling, the value closest to the bulk creation time will be used for auto close scheduler
If we want to turn off the automatic closing of the bulk, it is necessary to configure auto-close-by-age = 0s and schedule-at = ""
@RequiredArgsConstructor
@Value
public class ScheduleAutoClose {
    Duration autoCloseByAge;
    String scheduleAtCron;
    @SneakyThrows
    public Instant scheduleAt(Instant createdAt) {
        if (autoCloseByAge == null && (scheduleAtCron == null || scheduleAtCron.isEmpty())) {
            throw new IllegalStateException("At least one of autoCloseByAge or scheduleAtCron must be set");
        }
        if (autoCloseByAge == null) {
            CronExpression cronExpression = new CronExpression(scheduleAtCron);
            return Objects.requireNonNull(
                            cronExpression.getNextValidTimeAfter(
                                    Date.from(createdAt.atZone(ZoneId.systemDefault()).toInstant())))
                    .toInstant();
        }
        if (scheduleAtCron == null || scheduleAtCron.isEmpty()) {
            return createdAt.plus(autoCloseByAge);
        }
        CronExpression cronExpression = new CronExpression(scheduleAtCron);
        Instant cronInstant = Objects.requireNonNull(
                cronExpression.getNextValidTimeAfter(
                                Date.from(convertToTimeZonedInstant(createdAt)))
                        .toInstant());
        Instant byAge = createdAt.plus(autoCloseByAge);
            return cronInstant.isBefore(byAge) ? cronInstant : byAge;
    }
    private Instant convertToTimeZonedInstant(Instant createdAt) {
        TimeZone.setDefault(TimeZone.getTimeZone("UTC"));
        return Date.from(createdAt).toInstant();
    }
}
// end::class]
The ClientComponent interface is defined as follows.
public interface AutoCloseTrigger {
    boolean isTriggered(Bulk bulk, BulkComponent component); (1)
    AutoCloseTriggerType getName(); (2)
}
1
isTriggered
Defines the condition after which the bulk is sent to the finalized state
2
getName
Name of the trigger
Bulker Finalisation
Component Parser
------------------------------
Component Parser
A ComponentParser is a pluggable component whose purpose is to detect byte position where child components will be injected into that component. Those positions are called Insertion Points.
A component hierarchy is used to define how each component relates to each other, and where within one component’s content its child component’s content should be placed.
Concrete implementation are expected to know the format of the content of a component, i.e. xml, json , csv, etc.. This is required, since the joining of the components is dependent on the way the data is structured. That is why a separate ComponentParser implementation is going to be needed for each format.
Interface
The ComponentParser interface is defined as follows.
public interface ComponentParser {
    String getName(); (1)
    List<InsertionPoint> parse(String content, Node node); (2)
}
1
getName returns a name of the ComponentParser.
2
parse takes a bulk component Content, a bulk specification Node and returns a InsertionPoint list.
Implementations
Links to the documentation for each implementation are listed below.
XML
Automatic Finalisation of a Bulk
New Bulk Notification
------------------------------
New Bulk Notification
This provides a receive connector that consumes new bulk notification together with bulk component ids from which a new bulk is to be composed.
When the NewBulkNotification notification is received, the process of composing a new bulk is triggered.
Config
Type
Comment
Default
new-bulk.transport
String
Property which defines which transport will be used.
kafka
new-bulk.kafka.consumer.topics.new-bulk-request
String
The topic from which messages will be consumed.
NEW_BULK_REQUEST
Component Parser
Default
------------------------------
Default
The DefaultJoiner provided by IPF is implemented using the StAX API, which is included within the standard Java language.
The joiner has a dependency on the component store where it is expected to be already populated with components that, when combined with a component hierarchy to define the components relationships, can be merged together to form a valid XML document that can be ingested by another system.
Usage Example
This usage example follows the same scenario used to demonstrate the XML Splitter but in reverse.
We will populate a component store, create a component hierarchy to determine how the components should be joined together and finally provide an output stream so that the joined content can be streamed out to wherever it needs to go.
This example joins together only a handful of components for demonstrative purposes, but this can scale to many more components.
Below are each of the component’s content that is persisted within the component store.
example-library-component.xml
<library>
    <name>Library of Alexandria</name>
</library>
example-book-component-1.xml
<book>
    <author>Martin, Robert</author>
    <title>Clean Code</title>
</book>
example-book-component-2.xml
<book>
    <author>Bloch, Joshua</author>
    <title>Effective Java</title>
</book>
example-chapter-component-1.xml
<chapter>
    <name>Clean Code</name>
    <startPage>1</startPage>
</chapter>
example-chapter-component-2.xml
<chapter>
    <name>Meaningful Names</name>
    <startPage>17</startPage>
</chapter>
example-chapter-component-3.xml
<chapter>
    <name>Introduction</name>
    <startPage>1</startPage>
</chapter>
example-chapter-component-4.xml
<chapter>
    <name>Creating and Destroying Objects</name>
    <startPage>5</startPage>
</chapter>
Let’s write an example program to first load the components into the component store and then process them using the XML Joiner.
ComponentStore<List<InsertionPoint>> componentStore = new InMemoryComponentStore<>();
// Create component hierarchy
var rootNode = Node.root("library", "xml");
var bookNode = rootNode.createChild("book", Collections.emptyList());
var chapterNode = bookNode.createChild("chapter", Collections.emptyList());
// Populate the component store
BulkId bulkId = BulkId.random();
var root = Component.<List<InsertionPoint>>builder()
        .bulkId(bulkId).id(ComponentId.of(bulkId.getValue()))
        .index(0L).marker("library")
        .content(readResourceFile("example-root-component.xml"))
        .custom(List.of(new InsertionPoint(bookNode, 49))).build();
var book1 = Component.<List<InsertionPoint>>builder()
        .bulkId(bulkId).id(ComponentId.random()).parentId(root.getId())
        .index(1L).marker("library.book")
        .content(readResourceFile("example-book-component-1.xml"))
        .custom(List.of(new InsertionPoint(chapterNode, 73))).build();
var book2 = Component.<List<InsertionPoint>>builder()
        .bulkId(bulkId).id(ComponentId.random()).parentId(root.getId())
        .index(2L).marker("library.book")
        .content(readResourceFile("example-book-component-2.xml"))
        .custom(List.of(new InsertionPoint(chapterNode, 76))).build();
var chapter1 = Component.<List<InsertionPoint>>builder()
        .bulkId(bulkId).id(ComponentId.random()).parentId(book1.getId())
        .index(3L).marker("library.book.chapter")
        .content(readResourceFile("example-chapter-component-1.xml"))
        .custom(Collections.emptyList()).build();
var chapter2 = Component.<List<InsertionPoint>>builder()
        .bulkId(bulkId).id(ComponentId.random()).parentId(book1.getId())
        .index(4L).marker("library.book.chapter")
        .content(readResourceFile("example-chapter-component-2.xml"))
        .custom(Collections.emptyList()).build();
var chapter3 = Component.<List<InsertionPoint>>builder()
        .bulkId(bulkId).id(ComponentId.random()).parentId(book2.getId())
        .index(5L).marker("library.book.chapter")
        .content(readResourceFile("example-chapter-component-3.xml"))
        .custom(Collections.emptyList()).build();
var chapter4 = Component.<List<InsertionPoint>>builder()
        .bulkId(bulkId).id(ComponentId.random()).parentId(book2.getId())
        .index(6L).marker("library.book.chapter")
        .content(readResourceFile("example-chapter-component-4.xml"))
        .custom(Collections.emptyList()).build();
Mono.zip(
        Mono.fromCompletionStage(componentStore.save(root)),
        Mono.fromCompletionStage(componentStore.save(book1)),
        Mono.fromCompletionStage(componentStore.save(book2)),
        Mono.fromCompletionStage(componentStore.save(chapter1)),
        Mono.fromCompletionStage(componentStore.save(chapter2)),
        Mono.fromCompletionStage(componentStore.save(chapter3)),
        Mono.fromCompletionStage(componentStore.save(chapter4))
).block();
Joiner joiner = new XmlJoiner(componentStore);
OutputStream stream = new ByteArrayOutputStream();
var rootId = BulkComponentId.of(root.getId().getValue());
Mono.fromCompletionStage(joiner.join(rootId, rootNode, stream)).block(Duration.ofSeconds(5));
String output = stream.toString();
System.out.println(output);
Running this code should print out the following to the console.
The whitespace formatting may look a bit different due to the way the components are appended to the stream, but the content should effectively be the same.
output
<library>
    <name>Library of Alexandria</name>
    <book>
        <author>Martin, Robert</author>
        <title>Clean Code</title>
        <chapter>
            <name>Clean Code</name>
            <startPage>1</startPage>
        </chapter>
        <chapter>
            <name>Meaningful Names</name>
            <startPage>17</startPage>
        </chapter>
    </book>
    <book>
        <author>Bloch, Joshua</author>
        <title>Effective Java</title>
        <chapter>
            <name>Introduction</name>
            <startPage>1</startPage>
        </chapter>
        <chapter>
            <name>Creating and Destroying Objects</name>
            <startPage>5</startPage>
        </chapter>
    </book>
</library>
New Bulk Notification
Getting started
------------------------------
IPF Debulker
The IPF Debulker provides the ability to process a bulk files which contains multiple messages and  transactions, potentially between multiple debtors and creditors. The Debulker takes the responsibility for receiving the file bulk and breaking it into components. It allows large (bulked) files to be processed by IPF and does not perform activities which modify the incoming data (e.g. enrichment, validation). Thus, it is a technical enablement feature to support large, multi-transaction files.
Once the file is split, those components can then be processed by individual component IPF flows, meaning we can split a PAIN001 or PACS008 into individual credit transfer instruction components and process each separately. The Debulker itself does not provide those processing flows, but we do provide guides and 'how to' sections as examples or starters.
Explore the concepts, features and guides:
Concepts
Features
Getting started
Getting started
Concepts
------------------------------
Concepts
Introduction
At a high-level the Debulker will transformation an incoming file into the component store. It does this by either polling for the file or receiving a notification that a new file is available for Debulking processing. The file can them be streamed and pushed through the appropriate splitter which will publish a stream of events containing smaller chunks (components).
Key Concepts
The following are key concepts which are explored in more detailed within the linked features section. They are explained here to show how these concepts and features relate to each other.
Debulking Configuration
Every type of bulk file to be processed requires a specific configuration, to tell the Debulker what sort of file format to expect (e.g. XML, Json) and crucially a component hierarchy which provides the tree structure of that specific type of file. This tells the debulker how to break the file apart, to split it, into its component parts.
File Notification
There are two ways the Debulker can learn that there is a file ready for processing. The first is via a notification, an API is provided which is essentially a receive connector. The Debulker comes with a Kafka implementation of this receive connector. Thus an implementation could have another process or script run to send a Kafka event to a specific topic, thus communicating a new file is ready for processing.
File Polling
The second way for a file to be fed into Debulker processing is to configure a File Poller, which will poll at a defined frequency for new files. The File Poller can also be used to sweep up missed files, whereby you could configure it to look for files not yet processed (useful in the case that file notifications could not reliably be sent).
Input Stream
The Debulker provides a pluggable component whose purpose is to take a FileDefinition and return an InputStream. This decouples the debulker from the underlying details of the file storage, and allows a range of storage options (e.g. file system most commonly or S3 bucket).
File Processing Uniqueness
We typically want the files to be processed once and only once, thus the debulker has the option to configure a duplicate check. This is based on the entire contents of the file and will stop processing a file which it has seen before.
Splitter
A Splitter is a pluggable component, where most of the Debulker work is done. The Splitter takes a stream of data (current from a large file); and publishes a stream of events containing smaller chunks (components).
Component Store
The File Component Store is a plugable component and represents the 'place' where payment components are stored. Typically this will be a Mongo backed store, but could equally be implemented or swapped for another implementation.
Client Processing Notification
This is enabled using a pluggable component which sends notifications to a client indicating that components generated by the debulker are ready for processing.
Housekeeping
Housekeeping functionality exists to remove components which have been processed by the client flows.
Debulker
Features
------------------------------
Features
Bulk Input Stream Provider
Local Filesystem
Client Processing
Client Processing Kafka
Debulker File Archiver
Local Filesystem
S3 Archiver
Debulking Configuration
Housekeeping component remover
Housekeeping file deleter
Housekeeping - scheduler
New File Notification
New file polled connector
Splitter
JSON Splitter
XML Splitter
Duplicate check
Concepts
Bulk Input Stream Provider
------------------------------
Bulk Input Stream Provider
A BulkInputStreamProvider is a pluggable component whose purpose is to take a FileDefinition and return an InputStream.
This decouples the debulker from the underlying details of the file storage and consequently.
Since the debulker allows multiple input sources, the BulkInputStreamProvider is expected to return a name that uniquely identifies it.
Interface
The BulkInputStreamProvider interface is defined as follows.
public interface BulkInputStreamProvider {
    String getName(); (1)
    InputStream stream(FileDescriptor fileDescriptor); (2)
}
1
getName is used to uniquely identify the implementation.
2
stream takes a FileDescription returns an InputStream.
Implementations
As with other IPF libraries, default implementations for the most common use-cases are already provided.
Links to the documentation for each implementation are listed below.
Local Filesystem
Features
Local Filesystem
------------------------------
Local Filesystem
An implementation of BulkInputStreamProvider for reading files from the local filesystem.
Maven Dependency
To use the LocalFilesystemBulkInputStreamProvider, the following dependency must be provided, with a verison matching ipf-debulker-core to ensure compatibility.
<dependency>
    <groupId>com.iconsolutions.ipf.debulk</groupId>
    <artifactId>ipf-debulker-bulk-input-stream-local</artifactId>
    <version>${ipf-debulker-core.version}</version>
</dependency>
Bulk Input Stream Provider
Client Processing
------------------------------
Client Processing
Client Processing is a pluggable module which purpose is sending notification that components generated by debulker are ready to be processed, and handling notifications that components are processed so that debulker can perform housekeeping operations.
Interface
The ComponentProcessingInitiationPort interface is defined as follows, and it is used to send notification that components are ready to be processed.
public interface ComponentProcessingInitiationPort {
    CompletionStage<Void> initiateProcessing(InitiateComponentProcessingCommand command); (1)
}
1
initiateProcessing is used to send InitiateComponentProcessingCommand.
The ComponentProcessingCompletionPort interface is defined as follows, and it will handle notification that processing of components is completed. Application which uses this module will need to implement it.
public interface ComponentProcessingCompletionPort {
    CompletionStage<Void> processingComplete(ComponentProcessingCompleteCommand command); (1)
}
1
processingComplete is used to handle InitiateComponentProcessingCommand.
Implementations
We use connector library to send and consume messages to/from specific transport. Currently, there is only - kafka implementation.
Local Filesystem
Client Processing Kafka
------------------------------
Client Processing Kafka
It consists of:
A SendConnector implementation which serializes InitiateComponentProcessingCommand as json and sends it to kafka. Root configuration path for this connector is ipf-debulker.client-processing.
A ReceiveConnector which consumes ComponentProcessingCompleteCommand and passes it to ComponentProcessingCompletionPort.
Configuration
Config
Type
Comment
Default
ipf.debulker.client-processing.connector.kafka.producer.topic
String
The topic to which InitiateComponentProcessingCommand will be sent.
CLIENT_PROCESSING_REQUEST
ipf.debulker.client-processing.connector.kafka.consumer.topic
String
The topic from which ComponentProcessingCompleteCommand will be consumed.
CLIENT_PROCESSING_RESPONSE
Client Processing
Debulker File Archiver
------------------------------
Tutorial Introduction
Getting Started
Using an Icon Provided AWS Environment
When using the Icon supplied AWS environments, the setup is completed for you and there’s nothing else to do.  The core project has been downloaded onto the desktop for you in the 'ipf-tutorial' folder.  At various times, the tutorials will refer to the '<install-root>' of the tutorial project - for you this is just his desktop/ipf-tutorial folder.
If you have a look in there, you’ll see a 'solutions' folder containing the various solutions to each step of the tutorial.  You’ll start by using the 'initial' solution.
Setting up your own Environment
If you’re using your own environment, you’ll need to make sure you have the following installed before starting this tutorial:
Maven - version 3.8.4 or higher (Maven)
MPS/ Flo-designer - version 2021.3 (MPS)
Intellij (or other IDE) (IntelliJ)
Java 11 (JDK11)
You’ll also need access to the project repositories are stored in bitbucket.  We recommend you use a git client for retrieving the projects, although they can be downloaded through the website also.  Please take a note of the location you install the project too as this will be needed in the later tutorials.
If you can’t access the tutorial repository, please contact Icon support (support@iconsolutions.com) who will be able to assist.
The preferred approach to run the sample application is using containers: docker containers. However, the tutorial can run as a standard Java application providing that access is available to MongoDB and Kafka.
Concepts to be Aware of
Please note that this tutorial assumes a basic working knowledge of:
The sample application uses the Spring Framework
Event Sourced applications - this means that for every change made to a payment we will raise an event. These events represent a definition of the change, including all the data points that have been altered.
Maven as a build tool
At the core of IPF is the DSL - this is Icon’s payment language which allows you to define payment flows. It’s used within Jetbrains MPS to create flows and will generate a set of Java components that form the basis of any implementation. These components use Hexagonal Architecture (aka Ports and Adapters) which separates areas of concerns by interfaces (Ports) and then implementing them using adapters.
Introducing the Basic Application
This tutorial starts by building a simple application that uses some of the core components of IPF. Let’s start by defining a few key things:
A Flow - a flow is a single payment process.
An IPF Domain - an IPF Domain is the container around one or more flows. It provides the API for accessing and working with those flows.
An IPF Application - an IPF Application in this case is a simple spring boot application that provides an environment onto which an IPF domain can execute.
An Event - the IPF processing module works by processing events.
The Journal Data Store - the journal data store is the master IPF Data store that contains all the events. For this tutorial, we use Mongo.
Putting this all together, our initial application is setup like this:
Using the Tutorial Code
The tutorial application is broken into three folders:
These contain:
ipf-tutorial-docs : this contains this tutorial documentation in ascii doc format!
solutions : in here are the solution code to every section of this tutorial.
supporting-apps : some sections of this tutorial require access to simulator apps. The sections in here explain how to use these.
So to start, open in IntelliJ <install-root>/solutions/initial/pom.xml. This will load up the initial version of the application. We can use this throughout our work with the tutorial, building upon it at each step of the tutorial to enrich our application.
However, if at any stage you need to change to the expected code, you can simply jump to the solution by opening: install-root>/solutions/<tutorial-name>/pom.xml instead!
Navigating through the Tutorial
This tutorial is broken into a number of different learning paths.
It’s recommended to start here: Reviewing the initial tutorial application
Once complete you can work your way through the DSL Path:
DSL 1 - Introducing Icon’s DSL
DSL 2 - Opening the Sample Project
DSL 3 - Using a Domain Function
DSL 4 - Using an external domain
DSL 5 - Using a decision
DSL 6 - Using an aggregate function
DSL 7 - Handling Timeouts
DSL 8 - Versioning
DSL 9 - Using Subflows
DSL 10 - Calling other flows (Part One - Within a model)
DSL 11 - Using additional events
DSL 12 - Using custom business data
DSL 13 - Using shared concepts (Part One - Within a Solution)
DSL 14 - Using shared concepts (Part Two - Across Solutions)
DSL 15 - Using shared concepts (Part Three - remote models)
DSL 16 - Dynamic Error Text
These DSL tutorials build up a complex flow to demonstrate how to use different features of the DSL.
Once complete, we delve a little further into implementation and focus on interacting with other systems in the connector path:
CON1 - Adding payment initiation
CON2 - Writing your own connector (Kafka)
CON3 - Writing your own connector (HTTP)
We then look at some of the runtime considerations in:
RES1 - Resiliency and retry settings (HTTP)
RUN1 - Running your application on Kubernetes
And finally we look at how to boostrap your own application through:
BUILD1 - Creating a new project
The tutorial is principally aimed at engineers, however there are many sections - particularly the DSL path that much of the work can be done without any engineering work, and those engineering sections could be skipped if required. Throughout the tutorials it is made clear where engineering knowledge is required.
Learn IPF
Reviewing the Initial Application
------------------------------
Reviewing the initial tutorial application
Getting Started
The tutorial step uses the "initial" solution of the project as it’s starting point. You can open it by selecting <install-root>/solutions/initial/pom.xml in Intellij.
Business Orientation
This tutorial uses a common application which we will build upon in a step by step manner to investigate different features and capabilities of the IPF Platform. The initial application is a very simple flow that simply does:
You’ll move on from here to add calls to other services whilst looking at advanced concepts like reusability and versioning.
Understanding the application structure
You’ll now have a brief dive into the actual application and the way it’s structured.
Let’s start by looking at the application we have downloaded. If we open the project in Intellij we can see that it has 3 major folders inside it:
domain-root - this contains all the artefacts relating to the DSL and flow definitions.
ipf-tutorial-app - this contains all of the application runtime code, this is the code that will be deployed and use the IPF generated domain to process transactions.
docker - this contains all the script and configuration needed to run the application in a docker environment.
When the application receives a request it will perform a number of simple steps:
It will generate a unique ID for the process.
It will initiate a new flow.
It will generate a new FlowInitiated event.
It will move the flow to the "Complete" state (note that this is simply because of how our flow is current configured, this is investigated in more detail in )
The application is also using the processing data module within IPF. You’ll look at this in greater depth in a later tutorial, but for now what we need to know is that the data being generated by IPF is being packaged and sent out over HTTP. In our case, we’ve added a small "ipf-developer-app" container that has a published endpoint for that data to be pushed to. This we’re using as a way to make it easier to see and understand how our IPF Application is working.
Lets now set the application up to actually process a payment.
Application Configuration
The application uses hocon to define properties. You can see the application configuration within the src/main/resources directory of the main ipf-tutorial-app project. In there let’s have a look at a few settings within the application.conf file:
ipf {
  mongodb.url = "mongodb://localhost:27017/ipf"
  processing-data.egress {
    enabled = true
    transport = http
    http {
      client {
        host = "localhost"
        port = 8081
        endpoint-url = "/ipf-processing-data"
      }
    }
  }
  system-events.exporter {
    type = ipf-processing-data-egress
  }
  behaviour.retries {
    initial-timeout=250ms
  }
}
Let’s take each section of this and work through what it is telling us.
Firstly, all the ipf application specific properties are configured to start with "ipf".
The first property we come across is ipf.mongodb.url, in the tutorial case we’re using mongo as our database provider so this is simply the connection url. We’re assuming in the above that there will be a mongo service running on localhost:27017,
Next we come to the "processing-data.egress". The processing data egress module is used to export data from IPF onto a desired location, for example a Kafka topic or a HTTP endpoint. Here we are defining:
ipf.processing-data.egress.transport to be http, as the tutorial application is using HTTP as it’s protocol for sending out processing data.
ipf.processing-data.egress.http contains the HTTP configuration of where we are going to send our data. So in this case we’re sending:
ipf.processing-data.egress.http.client.host - this is the host we are sending the data too, in our case the ipf-developer-app we’ll talk about below.
ipf.processing-data.egress.http.client.port - the port we’re sending to.
ipf.processing-data.egress.http.client.endpoint-url - the URL of the endpoint we’re sending to.
We will use the "ipf-developer-app" to consume the data generated from the processing data services. This is a lighweight simple example application that provides a view across the data generated by IPF. It’s not a production utility, but is used to assist development. Details to use the developer app are listed here: Running the developer app.
The next section is for "system-events.exporter". System events are IPF’s way of emitting information about things that are happening within IPF itself. You’ll come onto these in greater depth later. For now the key is simply that we are setting ipf.system-events.exporter.type to be "ipf-processing-data-egress", telling our application to send the system events to the processing data setup we have deployed.
The next configuration is the "behaviour.retries.initial-timeout". Behaviour retries are a mechanism to resubmit commands into IPF if for any reason a reply has not been received back. The default configuration is set to wait 100ms before a reply, however here we’re increasing this to 250ms simply under the assumption that this tutorial is being run on a lower grade machine and hence to give it a little more time to complete any requests.
Building the application
To build the application we use Maven, we’ll need to run this from the root of our initial module:
mvn clean install
The build will use the defined flows to generate application code. The generated code can be seen in a number of key places:
The generated domain code will be in "domain-root/domain/target" - this will include all the Akka event sourced behaviour implementations together with all the ports that the domain will communicate with.
The generated test code will be in "domain-root/test/target" - this contains the generated test framework code to provide a testing capability against our domain.
The generated sample adapters will be in "domain-root/sample/target - this contains a set of simple sample implementations for all the generated ports. These can be used to quickly bootstrap a complex flow into being executable. At this stage as our flow is so simple, all we have in here is an example configuration file.
Environments
The tutorial supports both running the code using docker or as a standalone spring boot application. Both of these approaches are detailed below:
Docker
Creating the docker container
It is also possible to build a docker image for the tutorial application. To do this we need to use a profile that has been setup to generate the image. If you’re intending to use docker to run the application, you can build this now by running:
mvn clean install -rf :ipf-tutorial-app -Pjenkins
The key things to note here is the -Pjenkins flag, this tells Maven to activate the profile that contains the docker image generation plugin. Go and have a look at the pom file of the ipf-tutorial-app to see how this is done!
Note in the above the use of -rf :ipf-tutorial-app → this tells maven to only rebuild the ipf-tutorial-app project. You could rebuild everything and skip this, but since we just generated all our code a moment ago there’s no need to rebuild those projects here.
The docker image that is generated is the container that is expected by the example docker configuration in the /docker/application.yml file namely:
ipf-tutorial-app:
  image: registry.ipf.iconsolutions.com/ipf-tutorial-app:latest
  container_name: ipf-tutorial-app
  ports:
    - 8080:8080
    - 8559:8558
    - 5006:5005
    - 55002:55001
    - 9002:9001
  volumes:
    - ./config/ipf-tutorial-app:/ipf-tutorial-app/conf
    - ./logs:/ipf/logs
  environment:
    - IPF_JAVA_ARGS=-Dma.glasnost.orika.writeClassFiles=false -Dma.glasnost.orika.writeSourceFiles=false -Dconfig.override_with_env_vars=true
  depends_on:
    - ipf-mongo
    - ipf-developer-app
  healthcheck:
    test: [ "CMD", "curl", "http://localhost:8080/actuator/health" ]
A few things to note around this setup:
We expose the port "5006" - this is the "debug" port, it will allow us to attach a remote debugger.
We map the "logs" directory on our local machine to the "/ipf/logs" directory on the running docker container. This means that we can see all the application logs at docker/logs/ipf-tutorial-app.log
We’re using mongo as the database for this tutorial series
Starting the docker environment
You’ll start by looking at how to run the application using the docker setup provided. All the docker environment setup is in the docker directory of the root project. Let’s have a quick look what is in here:
application.yml - this is the docker file for running the full ipf application. It includes definitions for mongo, ipf-developer-app and the tutorial-app itself.
the config directory - in here we maintain all the docker based configuration.
The important part of the configuration to look at here is that actual configuration for the ipf-tutorial-app itself. Let’s look at this by going to docker/config/ipf-tutorial-app. You’ll note here we have an application.conf. When we open this we see:
ipf.mongodb.url = "mongodb://ipf-mongo:27017/ipf"
ipf.processing-data.egress.http.client.host="ipf-developer-app"
So here we have just two properties - these are both overriding properties we defined in the application code itself to provide the docker environment specific properties - the difference here being the names of the servers running mongo and the developer-app. We don’t need to define the other properties here as they will still be brought through from the main application!
Let’s now start up our environment. From the docker directory, run the following command from the docker directory:
docker-compose -f application.yml up -d
This will start the application, a mongo database and an instance of the developer application. After a short while the environment will be ready. We can see the current state of the actual ipf-tutorial-app application by invoking the health check URL:
curl localhost:8080/actuator/health
When this returns a success code, the application is healthy and ready to process work.
Without Docker
As a Spring Boot application
Previously we looked at running the application as a docker container, but we may wish to bring it up as a traditional spring boot application and use some existing infrastructure, for example a shared mongo environment. To do this all we need to do is change the values of "localhost" in the application.conf file within the ipf-tutorial-app project:
ipf.mongodb.url = "mongodb://localhost:27017/ipf"
ipf.processing-data.egress.http.client.host="localhost"
ipf.processing-data.egress.http.client.port=8081
Note that if we’re not using docker at all, we’ll need to run the ipf-developer-app another way.
For this we have provided a runnable version of the ipf-developer-app, this can be downloaded from the ipf-releases Nexus repository, for example version 1.0.29 is:
nexus.ipf.iconsolutions.com/repository/ipf-releases/com/iconsolutions/ipf/developer/ipf-developer-app/1.0.29/ipf-developer-app-1.0.29-runnable.jar
You must choose and download the correct version of the ipf-developer-app for the IPF Release you are using.
The versions must be compatible, please check and replace the numbers based on IPF’s release documentation and/or the details for your target environment!
To run this, we simply need to fire the following command (replacing the version you are using):
java -cp "ipf-developer-app-1.*.**.jar:config" -D"ma.glasnost.orika.writeClassFiles"=false -D"ma.glasnost.orika.writeSourceFiles"=false -D"config.override_with_env_vars"=true -D"loader.main"="com.iconsolutions.ipf.developer.app.IpfDeveloperApplication" "org.springframework.boot.loader.PropertiesLauncher"
If using windows we need to replace the first : with a ;  ,namely: "ipf-developer-app-1.0.29.jar;config"
This will run without default configuration namely:
assuming mongo is on localhost:27017
will run itself on 8081
It is possible to change these by adding under a config directory (from the location you’re running the file) and override application.conf file. An example is provided in the dev-app folder of the project. This folder also contains the run command and a helper script to execute it.
So once we have a mongo instance we can connect to together with a running implementation of our developer app jar, we can simply start the application just as with any other spring boot application. In Intellij we do this by right clicking on the "Application.java" file and choosing Run > Application (main).
Running a payment
Now that we’ve started our application, it’s time to run some payments through it. Let’s start by taking a quick look at the InitiationController.
This class has been built simply as a quick way to interact with the IPF application system and depending on your solution requirements may well not be needed. It uses a simple rest endpoint to provide healthcheck and submission capabilities. In a "real" application, initiation may come from many different channels and over any number of protocols.
If we look at the main initiation method on the controller:
    @RequestMapping(value = "/submit", method = RequestMethod.POST)
    public Mono<InitiationResponse> submit(@RequestBody(required = false) InitiationRequest request) {
        final String unitOfWorkId = UUID.randomUUID().toString(); // this is the common id used to link all flow elements and uniquely identify a request.
        String clientRequestId = request != null && request.getRequestId() != null ? request.getRequestId() : UUID.randomUUID().toString();  // this is a client id (if required)
        String entityId = UUID.randomUUID().toString(); // this is the entity id to uniquely identify the current flow.
        var dummyPacs008 = PayloadGenerator.generatePacs008();
        if (request != null && request.getValue() != null) {
            dummyPacs008.getCdtTrfTxInf().get(0).getIntrBkSttlmAmt().setValue(request.getValue());
        }
        return Mono.fromCompletionStage(IpftutorialmodelDomain.initiation().handle(new InitiateIpftutorialflowInput.Builder(entityId)
                .withProcessingContext(ProcessingContext.builder()
                        .unitOfWorkId(unitOfWorkId)
                        .clientRequestId(clientRequestId)
                        .build())
                .withCustomerCreditTransfer(dummyPacs008)
                .build()).thenApply(done -> InitiationResponse.builder().requestId(clientRequestId).uowId(unitOfWorkId).aggregateId(done.getAggregateId()).build()));
    }
We can see here that our controller takes an optional "InitiationRequest", let’s look at that too:
    @Data
    public class InitiationRequest {
        private String requestId;
        private BigDecimal value;
        private String version;
    }
We can see here we’ve provided 3 properties on our initiation request - a requestId, a value and a version. This will be used in later tutorial sessions in order to test various things.
This tutorial series assumes you have a little utility called jq installed on your machine, this simply will format the returned json from our commands in a pretty fashion. In all of these therefore you can simply remove the | jq to return pure json!
Let’s create a payment in our new environment. To do so we simply need to hit the example InitiationController’s submit endpoint, which we can do by running the following command:
curl -X POST localhost:8080/submit | jq
This will return the details of the payment that has been created, such as:
{ "requestId": "220b4868-a96c-4d9a-8b17-b1aa714f05e8", "uowId": "8995c642-a887-4676-a690-2bf261fba172", "aggregateId": "Ipftutorialflow|44d0f1df-eb66-4826-8b1f-63eac604b730" }
Here we can see a number of key things:
The "requestId" - this identifies the request being made. Normally this would be supplied by the calling system, but in our case it is auto-generated by the test "Initiation Controller".
The "uowId" - this is the "unit of work id", it is the single ID that can be used throughout the application to track the payment. This becomes particularly useful for example when considering a payment that spans multiple flows.
The "aggregateId" - this is the ID of an the individual payment flow that has been kicked off.
If we wanted to, we could supply the additional parameter’s into our command for example:
curl -X POST localhost:8080/submit -H 'Content-Type: application/json' -d '{"value": "150", "requestId":"abc"}' | jq
This would submit a request with a value of "150" and a requestId of "abc". If you try this now, we’ll see the "requestId" in your response having the value "abc". Later we can look to see that the value of our payment has also been set to 150 USD.
Querying the payment
The tutorial application also provides a simple query controller that allows us to make basic calls to IPF to retrieve data. So for example, you can retrieve the status of the payment by calling:
curl -X GET localhost:8080/query/Ipftutorialflow%7C44d0f1df-eb66-4826-8b1f-63eac604b730/status | jq
This should return the value "COMPLETE" - note we’ll need to swap the aggregateId given in the command to that which was returned when you created your payment!  You’ll also need to replace the | character with the encoded '%7C'. Alternatively you could just paste the path into your favourite browser.
The table below shows the list of available endpoints that can be queried. To use them simply swap "aggregate/status" in the command above to the endpoint path given in the table.
Endpoint Path
Provides…​.
query/{aggregateId}
A full copy of the aggregate for your payment
query/{aggregateId}/status
The current status of your payment
query/{aggregateId}/eventNames
A list of events that have happened to your payment
query/{aggregateId}/flowName
The flow name of your payment
query/{aggregateId}/processingContext
The processing context of your payment
Try a few of these now to get a feel for using the controller and what sort of data it provides!
Note that the query service at this point in the tutorial is a very simple controller based implementation. In later sessions of this tutorial series we’ll work on providing much greater depth and possibilities through data projection and introduce IPF’s standalone ODS Service.
The payment journal
As IPF performs it’s processing, it writes all it’s events to the journal. This is a collection which is stored in mongo, under the IPF database. If you use your favourite mongo viewer (For example, our engineers often use Robo3t)
Then you can see the events that have been persisted down to the journal. You should be able to find a record like:
{     "_id" : ObjectId("62bb257a29d64365337f88be"),     "eventPayloads" : [          {             "deleted" : false,             "manifest" : "",             "payload" : {                 "type" : "om.iconsolutions.ipf.tutorial.ipftutorialflow.events.FlowInitiated",                 "value" : "{"createdAt":1656431994.593398000,"originalCommandId":"HandleInitiateIpftutorialflow|07adee15-80ea-44d0-ab3b-efe3be1ba20e|be7cf0a8-29af-4d96-9018-8efecb8b3bc6","status":{"originatingStatus":"Initial","resultingStatus":"Complete","globalStatus":"ACCEPTED"},"eventId":"Ipftutorialflow|44d0f1df-eb66-4826-8b1f-63eac604b730|1","processingContext":{"associationId":"Ipftutorialflow|44d0f1df-eb66-4826-8b1f-63eac604b730","unitOfWorkId":"8995c642-a887-4676-a690-2bf261fba172","clientRequestId":"220b4868-a96c-4d9a-8b17-b1aa714f05e8","processingEntity":"UNKNOWN"},"initiatingId":"44d0f1df-eb66-4826-8b1f-63eac604b730","causedByEventId":null,"responseCode":null,"originalResponseCode":null,"reasonCode":null,"originalReasonCode":null,"reasonText":null,"originalReasonText":null,"failureResponse":false,"customerCreditTransfer":\{"grpHdr":\{"msgId":"42708ab4-a5ee-4c07-9331-3c34a1438bfa","creDtTm":1656431994.184934000,"btchBookg":null,"nbOfTxs":"1","ctrlSum":null,"ttlIntrBkSttlmAmt":\{"value":10.45,"ccy":"USD"},"intrBkSttlmDt":[2022,6,28],"sttlmInf":\{"sttlmMtd":"CLRG","sttlmAcct":null,"clrSys":null,"instgRmbrsmntAgt":null,"instgRmbrsmntAgtAcct":null,"instdRmbrsmntAgt":null,"instdRmbrsmntAgtAcct":null,"thrdRmbrsmntAgt":null,"thrdRmbrsmntAgtAcct":null},"pmtTpInf":null,"instgAgt":\{"finInstnId":\{"bicfi":"IPSTFRP0","clrSysMmbId":null,"lei":null,"nm":null,"pstlAdr":null,"othr":null},"brnchId":null},"instdAgt":\{"finInstnId":\{"bicfi":"ICSLGBL1","clrSysMmbId":null,"lei":null,"nm":null,"pstlAdr":null,"othr":null},"brnchId":null}},"cdtTrfTxInf":[\{"pmtId":\{"instrId":null,"endToEndId":"866afe26-c173-4921-9587-468f2f974b73","txId":"cecbe387-3906-4351-9fab-dd8b1ad48333","uetr":"1e97e521-d7d6-4913-b941-63e437f884f9","clrSysRef":"2c3b0220-1df4-4a3e-8372-0d7a5a15b3c3"},"pmtTpInf":null,"intrBkSttlmAmt":\{"value":10.45,"ccy":"USD"},"intrBkSttlmDt":null,"sttlmPrty":null,"sttlmTmIndctn":null,"sttlmTmReq":null,"accptncDtTm":1656431994.191809000,"poolgAdjstmntDt":null,"instdAmt":null,"xchgRate":null,"chrgBr":null,"chrgsInf":[],"prvsInstgAgt1":null,"prvsInstgAgt1Acct":null,"prvsInstgAgt2":null,"prvsInstgAgt2Acct":null,"prvsInstgAgt3":null,"prvsInstgAgt3Acct":null,"instgAgt":null,"instdAgt":null,"intrmyAgt1":null,"intrmyAgt1Acct":null,"intrmyAgt2":null,"intrmyAgt2Acct":null,"intrmyAgt3":null,"intrmyAgt3Acct":null,"ultmtDbtr":null,"initgPty":null,"dbtr":\{"nm":"7f0d8165-9c67-4989-865e-15fce48689c1","pstlAdr":null,"id":null,"ctryOfRes":null,"ctctDtls":null},"dbtrAcct":\{"id":\{"iban":"GB26MIDL40051512345674","othr":null},"tp":null,"ccy":"EUR","nm":null,"prxy":null},"dbtrAgt":\{"finInstnId":\{"bicfi":"ICSLGBL1","clrSysMmbId":null,"lei":null,"nm":null,"pstlAdr":null,"othr":null},"brnchId":null},"dbtrAgtAcct":null,"cdtrAgt":\{"finInstnId":\{"bicfi":"ICSLGBL2","clrSysMmbId":null,"lei":null,"nm":null,"pstlAdr":null,"othr":null},"brnchId":null},"cdtrAgtAcct":null,"cdtr":\{"nm":"bc67af37-93f1-452b-918e-c4fb65f2d6f2","pstlAdr":null,"id":null,"ctryOfRes":null,"ctctDtls":null},"cdtrAcct":\{"id":\{"iban":"GB26MIDL40051512345675","othr":null},"tp":null,"ccy":"EUR","nm":null,"prxy":null},"ultmtCdtr":null,"instrForCdtrAgt":[],"instrForNxtAgt":[],"purp":null,"rgltryRptg":[],"tax":null,"rltdRmtInf":[],"rmtInf":null,"splmtryData":[]}],"splmtryData":[]},"paymentJourneyType":null,"paymentType":null}"             },             "persistenceId" : "Ipftutorialflow|44d0f1df-eb66-4826-8b1f-63eac604b730",             "sequenceNumber" : NumberLong(1),             "tags" : [                  "tag-1"             ],             "timestamp" : NumberLong(0),             "writerUuid" : "06cf3f62-65fe-43db-80f2-7a98a483fea4"         }     ],     "hiSeq" : NumberLong(1),     "loSeq" : NumberLong(1),     "persistenceId" : "Ipftutorialflow|44d0f1df-eb66-4826-8b1f-63eac604b730" }
Let’s break the payload down a little so we can see the constituent parts:
"type" : "com.iconsolutions.ipf.tutorial.ipftutorialflow.events.FlowInitiated" -  here can see the name of our event.
"status":\{"originatingStatus":"Initial","resultingStatus":"Complete","globalStatus":"ACCEPTED"} - here we can see that this event was received in the "Initial" state and caused the payment to move to the "Complete" state. It’s transformed the payment to the global "ACCEPTED" state.
"associationId":"Ipftutorialflow|44d0f1df-eb66-4826-8b1f-63eac604b730" - this is the aggregateId.
"unitOfWorkId":"8995c642-a887-4676-a690-2bf261fba172" - this is the overall payment unit of work id.
"clientRequestId":"220b4868-a96c-4d9a-8b17-b1aa714f05e8" - this is the originating request id.
Note how for the data shown above, these ids correspond to the Ids received as the response to the initial submit request.
The Developer GUI
The IPF Tutorial app has been configured to publish its data so that it can be read by the IPF Developer GUI. This is a lightweight GUI used for testing and to demonstrate the ability to create different views of the data being published by IPF. We can access it by going to localhost:8081/explorer.html. Let’s explore a little what it tells us about our simple application.
Search
You’ll start by searching for our transaction by using the unit of work ID that was returned to us from our initiation request:
Flows
We should see one record returned, with the basic details of that payment. Let’s now press the "view" button to see some more details:
Here we see the "flows" that make up our payment. A payment can have many legs through different flows in it’s lifecycle. For the moment we have the simplest setup available and it’s just a single flow hence why we only see one record. In  for example we’ll start adding multiple flows.
In the meantime, we can see many things about our flow so let’s try a few of those out.
Graphs
Firstly we can see the graph - click the graph button and we should see the graph we had from our MPS setup before:
Note how this time we are colouring in the flow - the path taken through is highlighted in orange. This will become more obvious later as we build the complexity of our flow and can see our payment taking decisions about which route to run.
Aggregate Data
Let’s close the graph and then click to view the aggregate data.
Here we can see all the "live" data of the flow. It tells us everything IPF has in memory about that flow, including all the data available to the flow. This can be either data supplied to the flow via events or calculated by IPF itself (we’ll cover this in DSL 6 - Using an aggregate function)
Domain Events
The next interesting view is the domain events one, let’s click the orange "Domain Events" button:
This shows us all the domain events that have been recorded during the payment journey. There are a couple of interesting points to note here:
Sequence Number - this is an ordering of the events as they arrive. In our case it’s trivial, but in more complicated multi flow cases this will be the ordering across all flows not just one.
Process Flow - this becomes useful when multi-flow are in place.
Content - this provided a full view of the entire domain event.
System Events
Now let’s look at the system events view:
Here we can see that our flow has generated 7 system events. System events are a way of tracking what has happened within an IPF application. You’ll discuss these in greater depth at a later tutorial, but for now let’s just quickly think about what these are telling us. They are effectively providing a story of the "events" that have occurred to our "system" that are associated to this particular payment. It tell’s us a story:
_The flow started > A command was received (the initiation instruction) > A domain Event was persisted (the Flow Initiated event) > There was a state transition (from Initial to Complete) > The Flow Finished. _
Have a look at the different events generated here, and view the details they are providing.
Payment Objects
Next lets look at the payment objects tab:
This is showing us the different pre-configured payments objects that are available to the flow. We’ll discuss this much more in later tutorials but for now we can see that we have the full pacs.008 available together with the individual credit transaction that is for our payment (noting for now we are only dealing with 1-1 relationships between pacs008 and credit transfers but later this may become 1-n).
The other tabs - messaging and custom objects are blank for now. These aren’t being used by our simple flow as yet. The messaging tab will show us any messages being sent to or from IPF, whilst the custom objects allows a user to define non-payment objects to be stored in this view.
Conclusion
In this section we have successfully:
Built the application.
Started a docker environment.
Executed a payment.
Queried the result of our payment
Reviewed the data in the mongo journal
Looked at the Developer GUI for viewing a transaction.
Tutorials
DSL 1 - Introducing Icon’s DSL
------------------------------
DSL 1 - Introducing Icon’s DSL
Concepts
This section of the tutorial introduces the concepts within Icon’s payments DSL. It is a theoretical walk through and does not require access to any IPF components.
Business Data
We start by considering data. Data is what drives processing and decision making throughout IPF.
The first concept we consider therefore is the "Business Data Element". It has four properties:
A name
A description
A "data type" - the data type can be any Java type, whether it be standard classes like String, Integer etc or your own bespoke types.
A "data category" - an optional field, the possible values are an enumerated set that refers to the type of data that is being represented by this BusinessDataElement. This Data Category label is used by various IPF components such as IPF Data Egress and Operational Data Store, which can automatically record data captured from Process Flows automatically, depending on the Data Category. There are four core data categories:
PAYMENT - This is payment data that is modelled as ISO20022 message components within IPF.
PAYMENT_PROCESSING - This is data that relates to the processing of payments, such as meta-data and payment type information
CUSTOM - This represents custom data which may be attached to the payment
ADDITIONAL_IDENTIFIER - This applies to data elements that represent additional identifiers to be associated with the payment
Any MPS project can have as many different business data elements as you need. These elements are defined within a "Business Data Library" which is simply a collection of related business data and as many different business data libraries can be defined as needed.
IPF provides a number of pre-configured business data libraries. By default, any process is given the "error"  library which provides default elements for handling flow failures, namely:
Failure Event Name - this is the name of the event that registered the first failure in a flow.
Failure Response Code - this is the IPF response code for the failure.
Failure Reason Code - this is the IPF reason code for the failure.
Failure Reason Text - this is the IPF text description of a failure.
Failure Original Response Code - This allows specification of any original response code involved (which may have then been mapped to an IPF one)
Failure Original Reason Code - This allows specification of any original reason code involved.
Failure Original Reason Text - This allows specification of any original reason text involved.
The concepts of reason and response codes are discussed later in this document.
Within the lifetime of a payment each business data element is unique and it can be updated as required.
Flow
The processing of a payment is performed by a "Flow". A flow represents a single business process from end to end and is designed to specify the lifetime of a single payment. A single flow might have many paths through it, each representing a different way of processing an individual payment based on the data provided for that flow. A flow contains a number of things:
A name
A description
A version
A global state set
A list of "States"
A list of "Events"
An "Initiation Behaviour"
A list of "Input Behaviours"
A list of "Event Behaviours"
A list of "Aggregate Functions"
A definition of each of these aspects are discussed in the following sections.
The combination of "Flow Name" and "Flow Version" uniquely identify a flow. The version is just an optional numeric identifier, so for example a flow may be called "Test" and have version 3. Then the flow can be unique identified as "TestV3". If there was no version defined it can be identified simply by the name "Test". This identifier is known as the "FlowId"
Global States
First we consider the "Global State Set". The global state set is a set of states that represent the overall state of a payment. It is particularly used where a payment may span multiple flows (for example if the payment processing is split into "initiation" and "execution" parts) but can also apply an overall grouping type state to the individual flow parts to simplify the apparent state transitions from a payment level. Each flow level state can be mapped to a global state such that multiple flow level states can all be considered to leave the payment in the same overall global state.
A default global state set is provided which provides the following standard states: Pending, Accepted, Rejected, Manual Action Required and Cancelled.
States
The next concept to consider within our flow is a "State". This is very simply a resting point on the flow that the payment can pass through in it’s journey, so for example we may have a very simple flow that goes from "State A" to "State B".
A state itself has a number of properties:
A name
A description
A global state
A terminal flag - the terminal flag is used to indicate that this ends the flow to which the state belongs.
Each flow can contain many different states.
Events
When a flow moves from one state to another, this is known as a "State Transition". Within IPF, for a state transition to occur then the system needs to receive an "Event" on the processing journey of the payment. In this case, it is actually a specific type of event known as a "Domain Event". A domain event is a persisted factual occurrence - the arrival of an event means that something explicit has occurred which may cause some form of change to the processing of our payment.
An event has a number of properties:
A name
A description
A list of business data elements.
When an event is formed, then the system will check it’s own behaviour to determine what actions should be performed. Whilst this behaviour is explored later in this document, it is worth noting here that there are three occasions when an event can cause a change to the processing, these are known as the "Event Criteria" conditions and are defined as:
On - this movement will happen upon the arrival of a single event (e.g. we may transition when receiving "Event 1")
On any of - this movement will happen upon the arrival of one of multiple events  (e.g. we may transition when receiving either of "Event 1" or "Event 2")
On all of - this movement will only occur upon the arrival of multiple events (e.g. we may transition only after receiving both "Event 1" and "Event 2")
Here we have described the "Domain Event" which is the type of event that is explicitly declared within any MPS solution. However, IPF as a whole uses a number of different types of event:
"System Event" - these occur when something happens to the system and can be tailored to be specific to individual needs.
"Action Timeout Events" - these events occur during processing when configured timeout settings are broken.
"Decision Events" - these event are used as a response to receiving results from decisions.
All these event types are discussed later in this document.
External Domains
After an event is processed, the application can then perform one or more activities to determine what happens next on a payment. So for example on receipt of "Event A" we may wish to do some validation and call some other application to ask it to validate our data.
To support this post-event processing, the most important concept is the "External Domain". This represents some business domain - not our current flow’s - that we need to interact with.
For example, let’s assume that we need to talk to a sanctions system during part of the flow. To support this, we would model that sanctions system as an external domain.
Each external domain consists of the three types of interaction we can make with it:
"Instructions" - instructions are the simplest thing we receive from an external domain. It can be triggered by the external domain at any time and we will start processing. This can be thought of as the external domain pushing information to us.
"Notifications" - notification are the opposite of instructions. These are used when we want to push our data out to an external domain.
"Requests" - a request is used when we need a "response" back from the external domain in reply.
Instructions
Firstly let’s consider the instruction. These can be initiated by an external system and contain the following properties:
A name
A description
A list of "Business Data Elements"
When the IPF application receives an instruction it will raise a corresponding event (the decision of which event to raise is described later). The event’s business data is then populated with all the matching business data elements.
Notifications
Next up is the notification, like an instruction it has the following properties:
A name
A description
A list of "Business Data Elements"
When the IPF application sends out a notification it will populate on it all the business data elements it has a matching record for.
Requests
Finally, we consider the requests. The request can be thought of to have to parts, the request itself and the corresponding response.
The request part contains:
A name
A description
A list of business data
A list of responses
The response part is slightly different and has some new features to consider:
A name
A description
A list of business data
A "response code set" - this is a group of response codes. A "Response Code" is an expected outcome code for a response that could be used for onward processing. In ISO terms this is analogous with a Status.
A "reason code set" - this is a group of reason codes. A "Reason Code" is a reason why the response is set the way it. So for example your response code could be "Rejected" with a reason "Incorrect Currency". In ISO terms a reason code with a Status Reason.
A completing flag - this defines whether the calling request should be considered completed when this response arrives. So for example consider a request where the external system sends a technical acknowledgement following by a final business response. In this case we would define two responses - one to represent the technical ack (non completing) and one the business response (completing).
In ISO terms, a response code is analogous with a "Status", whilst a reason code is analogous with a "Status Reason"
The system provides a default "AcceptOrReject" response code set which is used for standard pass / fail type responses. It also provides a set of all the ISO reason codes.
Now let’s put these elements together and form the basis of any flow:
So here we can see that when IPF receives something from an external domain (either an instruction or a response), it leads to an event being raised which may cause a state transition followed by the invocation of a notification or request to an external domain.
Domain Functions
It’s possible that we don’t want to have to call an external domain in order to continue processing our flow. This might happen because either we know what to do next or we can calculate what to do next. For this there are two other concepts that we need to consider:
In this case, one option is to use the "Domain Function" capability that the flow itself offers. It works in a very similar way to a request / response pair in an external domain call except that in the case of a domain function the IPF application itself is a domain so the actual call stays internal (consider for example creating an external domain that represents our current flow - this would work the same way as a domain function but would be a mis-representation of the actual control logic). So when we call a domain function, we will expect to get a response and then that response will be transformed into an event which can then cause onward processing.
Like a request, the domain function has a number of properties:
A name
A description
A list of business data
A list of responses
Additional Events
The second option is an "Additional Event"- these can also be used to move the flow on.
When an additional event is raised, the system will process it as though it has been received into the application via an instruction or response.
Let’s add these to our diagram:
Decisions
What however if we want to perform some logic conditionally. So for example, we may only want to run a fraud check if the value of the payment is over £50. In this case we can use a "Decision".
A decision allows us to perform some logic and then take different processing routes subsequently based on the outcome of that decision. A decision has a number of properties:
A name
A description
A list of business data - this is the data that is sent when calling the decision so that it can process based upon it.
A list of "Decision Outcomes" - these are the possible results of running the decision, each decision can have as many different outcomes as needed and these outcomes are unique to the decision. They are defined simply by providing a name.
The decisions themselves are stored within a "Decision Library". The libraries are flow-independent and as such the same decision can be used in multiple flows.
We can use a decision in two places:
To determine which event needs to be raised in response to an input (response or instruction)
To determine which actions need to be performed after a state transition.
Lets add these to our diagram:
A special type of event "A Decision Outcome Event" will also be raised so that the fact the decision has been invoked and a result returned will be audited and can be used on onward processing.
Aggregate Functions
Another useful utility to consider is the "Aggregate Function". An aggregate function is a piece of logic that can be executed upon receipt of an event to perform some kind of logic upon the data received. This data is considered "in flight" and thus is not persisted by the application.
So a good example of this is say a counter that tracks the number of times something has occurred during a flow - each time the function is called we may update that counter. The outcome of the aggregate function then becomes available down the flow.
Another good use case may to perform a data mapping exercise to transform the data into something that can be used downstream.
Let’s add the aggregate function to our diagram:
Behaviours
The next concepts to consider are both types of grouping. In order to separate the logic we need to define when processing an input to the system (from a response or instruction) and generating the event to the logic required when processing the actual behaviour of the system based off that event we have two grouping concepts:
"Input Behaviour" - this is a the behaviour that specifies for each input what event will be generated.
"Event Behaviour"  - this is the behaviour that specifies what actions should be taken on receipt of an event.
Input Behaviour
An input behaviour has a number of properties:
An input - this is the input (instruction or response) upon which the behaviour is triggered.
A response code - this is the response code (linked to the response if the response is an input, otherwise this field is not applicable) for which the behaviour applies
An event - this can be either an event directly or via the execution and resulting outcome of a decision.
Note that when using response codes, if one is not defined on an input behaviour this will be considered the "default" behaviour for all response codes.
Event Behaviour
The event behaviour is a little more complicated. It has a number of properties:
A "Current State" - this is the state upon which the flow must be in for the behaviour to apply.
A "Criteria" - this is when the behaviour applies ( on / on all of / on any of)
A list of events - one or more events, these can be any type of event (e.g. domain, timeout etc)
A "Move to State" - the destination state of the behaviour
A list of actions - these are the actions that should be performed after the state transition, i.e. requests, notifications etc.
Lets update our diagram to show these:
Note that the aggregate function, as a self contained unit of calculation is not considered as either the event or input behaviour but as a functional capability of it’s own
Initiation Behaviour
There is one more key type of behaviour to consider, that is the "Initiation Behaviour". The initiation behaviour is a specialised version the previously defined input behaviour but is only used to start a flow. It is not linked to an external domain so that we can initiate the flow potentially from many different sources.
An initiation behaviour has a number of properties:
A list of business data
An initial state to move to
A list of actions to perform
Note that when the initiation behaviour is invoked, a flow will start and the "FlowInitiated" event will be raised.
We have now reviewed all the components that make up a single flow.
Subflows
The next thing to consider is reusable segments of flow.
For example,  consider a sanctions check that may be required to be called from various different places within the flow. We could specify each section of the flow separately and write out the logic each time but ideally we would like to be able to simply reuse common logic each time. This is where the "Subflow" concept is introduced.
A subflow is a reusable flow component. It is essentially the same as a flow in that it has states, input behaviours and event behaviours. However, a subflow has no life of it’s own and is only used as a mechanism of reuse and therefore MUST be contained within an outer flow. When calling a subflow it is very similar in behaviour to receiving an event:
The key thing to understand here that is instead of moving to a state and then calling an action like the normal processing above, here we move to a pseudo-state that acts as an entry point into the subflow. "Control" of the flow is then handed over into the subflow, at this point it will process through the input and event behaviours until it reaches a terminal state in the subflow. When it reaches a terminal state, control will be passed back to the calling flow with a result of the name of the terminal state. This can then be used for onward processing.
Note that in order to achieve reuse of the subflow in multiple places, then when a subflow is placed within a main flow it’s state’s will be displayed as "<subflowid> <subflow state name>" where <subflowid> is the psuedo-state name of the calling flow and <subflow state name> is the name of the state within the subflow.
Flow Calls
Finally, it’s also possible to directly call one flow from another. In this case control is handed over to the secondary flow and we wait for a result execution back. The child flow can report that it has reached any state back to the parent flow. Most commonly, this will be when a terminal state is reached and the child flow has finished processing, but it also allows for feedback from the child flow for intermediary states before it finishes. This provides an opportunity to pass control back and forth between the child and parent as required.
Conclusions
In this section, we have discussed the core concepts that make up the Icon payments DSL and how they fit together. These concepts will be key going forward and the next sections of this tutorial series starts to take us through how to use different ones of these to create our payments application!
Reviewing the Initial Application
DSL 2 - Opening the Sample Project
------------------------------
DSL 2 - Opening the Sample Project
Getting Started
The tutorial step uses the "initial" solution.
In Reviewing the initial tutorial application we looked at the basic project setup and started a flow and ran a payment through it whilst in DSL 1 - Introducing Icon’s DSL we learnt about the basic concepts that make up the payments DSL.
Flow’s are maintained through the Jetbrains product MPS which then uses Icon’s DSL. If you need to download it, it is available as an open source product here: Jetbrains MPS 2021.3.
This section of the tutorial is a beginner’s guide to opening the sample flow in MPS and learning basic navigation around it’s constituent parts.
Opening the Flow
To start with, let’s open our flow. For this we start the Jetbrains MPS product and you should be prompted to open a project:
As we are using an existing project, we’ll click "Open" here to view our ipf-tutorial project. To open our project we need to navigate to the domain-root/mps directory in our project [<tutorial-install-root>/solutions/initial/domain-root/mps]. It will be highlighted with a small black square in the corner of the directory name, indicating that MPS can identify the directory as the root of an MPS project.
Project Structure
When the project opens up you should be able to see in the project view on the left:
Troubleshooting
If you haven’t previously built your application (for example if you’ve come straight from the solution) you may see this when you open the project:
This means that the language hasn’t been built yet, to fix this simply run from the root of the project.
mvn clean install
If you need to, building and running instructions are covered in more depth during: Reviewing the initial tutorial application
Let’s take a quick moment to discuss the way this is structured.
You’ll start at the lowest level - the Ipftutorialflow. Note the "F" symbol here which denotes that the "Ipftutorialflow" is a flow, we’ll see as we expand and add concepts into our solution that other components have different symbols to declare easily what they are!
Our flow is contained in ipftutorialmodel - a model is a grouping container that allows us to associate flows together. From an application perspective, we work at the model level so all the interaction is done at that level. You’ll introduce the concept of the domain class - which represents the single interface into a model - later.
Our model is contained in Ipftutorialsolution -  a solution is a grouping of models, the key thing to be aware of solutions is that they are built together and so it is easy to share components of different models within different solutions.
Finally our solution is contained in "IPF Tutorial Project" - a project is simply a collection of solutions.
Generally speaking, most applications we’ll build will effectively works as a project - single solution - single model - one (or more) flows setup. You’ll introduce in Reviewing the initial tutorial application how we can share models across different projects.
At the top of the project view, we can see that we’re currently in the "logical view". This is the best view for navigating around and working with the DSL. There are also other views available:
These allow us to view our components in different ways, for example looking at the raw files on the File System. This is most useful when trying to review how the generated code looks and is structured.
Viewing the Flow
Now, let’s have a look at our flow. If we double click on it in the project view then we should bring up our flow in the main window:
As you scroll down the flow, hopefully we’ll start seeing sections to represent the concepts we reviewed in the previous section of the tutorial series.
The Flow Viewer
Lets start by clicking anywhere on the flow tab, and using the main toolbar to choose Tools > Open Flo Viewer. This should show a new tab on the screen which displays a diagram of our flow:
So here we can see we have a very simple flow that receives an initiation request, creates the "Flow Initiation" event and then immediately completes. It’s effectively the simplest flow we can have.
The flo-viewer by default opens inline a separate tab. If you want, you can click on the little cog in the top right hand corner:
And then choose to for example undock it which allows you to move the flo viewer. This is great when working with multiple screens!
One of the key features of the flo viewer is that is is updated in real time as your editing your flow! That means you can see live how the changes you are making to your flow definition impacts the flow in a graphical manner.
Intentions
For those familiar with JetBrains products, you may have already come across intentions. These are short cuts functions that are there to provide quick and easy access to common utilities. The Icon DSL has a number of different intentions throughout to try and help make things that little bit easier. You can access the intentions from anywhere by pressing ALT+ENTER. If you do this now we’ll see:
So here we can see we have two intentions, one to create a new version of the flow (we’ll do this in DSL 8 - Versioning) and one to validate the flow (we’ll use this a lot throughout the tutorial series!)
The other thing to note about intentions is that they are specific to what you are currently doing, so as we’ll see in later sections the intention list can provide a whole host of useful little tricks that are specific to what you’re trying to do at that time!
Inspecting the Flow
Now that we have a flow it’s time to build it. First lets check  our settings, on the top menu go to build and ensure that both "Save Transient Models" and "Check models before generation" are selected.
The "Check models before generation" option makes MPS validate your model before it tries to build it. Any errors will then be displayed back to you - you can then click on them to be taken to the problem so you can resolve it.
The "Save Transient Models" option makes MPS show you all the steps it’s taken to generate the code. Normally we don’t need this, but we’re setting it here as a way to help you understand what has been done during the build.
Let’s build our flow by right-clicking on the Ipftutorialmodel model and click rebuild model ipftutorialmodel. This will take a few seconds, then once finished in the navigator expand out the final transient:
Inspecting the generated artefacts
Here you can see some of the artefacts that have been generated for the flow, these include:
All of the application code - this includes
The generated AKKA Event Sourced Behaviour
All the generated ports that the application needs to integrate to
The generated domain class that allows external interaction to the flow.
The generated documentation
The generated BDD Test Classes
The generated sample implementations of the ports created.
Have a look at some of these artefacts, they will be discussed in greater depth in future sections.
Conclusions
In this section of the tutorial we looked at:
Opening flows within MPS.
Basic MPS Project structures.
Flow Viewer
Intentions
Basic Generation
DSL 1 - Introducing Icon’s DSL
DSL 3 - Using a Domain Function
------------------------------
DSL 3 - Using a Domain Function
Getting Started
The tutorial step uses the "initial" solution of the project as it’s starting point.
If at anytime you want to see the solution to this step, this can be found in "add_domain_function" under solutions!
What is a Domain Function?
A "Domain Function" is any process that you need to undertake within the boundaries of your domain.
 These could be a number of things like simple calculations, mapping or invoking business rules.
For the scope of this tutorial, we will assume that the first thing we want to do on our flow is perform a duplicate check.
 It will be a very simple check that we send our pacs.008 too and in return we expect either a success result (not a duplicate) or a failure (is a duplicate).
DSL Set Up
Adding the Domain Function
First of all lets create a new domain function library to hold our function within. From the navigator on the left right click on the model:
And then, right click, select New > v2Flo > Domain Function Library.
 When prompted enter a name and description for the library like this:
Now lets click the "Add Domain Function" button, this will populate the table with a new row.
To start with lets add the name and description of our domain function - these are simple free text entry boxes:
Name - Duplicate Check
Description - A simple duplicate check
Next we have to choose the business data we will send to our domain function.
 In this case we’re going to send the customer credit transfer (pacs.008) itself.
 To do this we will use the MPS selection capability by pressing "CTRL + SPACE", this will present a list of possible data elements we have available.
 Select the "Customer Credit Transfer".
After using "CTRL+SPACE" to bring up the options, you can start typing and this will narrow down the options to only those that match what you have entered.
Now we need to define the response to our domain function. to do this we simply press "RETURN" in the response box to generate a new response, and then for the properties of our response:
For the name we’ll use "Duplicate Check Response"
For the description we’ll use "A response from the duplicate check"
For our duplicate check, we are not expecting it to return any business data so we can leave it blank, but here you could add business data simply by clicking "CTRL + SPACE" in the box again and selecting from the business data options.
For response codes, as we simply need a positive and negative response we can use the inbuilt "AcceptOrReject" codes. So again here we use "CTRL + SPACE" to bring up the options and choose the "AcceptOrReject" option.
For our duplicate check, we don’t need to worry about reason codes so we’ll ignore these for now.
For completing flag, we will leave the default selected.
You’ll come back to completion flags in a later tutorial.
If you’ve entered everything correctly your domain function definition should look like:
That’s our domain function defined, so how do we use it within our flow?
Calling the Domain Function
Currently in our flow, on initiation we simply move directly to the complete state:
Now we’ll change this so that we call our domain function, and if successful we’ll move to the complete state but on failure we’ll move to a new rejected state.
So first of all we’ll need to create two new states:
Duplicate Checking - this will be the state the flow is in whilst the duplicate check is being performed.
Rejected - this will be our failure state for when the duplicate check has failed.
Let’s add these to the flow by clicking the "Add State" button within the state section of the flow and then setting them up as follows:
Let’s briefly talk about the other values we’ve set up here:
In our case:
The duplicate checking state is in the middle of the flow while the process is still in flight and more work is expected - hence we select the PENDING global state and mark the state as not terminal.
The rejected state happens when the flow has finished unsuccessfully, so we use the REJECTED global state and as no more work is expected we mark it as a terminal state.
The other thing you might notice in the screenshot above (and your implementation) is that the two new states are underlined in red.
 This is a common theme within MPS that errors are marked as red underlined, if we hover over the underline we can see that both errors are because the states are not yet used in the flow.
 You’ll fix this going forward.
Sometimes it can be hard to hover over the error, in these cases there are two other options:
On the right hand side of the flow will be little red lines indicating where in the flow the errors are, hover over these to see the error.
If you press "ALT+ENTER" anywhere in the flow we’ll get the intentions popup box, one of the options here will be "Validate Flow". If you select this the flow validation box will come up clearly displaying the errors like:
Here you can simply double-click on an error to be taken straight to the point of failure.
Now lets return to our "Initiation Behaviour" and change it to invoke our domain function.
 To do this we change the "Move To State" from "Complete" to our new "Duplicate Checking" which should now be available in the options.
 We then select the "Duplicate Check" function within the "Perform Actions" block.
Once done it should look like:
So now when our flow receives the customer credit transfer, we will move to the duplicate checking state and call our duplicate check function.
 The next step is to process the response we receive from it.
Using the Domain Function Response
The first thing we need to define here is the events that will occur when we receive our duplicate check response.
 In our case we want two events:
To signify that the duplicate check has passed and that this is a unique transaction.
To signify that the duplicate check has failed and that this is a duplicate of an existing transaction.
Let’s create these two events by clicking the "Add Event" button, in the "Event Definitions" section of our flow.
Note that for these events, we’re not receiving any data so the business data column is blank.
 We can see again there are errors on our two events, have a look to see why.
Now that we have our events setup, we need to handle the receipt of the actual duplicate check response.
 The response is a type of input, so to process it we need to go to "Input Behaviour".
 Let’s click the "Add Input Behaviour" button on our flow and we should see this:
So here for the input we need to enter two input behaviours, one that is based on a successful duplicate check response that forwards to the duplicate check passed event, and the other on the failure case going to the duplicate check failed event.
 If we set these two up we should see:
Select our "Domain Function Response" (again using "CTRL + SPACE").
 For our response code, we’ll start on the "Accepted" case so select that.
 Then we need to consider the event we wish to raise "Event Selection" (again using "CTRL + SPACE").
Again, check the errors ("ALT+ENTER" → "Validate Flow") and we’ll see that it’s now prompting us that we need need to use these inputs in our flow.
The final step is to add the "Event Behaviours".
 This tells the flow how to act when it receives a particular event whilst in a given state (or states).
 So lets click "Add Event Behaviour" and we should see:
So in our case:
"With Current State" - this processing the of the success state should only occur when we’re in the "Duplicate Checking State" so we select that.
"When" - we’ll leave this as "On" - this means that every time this event is received we will fire the behaviour.
"For Event" - that’s our event we’re interested in, so for the success case - "Duplicate Check Passed"
"Move to State" - for success, we’re just going to move to "Complete" so select that.
"Perform Action" - for now this is the end of our flow, so we’ll leave this blank.
When complete, our new line should look like:
Now we need to add the equivalent block for the failure case, see if you can enter this yourself and when ready compare to the solution below.
Once complete, we should be able to validate our flow again ("ALT+ENTER" → "Validate Flow") and see that all the errors have now been corrected.
That’s it, we’ve now added our new step to our flow.
 To see it in diagram form we can open the Flow Viewer (Tools > Open Flow Viewer`): and we’ll see:
So here we can see that on initiation, our flow will move to the "Duplicate Checking" state, will call the duplicate check function and then depending on the result move to either the "Complete" or "Rejected" state.
Now that we’ve defined our domain function and told our flow how to use it within it’s processing, it’s time to look at how we provide an implementation for the domain function.
Java Implementation
Defining the Adapter
Let’s switch to IntelliJ to work with the Java side.
First we need to regenerate the application code to pick up the changes we’ve made in our DSL editing.
 You’ll do this by running the following from the root of our project (ifp-tutorial):
mvn clean install
This should take a minute or so as all the code and dependencies are generated.
 Once it’s complete, navigate to the target directory of the domain-root/domain project and we should see a few key classes:
   package com.iconsolutions.ipf.tutorial.adapters;
   import om.iconsolutions.ipf.tutorial.actions.DuplicateCheckAction;
   import om.iconsolutions.ipf.tutorial.inputs.DuplicateCheckResponseInput;
   import java.util.concurrent.CompletionStage;
   public interface TutorialDomainFunctionLibraryPort {
      CompletionStage<DuplicateCheckResponseInput> execute(DuplicateCheckAction var1);
   }
This method is the definition of our interface to the domain function.
 So when the application receives the initiation request, it will make a call to this interface to invoke the domain function and based on the response will raise the appropriate events.
Have a look at the "DuplicateCheckAction" and "DuplicateCheckResponseInput" classes.
 You’ll see that in our cases, the "DuplicateCheckAction" is a standard POJO containing some flow based meta data together with the pacs.008 input we defined.
 Similarly, the key element on the "DuplicateCheckResponseInput" is the "AcceptOrReject" response code enum which allows us to define the outcome of our duplicate check.
To use this we need to supply an implementation of the interface.
This implementation could be as complex as required but for simplicity of this tutorial we’re simply going to return a success response.
The implementation needs to be added to the definition of the domain, so let’s start by opening the "IPFTutorialConfig" java class (in the ipf-tutorial-app).
 The interesting part of this is the configuration of the domain which is currently:
@Bean
public IpftutorialmodelDomain init(ActorSystem actorSystem) {
   // All adapters should be added to the domain model
   return new IpftutorialmodelDomain.Builder(actorSystem).build();
}
With hexagonal architecture, our implementation of the "Port" is called an "Adapter".
 So let’s add an adapter to this configuration inline by providing a function that simply returns a new CompletableFuture holding a successful duplicate check response input.
 This is just a simple solution for this tutorial step, we’ll look at providing more enriched functions later in the tutorial series.
Our new code should look like:
@Bean
public IpftutorialmodelDomain init(ActorSystem actorSystem) {
// All adapters should be added to the domain model
return new IpftutorialmodelDomain.Builder(actorSystem)
.withTutorialDomainFunctionLibraryAdapter(input -> CompletableFuture.completedStage(new DuplicateCheckResponseInput.Builder(input.getId(), AcceptOrRejectCodes.Accepted).build()))
.build();
}
That’s all we need to do to add the domain function into our flow execution.
Checking our Solution
As normal let’s now check that the solution works. Start up the application as previously (instructions are available in Reviewing the initial application if you need a refresher!)
Then we can send in a payment:
curl -X POST localhost:8080/submit
And finally if we bring up the payment in the Developer GUI, using the uowId returned on the above curl POST (e.g. "uowId":"36d27cd1-c4c5-4cc2-9a2d-d4833beb718a") and bring up the domain events tab (search by unit of work id, click view, click domain events)
Here we can see that whereas before we only got the "Flow Initiated" Event, we now get both it AND the "Duplicate Check Passed" event! Hence we know everything is working and our duplicate check is being successfully invoked.
Conclusions
In this section we’ve learnt to use domain functions, both defining them within the DSL and implementing them in the Java solution.
DSL 2 - Opening the Sample Project
DSL 4 - Using an external domain
------------------------------
DSL 4 - Using an External Domain
Getting Started
The tutorial step uses the "add_domain_function" solution of the project as it’s starting point.
If at anytime you want to see the solution to this step, this can be found on the "add_external_domain" solution!
In this section we’re going to investigate how we can request information from external systems and receive their responses.
What is an External Domain?
From the DSL perspective, an external domain is any other domain with which we need to interact. This is what we refer to as an "External Domain". Usually this is another bank system like Fraud or Accounting but could be for example another IPF system.
We can interact with an external domain in three key ways:
Via an "Instruction" - these are when the external domain pushes information to us.
Via a "Request" - this is when we ask for information from an external domain.
Via a "Notification" - this is when we push information to an external domain.
As an example for our flow, we’re going to add a step to our flow that allows us to make a call out to a Accounting System that will validate our account.
From a logic point of view, here we want to send out the account information and retrieve back whether the account is valid. To make it slightly more complex we’ll say that the account system can return three responses:
Success
Failure
Not Found
And in the case of failure we’ll say that the failure can be because the account has been blocked or some unknown exception has occurred.
Therefore in our example we are "asking" for data, hence we will use a request in this case.
DSL Set Up
Setting up the Response and Reason Codes
In DSL 3 - Using a Domain Function, we used the default "AcceptOrReject" response codes to determine the result of the domain function call. Here we have defined a slightly more challenging requirement which requires the use of both the request and response code concepts.
The Response Codes
Firstly, lets consider the response codes. We have three outcomes "Success", "Failure" or "Not Found". So lets add these as a new response code set that we can use.
If we right click on our model and choose New  v2Flo  Response Code Library we should be presented with this screen.
Let’s go ahead and add a responde code set by clicking on the "Add Response Code Set" button. You’ll then set the name of the new set to "Account Validation Response Codes".
Now click on the "Add Response Code" button - this will show us a new table containing the response codes. Let’s set them up as follows:
Note here that for the "Failed" and "Not Found" options we’ve ticket the is failure box. This tells the DSL that these response codes are failure based and that it should store the failure reasons against the payment for analysis.
The Reason Codes
Having set up our response codes, let’s now do the same for the reason codes - it’s essentially the same process but using the reason code library instead of response codes. If you set it up correctly it should look like:
Alternate Definitions
In this tutorial we have defined the account validation outcomes as a combination of response and reason codes. There are many different ways you may choose to set up your flow depending on your requirements and what work’s best for you. For example, just as valid would be to set up a set of four response codes where 'Blocked' is also represented in that way.
Now we’ve set up our response and reason codes, let set up the external domain and use them within it.
Adding an External Domain
The first step is to add the Accounting System as an external domain, similar to when we created the domain function library. To do this we select the process flow (IpftutorialFlow) and right click then select New  v2Flo  External Domain. This should bring up the following view:
Lets add a name and a description to start:
name - Accounting System
description - A sample account system
For our requirement we’re going to ask the account system to validate our data, so we should model this as a "Request". So let’s click the add request button and then supply:
a name of "Account Validation Request"
a description of "Validates the account details"
the business data containing the "Customer Credit Transfer"
This is now a function we could call from our flow, that will take the customer credit transfer we’ve received from the payment initiation and send it onto the accounting systems validation capabilities. Note here if the accounting system had a specific format that it required, we could define it here and use the IPF mapping capabilities to transform to it. More on that later…​
Note that as per previous, we can see that our request has been highlighted as invalid as we haven’t yet completed it.
Next we need to add our response, so like domain functions, press return in the "Responses" section and then we’ll define:
a name of "Account Validation Response"
a description of "The result message from the accounting system"
no business data
a response code set of "Account Validation Response Codes" ("CTRL + Space" to list & select )
a reason code set of "Account Validation Reason Codes" ("CTRL + Space" to list & select )
a completing flag set to true
When complete, the external domain should look like this:
Updating the Flow
Having created our external domain we need to plug it into our flow, so lets go back to the flow and start by adding a new status for when our application is validating the account:
States
Top Tip
To shuffle the order of table entries hold down "Shift + Ctrl" and use the up/down arrows.
Events
Now we’re going to add three new events on our flow:
one for when the check passes
one for failure
one for not known
We as normal do this by clicking the "Add Event" button on the flow and then entering the details as below:
Input Behaviours
As with domain functions, the handling of the responses from the accounting system are handled as "Input Behaviours". So let’s click the "Add Input Behaviour" button and add input behaviours for each of our options and then entering the details as below. Again we’ll use the CTRL + SPACE to select the input of the request we created above, then select both the first response code, i.e. Accepted, then select the relevant event "Fraud OK". Repeat for response code "Rejected" for event
Finally, we need to plug in both the call to the validation system and how we handle the receipt of our three events into the "Event Behaviour" section of the flow.
Event Behaviours
Let’s start by adding in the call to the new validation request. Previously, on receipt of the duplicate checking confirmation (event - "Duplicate Check Passed"), we were completing the flow. Now let’s change this so that instead we move to our new "Validating Account" state and call the "Account Validation Request". This should like the below when done:
Now it’s calling the request, we need to handle the response. In our case, we’re going to say that success goes to complete whilst both not found and failed will go to our rejected state.
For the failure case, we’ll use a new feature of the event behaviour that we’ve not seen before - the ability to use the "On Any Of" when condition. This means we can say when either the "Account Not Found" or the "Account Validation Failed" event arrives, we can perform the same functionality.
We do this just as before, except choosing "On Any Of" from the "When" drop down. Once we’ve selected the first of our two events, we simple press return and then can use our normal selection approach to pick another.
Once complete this should like:
That’s now all the flow definition work complete, but before moving on let’s open the flow viewer again  (Tools > Open Flow Viewer) and see our new diagram.
And as expected here, we can see that we have now included our account validation request in our happy path.
Now we look at the implementation side of this, if you’re not interested in this then you can jump straight to the exercise.
Java Implementation
Let’s now switch back to Intellij and look at how we plug this into our implementation code. As normal we’ll start by running a build from a terminal window in the root directory of our project:
mvn clean install
Once built,  we can again look at the generated code in /domain-root/domain/target and we should now find the port for calling out to our accounting system like this:
package com.iconsolutions.ipf.tutorial.adapters;
import om.iconsolutions.ipf.tutorial.actions.AccountValidationRequestAction;
import java.util.concurrent.CompletionStage;
public interface AccountingSystemActionPort {
    CompletionStage<Void> execute(AccountValidationRequestAction var1);
}
So just like with our domain functions, this is what we need to implement in order to be able to call out to our external systems. This is the point that in a typical implementation we would use the connector framework to make a call out to the external system over a protocol (e.g. kafka). You’ll deal with the connector framework later in this tutorial series so for now we’ll use another feature of our generated code - the sample application.
First we need to add the dependency to the sample app, we do this by adding the following dependency into the pom for the ipf-tutorial-app (ipf-tutorial/ipf-tutorial-app/pom.xml).
<dependency
     <groupId>com.iconsolutions.ipf.tutorial.{solution}.domain</groupId>
     <artifactId>sampleapp</artifactId>
     <version>${project.version}</version>
</dependency>
Note that solution will be whichever solution folder you are currently working in e.g. initial would be com.iconsolutions.ipf.tutorial.initial.domain
If we look inside the sample app project (domain-root/sampleapp/target/generated-sources/com/icon/tutorial/ipftutorialmodel/sample_app)  we can see sample implementations of all of our ports. Functionally all these do is return with the first response code available to it but they are sufficient to be able to get a running application going. This makes them extremely useful as they can be used immediately to get our application running and then swapped out one by one as real implementations our developed.
Let’s now add the sample AccountSystemActionPort implementation into our configuration. We do that by going back to our IPFTutorialConfig and adding an extra line:
@Bean
public IpftutorialmodelDomain init(ActorSystem actorSystem) {
    // All adapters should be added to the domain model
    return new IpftutorialmodelDomain.Builder(actorSystem)
            .withDomainFunctionAdapter(input -> CompletableFuture.completedStage(new DuplicateCheckResponseInput.Builder(input.getId(), AcceptOrRejectCodes.Accepted).build()))
            .withAccountingSystemActionAdapter(new SampleAccountingSystemActionAdapter())
            .build();
}
Here we can see we’ve added a declaration to use our SampleAccountSystemActionAdapter.
That’s everything we need to do to add a simple implementation of our call to the accounting system.  In later sections we’ll provide more complex examples and how we could use Icon’s connector framework to call out to real systems.
Checking our Solution
We’re now ready to go and run our application again, Let’s start up the application as previously (instructions are available in Reviewing the initial application if you need a refresher!)
Then we can send in a payment:
curl -X POST localhost:8080/submit | jq
And finally if we bring up the payment in the Developer GUI and bring up the domain events tab (search by unit of work id, click view, click domain events)
Here we can see we now have three events coming through including our new "Account Validation Passed" as the 3rd event.
Exercise
As an exercise let’s add another step to our flow.  Let’s imagine we need to run a fraud check as part of our flow. The requirements of the fraud check service we will call are:
It takes a pacs008 as input data.
It can return either a pass or fail.
We only run the fraud check if the account has been successfully validated.
If the check fails, the payment should be rejected.
If the check passes, the payment should be completed.
Go ahead and try to implement a solution for this.  Use the query service to check that your new Fraud capability has been invoked.
When ready you can compare your solution to the "add_external_domain" solution.
Conclusions
In this section we:
successfully created our first external domain which allowed us to simulate making a call out to an external accounting service.
set up a request within our domain and integrated that into our event behaviour to send a request and later receive a response from the account system.
had a light touch introduction to response codes!
Now having configured our application to make calls to an external service, let’s discover how to use routing logic in our flows via: DSL 5 - Using a decision
DSL 3 - Using a Domain Function
DSL 5 - Using a decision
------------------------------
DSL 5 - Using a Decision
Getting Started
The tutorial step uses the "add_external_domain" solution of the project as it’s starting point.
If at anytime you want to see the solution to this step, this can be found on the "add_decision" solution!
What is a Decision?
So far we have added a domain function (internal call) and an external domain (external call). Now it’s time to consider some of the processing options available to us to determine how to process our payments. For that we use "decisions".
A "decision" in the DSL is very simple, it is a function that takes in some data and returns a result. It is similar to a domain function and indeed you could perform most decision logic using a domain function, however decisions have the advantages of:
A decision does not change data
A decision does not require definition of responses
A decision does not require definition of input behaviour
A decision does not require events to be defined.
Effectively therefore a decision can be thought of as a lightweight type of domain function that is used only for directing processing.
In this section of the tutorial we’re going to use a decision to perform the following logic:
If the value of the payment is < 10, then we skip the fraud check.
If the value of the payment is > 10, then we execute the fraud check.
It’s a very simple use case but will illustrate the basics of the concept.
DSL Set Up
Adding a Decision
Let’s start by adding a new decision library to our model. So as previously  we right click on the model and go New  v2Flo  Decision Library.
This should bring up the new decision library page:
So lets go ahead and add our logic for our decision. Firstly we’ll add the fields as we’ve done many times in previous steps:
For name we will use "Run Fraud Check"
For description we will use "Checks payment value requires a fraud check"
For business data we will use "Customer Credit Transfer"
The next field we have is "Outcomes" which is a new concept.
An "Outcome" is simply a possible result of the decision. A decision can have as many different outcomes as you like - these are very similar to the "Response Codes" we used earlier.
The outcome field is simple free text allowing us to enter a name, so let’s specify two outcomes:
FRAUD_REQUIRED
SKIP_FRAUD
To enter new outcomes we simply press the return key after completing the previous one.
Once complete, our decision should look like:
That’s our decision defined and ready to use, so the next step is to integrating it to the flow.
Using a Decision
A decision is handled slightly differently to the domain function or external domain calls that we have used to date. In those examples, we used specific concrete steps to represent the fact that the system was performing those functions. In the case of a decision however, it is only a routing position on the flow and as such we now introduce the concept of a pseudo state. These will be used in different cases later in the tutorial but for now we’ll just use the one we need for decisions which we’ll refer to as a decision state.
As mentioned in the introduction, the decision is very lightweight so there is no need to add events or input behaviours here. We can jump straight to the Event Behaviour processing.
We effectively now want to say "Once we have received the Account Validation Passed event, we should run the fraud check decision. If it requires a fraud check, we’ll execute the fraud check as before otherwise we’ll simply complete our payment".
So let’s start by editing our handing of the "Account Validation Passed" event, before we had:
Now instead of moving to the "Checking Fraud" state we will move to the "Run Fraud Check" decision state and execute our decision. For this instead of selecting a concrete state like "Checking Fraud" in the "Move To State" section we choose to "Create Decision State".
Here we can enter the name of the state as "Run Fraud Check" (noting that this does not have to be the same name as the decision we intend to use. It is a good general rule to do so but by allowing any naming here we can reuse the same decision in multiple different use cases throughout our flow).
Then in the perform action, we choose "Routing Decision" and then we simply choose the "Run Fraud Check" decision (which we created in our "Decision Library".
Once complete, we should look like this:
Now let’s handle the result of our fraud check. So, for that we will to add Event Behaviours to either complete the flow (if the check is skipped) or run the fraud check (if the check is required).
So let’s start by adding the skip case. You’ll add a new event behaviour and then specify:
the "With Current State" to be our "Run Fraud Check" decision state.
the "When" to be "On"
For the event, we use a special type of event - the "Decision Outcome Event" so we select "Decision Outcome" and then choose our "SKIP_FRAUD" outcome.  Then to complete our behaviour we simply move to "Complete". Once finished it should look like:
Next we need to do the behaviour when the decision says we need to run the fraud check. In that case we simply want to move to the "Checking Fraud" state and run the "Fraud Check" request. Try and add this, or if you prefer the solution is below:
That’s it, we’ve now integrated our decision into our flow. Let’s open it in the Flow Viewer (Tools > "Open Flo Viewer") and see what it looks like:
Top Tip
As our graphs get more complicated there’s a few useful things to be aware of.
The first is at the top we have an option to "Show Actions" by default this is on but unchecking it will change the view to only show state transitions which may be easier to view.
The second is that we have the ability to manipulate the view of the diagram:
Shortcut
Purpose
Ctrl+I/CTRL+O
OR
SHIFT+Mouse Right Click
Zoom In/Out
Arrow Keys
OR
SHIFT+Mouse Left Click
Move around the diagram
CTRL+Left Click
Zoom in to drawn selection box
CTRL+Right Click
Rotate diagram
Note: If you want to reset to the default view you will need to minimise/close the flo viewer and re-open
Java Implementation
The Decision Interface
Let’s switch to Intellij to work with the java side.
First we need to regenerate the application code to pick up the changes we’ve made in our DSL editing. You’ll do this by running the following from the root of our project (ifp-tutorial):
mvn clean install
This should take a minute or so as all the code and dependencies are generated. Once it’s complete, navigate to the target directory of the domain-root/domain project and we should find a new Decision Interface in the decisions package (& emun for generated outcomes):
package com.iconsolutions.ipf.tutorial.ipftutorialmodel.decisions;
public interface DecisionLibraryPort {
  RunFraudCheckDecisionOutcomes performRunFraudCheck(RunFraudCheckDecisionParameters decision);
}
So as with other ports we have considered previously, the check takes in a set of parameters - in our case this is a holder for just the customer credit transfer but we may be sending multiple items to business data to a decision. It will return a new enum element (RunFraudCheckDecisionOutcomes) which has been generated for each of the outcomes we defined.
As per our requirements, if the value of the payment is <10 we should skip the fraud check and if > 10 we should run the fraud check. Have a go at implementing this logic and wiring it into the domain. Once ready, you can see the solution below:
@Bean
public IpftutorialmodelDomain init(ActorSystem actorSystem) {
    // All adapters should be added to the domain model
    return new IpftutorialmodelDomain.Builder(actorSystem)
            .withDomainFunctionAdapter(input -> CompletableFuture.completedStage(new DuplicateCheckResponseInput.Builder(input.getId(), AcceptOrRejectCodes.Accepted).build()))
            .withAccountingSystemActionAdapter(new SampleAccountingSystemActionAdapter())
            .withFraudSystemActionAdapter(new SampleFraudSystemActionAdapter())
            .withDecisionLibraryAdapter(input ->
                    input.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getIntrBkSttlmAmt().getValue().compareTo(BigDecimal.TEN)>0 ?
                            RunFraudCheckDecisionOutcomes.FRAUDREQUIRED : RunFraudCheckDecisionOutcomes.SKIPFRAUD)
            .build();
}
That’s it, our implementation is now complete.
Checking our Solution
Let’s check our application changes work, as usual we’ll start up the application as previously (instructions are available in Reviewing the initial application if you need a refresher!)
Fraud Required
Now we can use our ability to send in different payment values using the values on the initiation request (see  for a refresher!).
curl -X POST localhost:8080/submit -H 'Content-Type: application/json' -d '{"value": "150"}' | jq
This will send in a payment value of 150 USD. So remembering our implementation in this case a fraud check should be required. Let’s bring up the payment in the Developer GUI and bring up the domain events tab (search by unit of work id, click view, click domain events)
So we are now getting the new event for "Run Fraud Check FRAUD_REQUIRED" followed by the fraud check running, and we can see that our code is working as we would expect.
For interest, let’s also look at the graph here (using the "Flows" button)
Here we can see the path being taken, including the decision itself (represented as a diamond).
Fraud Skipped
Let’s now try our reverse case by sending in a payment with a value < 10 USD and then look at the events for our returned payment (remembering to replace the aggregate id with yours!):
curl -X POST localhost:8080/submit -H 'Content-Type: application/json' -d '{"value": "5"}' | jq
This time, we would expect to see the decision execute and decide that we can skip fraud and let the flow complete immediately.
Let’s bring up the payment in the Developer GUI and bring up the domain events tab (search by unit of work id, click view, click domain events)
And we find exactly what we expected! Hence we’ve shown that we’re now invoking our decision and we’re successfully skipped our fraud check!
Conclusions
In this section, we’ve:
Added a decision
provided a default implementation that checks payment value
used our decision to drive flow routing
verified our decision is working and being called
Now having configured our application to make decisions on how to process the payment, let’s look an another capability: DSL 6 - Using an aggregate function
DSL 4 - Using an external domain
DSL 6 - Using an aggregate function
------------------------------
DSL 6 - Using an aggregate function
Getting Started
The tutorial step uses the "add_decision" solution of the project as it’s starting point.
If at anytime you want to see the solution to this step, this can be found on the "add_aggregate_function" solution!
What is an Aggregate Function?
An aggregate is the live data store for a particular payment, it’s kind of the summary of all the events received to date in one place so that it can be quickly used and passed on where needed. However, what happens if you want to perform an action against that aggregate, say transform some data, track updates or do a calculation? This is where aggregate functions come in, the provide the capability for us to run a function against the aggregate data when a new event comes in.
An aggregate function therefore is relatively simple to define:
An aggregate function occurs on receipt of a given event
An aggregate function has access to the events data and any data previously on the aggregate
An aggregate function defines which data it provides in return - this could either be an update of previous data or the generation of new data.
Data generated by an aggregate function however is not persisted to the master journal.
We’re going to use an aggregate function as a method of being more precise in what we send to our accounting system when we made our Validate Account request in. In that case we sent the entire pacs008 to the accounting system call, now we want to just send the part of the pacs008 that has the account information in. So let’s use an aggregate function as a way of splitting up the pacs.008 and just making the relevant information available.
DSL Set Up
Setting up a new Business Data Point
Up until now we haven’t really considered business data, only using the supplied customer credit transfer. Now that we want to use a different type of data we will need to define that to make it available to our flow. This is where business data comes in.
Let’s start by creating a new business data library by right-clicking on our ifptutorialmodel and selecting (New > v2Flo > Business Data Library).
You’ll be presented with the business data library screen where we’ll set:
A description of "A sample data library"
A business data element (clicking "Add Business Data") containing:
A name of "Creditor Account"
A description of the "the creditor details"
For the data type we’ll select "CashAccount" (note - there are types for each ISO message type, you will need the pacs008 version)
Once done we should see:
Top Tip
Any java object can be used as a business data type. These java types can be imported into the project (we’ll cover this in <LINK TO OBJECT RESUSE>. If the type is in scope but not showing in the type ahead, trying pressing CTRL+R as this will search across objects not currently imported that the model can see too!
Updating the Request
Now that we have our data type, the first job is simply to swap out the "Business Data" on the call to the "Account Validation Request" so that instead of taking the "Customer Credit Transfer" as input business data we now take the "Creditor Account". Once done the new request definition should look like:
That’s all the supporting setup complete, now it’s time to actually create our aggregate function. If we remember, the function was due to extract the creditor details from the customer credit transfer. In our case, the customer credit transfer details come in on the "Flow Initiation" event, so let’s use that to set up our aggregate function.
Creating the Aggregate Function
So we start by clicking the "Add Function" button on our flow and then entering the following parameters:
A name of "Extract Creditor Account"
A description of "extracts the creditor account from the pacs008"
An event of "Flow Initiation"
the Input data as "Customer Credit Transfer"
the output data as "Creditor Account"
So here’s we are simply telling the flow that when the customer credit transfer is received during initiation, to extract the creditor details so that it can be used later in the flow. Our new function should look like:
That’s all our flow work done, the flow will now extract the creditor account details from the pacs008 and use that to send on to the accounting system.
Now let’s look at the implementation side of this requirement.
Java Implementation
Let’s now switch back to Intellij and look at how we plug this into our implementation code. As normal we’ll start by running a build from a terminal window:
mvn clean install
Once built,  we can again look at the generated code in /domain-root/domain/target and we should now find the port for calling out to our aggregate function like this (/domain-root/domain/target/classes/com/icon/tutorial/ipftutorialmodel/ipftutorialflow/aggregatefunction):
public interface IpftutorialflowAggregateFunctionPort {
  ExtractCreditorAccountForFlowIpftutorialflowAggregateFunctionInput performExtractCreditorAccount(ExtractCreditorAccountForFlowIpftutorialflowAggregateFunctionParameters parameters);
}
Here we can see the definition of our aggregate function port, it’s pretty straight forward. So we now need an adapater implementation for it. We do this as normal by adding it to the domain declaration in the IPF Tutorial configuration. Try and do that now and an implementation is below:
@Bean
public IpftutorialmodelDomain init(ActorSystem actorSystem) {
       // All adapters should be added to the domain model
        return new IpftutorialmodelDomain.Builder(actorSystem)
                .withTutorialDomainFunctionLibraryAdapter(input -> CompletableFuture.completedStage(new DuplicateCheckResponseInput.Builder(input.getId(), AcceptOrRejectCodes.Accepted).build()))
                .withAccountingSystemActionAdapter(new SampleAccountingSystemActionAdapter())
                .withFraudSystemActionAdapter(new SampleFraudSystemActionAdapter())
                .withDecisionLibraryAdapter(input ->
                        input.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getIntrBkSttlmAmt().getValue().compareTo(BigDecimal.TEN)>0 ?
                                RunFraudCheckDecisionOutcomes.FRAUDREQUIRED : RunFraudCheckDecisionOutcomes.SKIPFRAUD)
                .withIpftutorialflowAggregateFunctionAdapter(input -> new ExtractCreditorAccountForFlowIpftutorialflowAggregateFunctionOutput(input.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getCdtrAcct()))
                .build();
}
Checking our Solution
Here we can see that we have added our aggregate function to our configuration and everything is now complete and ready to test, so as normal let’s now check out solution works. Start up the application as previously (instructions are available in Reviewing the initial application if you need a refresher!)
Then we can send in a payment:
curl -X POST localhost:8080/submit -H 'Content-Type: application/json' -d '{"value": "5"}' | jq
Note - the choice of payment value is fairly arbitrary here as we’re only interested in seeing the change to accounting data. To check our change is working, we’ll need to look at the aggregate itself. So this time we’ll query for the whole aggregate:
Let’s bring up the payment in the Developer GUI and bring up the aggregate view (search by unit of work id → click "view" → "click to view aggregate data") and then towards the end of the aggregate details we should see the new creditor account details:
So there we have it, we have successfully extracted out the creditor details from our aggregate and can now use it elsewhere in the flow!
Conclusions
In this section we’ve:
learnt how to use aggregate functions to convert data types.
Now having configured our application to use an aggregate function, let’s look an another capability in: DSL 7 - Handling Timeouts
DSL 5 - Using a decision
DSL 7 - Handling Timeouts
------------------------------
DSL 7 - Handling Timeouts
Getting Started
The tutorial step uses the add_aggregate_function solution of the project as it’s starting point.
If at anytime you want to see the solution to this step, this can be found on the handling_timeouts solution.
Action Timeouts
In this section we’ll look at how we handle action based timeouts. Suppose we have a simple flow that is making a call out to an external domain, the call is successfully made, but then we never receive the allotted response. In that scenario, we may want to perform some form of compensating action.
To illustrate this, we’re going to use our Fraud system and look at how we cope with not getting a response from the downstream domain.
Configuring an Action Timeout (in the DSL)
Configuring action timeouts in the DSL is simple, all we need to do is add a special type of Event Behaviour line to tell the flow what compensating approach to take.
Let’s start therefore by opening MPS and going to our flow. Then let’s add a new Event Behaviour ("Add Event Behaviour").
Let’s add the basics of our event behaviour by saying it’s on the "Checking Fraud" state - that’s the state we would be in had we successfully made our call to the fraud system.
Now from the "For Event", we choose a new type we haven’t used before - the "Action Timeout"
Once selected, we have to choose the action that applies:
The first thing to note here is the default that has been applied "Any action" - this means that a timeout of "any action" on the "checking fraud" state will invoke the behaviour. In our case we only invoke the "Check Fraud" action on checking fraud so it doesn’t make too much difference, but there maybe occasions we are firing multiple actions and want to handle the outcome of any of them timing out.
For our scenario, we will therefore simply select "Check Fraud".
Now we need to enter the "Move To State" and "Perform Actions" as per any other event behaviour. Here we could do anything we would normally like call other actions, decisions etc. However, for simplicity in this tutorial we’ll simply move our flow to a "Timed Out" state.
So let’s add a new state called "Timed Out",  we’ll mark is as terminal to treat it as the end of the flow and to let our initiation flow know what’s happening!
and set that to our move to state. We won’t perform further actions.
Our event behaviour will look like:
That’s all our DSL work done, now let’s look at our graph for our  flow.
Open the flow by going to Tools  Open Flo Viewer. This will open the flow and at first glance, this doesn’t look any different. However, at the top of the panel we’ll see:
So here we now have a new option "Show Timeouts". Let’s click that box and "Apply" the updates (note - if you have Flo Viewer open already, you may have to close and re-open to see "Show Timeouts").
The new graph will show:
Here we can see that on the timeout from check fraud we’re routing to the new Timed Out state.
Note also that the graph shows that this transition will occur on the "CheckFraudActionTimeoutEvent" event. This event is generated by the application when the fraud call times out.
Java Implementation
As normal, let’s start our implementation by regenerating our code base.
mvn clean install
Let’s start by looking a little at how the code works.
You’ll start by opening the "SchedulerPort" interface (part of the IPF Core):
public interface SchedulerPort {
    void schedule(ScheduleItem var1, Function<ScheduleItem, CompletableFuture<Void>> var2);
    void schedule(ScheduleItem var1, Function<ScheduleItem, CompletableFuture<Void>> var2, Duration var3);
    void cancel(ScheduleItem var1);
    void cancelAll(String var1);
    int getRetries(ScheduleItem var1);
}
It is these functions that the generated code will invoke whenever it needs to set a schedule. In our case, whenever an action is called it will invoke the schedule method and provide a Schedule Item which contains the action details and a type of "TIMEOUT".
So if you specify to the scheduler that you want a timeout in 10s on this action, then it will return a failure after 10s if the schedule is still active. However, if in that time a cancel is called then this will close out the scheduler.
The IPF application needs an implementation of the scheduler port to be provided as part of the domain. However, so far we haven’t had to specify one! Why is this? It’s because by default a no-op scheduler is provided. What we need to do now is provide an appropriate implementation for our case.
The Akka Scheduler
You could here use any scheduler that conforms to the interface definition above,  for this tutorial we’ll use another one provided by the IPF framework, the AkkaScheduler.
Let’s start by adding a dependency into our ipf-tutorial-app’s pom (ipf-tutorial/ipf-tutorial-app/pom.xml):
<dependency>
    <groupId>com.iconsolutions.ipf.core.platform</groupId>
    <artifactId>ipf-flo-scheduler-akka</artifactId>
</dependency>
(note - you may need to reload Maven to pull the dependency down)
By adding the dependency, spring will automatically inject an instance of the scheduler into our application.  So all we need to do is configure our domain to use, it for this we simply need to update the domain declaration in IPFTutorialConfig.java to specify the scheduler adapter:
@Bean
public IpftutorialmodelDomain init(ActorSystem actorSystem, SchedulerPort schedulerAdapter) {
    // All adapters should be added to the domain model
    return new IpftutorialmodelDomain.Builder(actorSystem)
            .withTutorialDomainFunctionLibraryAdapter(input -> CompletableFuture.completedStage(new DuplicateCheckResponseInput.Builder(input.getId(), AcceptOrRejectCodes.Accepted).build()))
            .withAccountingSystemActionAdapter(new SampleAccountingSystemActionAdapter())
            .withFraudSystemActionAdapter(new FraudSystemActionAdapter())
            .withDecisionLibraryAdapter(input ->
                    input.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getIntrBkSttlmAmt().getValue().compareTo(BigDecimal.TEN)>0 ?
                            RunFraudCheckDecisionOutcomes.FRAUDREQUIRED : RunFraudCheckDecisionOutcomes.SKIPFRAUD)
            .withIpftutorialflowAggregateFunctionAdapter(input -> new ExtractCreditorAccountForFlowIpftutorialflowAggregateFunctionOutput(input.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getCdtrAcct()))
            .withSchedulerAdapter(schedulerAdapter)
            .build();
}
That’s us done from a code perspective.
Configuration
Our final job is to configure our scheduler. Configuration is done by using properties, the property string we require is of the format:
ipf.flow.<FLOW_NAME>.<STATE_NAME>.<ACTION_NAME>.timeout-duration=<DURATION>
The first thing to note here is that we need to provide the values of the three optional parameters - our flow name, state name, action name - and provide the value for the duration.
In our case then our property should look like:
ipf.flow.Ipftutorialflow.CheckingFraud.CheckFraud.timeout-duration=2s
The first thing to note in defining this property is that where we have spaces in any of the components, we simply ignore the space. So for example our action name is actually Check Fraud but we simply use CheckFraud. We’re also specifying a duration here of 2 seconds.
Let’s add this property into the application configuration. To do this open the file ipf-tutorial-app/application.conf and add the line above.
Enabling a Test Setup
We’re almost set to test our timeout, the one thing remaining is to actually make the check fraud call have an ability to timeout. Let’s update the definition of the fraud check we supplied previously to not use the sample fraud adapter but to allow an optional time out too. Let’s create a new package in our ipf-tutorial-app project for adapter implementations - com.iconsolutions.ipf.tutorial.app.adapters.
Then in our new package we’ll add an implementation of the FraudActionPort.
You’ll use the idea of "magic values" to say that if a payment of value > 50 USD is received by the fraud call we’ll timeout, otherwise we’ll return successfully. Try and implement that yourself and the solution when ready is below (Hint - take a look at the generated class example SampleFraudSystemActionAdapter in domain-root/sampleapp):
@Slf4j
public class FraudSystemActionAdapter implements FraudSystemActionPort {
    private static final Logger LOG = LoggerFactory.getLogger(SampleFraudSystemActionAdapter.class);
    private Duration duration = Duration.ofMillis(10);
    @Override
    public CompletionStage<Void> execute(final CheckFraudAction action) {
        LOG.debug("Received an action of type {} for id {}", action.getActionName().name(), action.getId());
        if (action.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getIntrBkSttlmAmt().getValue().compareTo(new BigDecimal("50")) >= 0) {
            return CompletableFuture.supplyAsync(() -> "delaying response", CompletableFuture.delayedExecutor(duration.toNanos(), TimeUnit.NANOSECONDS))
                    .thenAccept(string -> log.debug("Pretending to timeout the fraud call for aggregate {}", action.getProcessingContext().getAssociationId()));
        } else {
            return CompletableFuture.supplyAsync(() -> "delaying response", CompletableFuture.delayedExecutor(duration.toNanos(), TimeUnit.NANOSECONDS))
                    .thenCompose((String string) -> IpftutorialmodelDomain.fraudSystem().handle(new FraudCheckResponseInput.Builder(action.getId(), AcceptOrRejectCodes.Accepted).build()).thenAccept((Done done) -> log.debug("Sent input of type {} for id {} with result {}", done.getCommandName(), action.getId(), done.getResult().name())));
        }
    }
}
(Note - you may have a slightly different implementation for the FraudCheck (built at the end of section 'DSL 4') and your solution may use different classes (i.e. if you didn’t start from a fresh clone at the start of this section). For example the above expects AcceptOrRejectCodes, for the FraudCheckResponseInput, but you may have implemented separate response codes for fraud)
Finally we then need to add our new adapter into our config as normal (we are changing the .withFraudSystemActionAdapter to use our newly created Adapter FraudSystemActionAdapter):
@Bean
public IpftutorialmodelDomain init(ActorSystem actorSystem, SchedulerPort schedulerAdapter) {
    // All adapters should be added to the domain model
    return new IpftutorialmodelDomain.Builder(actorSystem)
            .withDomainFunctionAdapter(input -> CompletableFuture.completedStage(new DuplicateCheckResponseInput.Builder(input.getId(), AcceptOrRejectCodes.Accepted).build()))
            .withAccountingSystemActionAdapter(new SampleAccountingSystemActionAdapter())
            .withFraudSystemActionAdapter(new FraudSystemActionAdapter())
            .withDecisionAdapter(input ->
                    input.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getIntrBkSttlmAmt().getValue().compareTo(BigDecimal.TEN)>0 ?
                            RunFraudCheckDecisionOutcomes.FRAUDREQUIRED : RunFraudCheckDecisionOutcomes.SKIPFRAUD)
            .withIpftutorialflowAggregateFunctionAdapter(input -> new ExtractCreditorAccountForFlowIpftutorialflowAggregateFunctionOutput(input.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getCdtrAcct()))
            .withSchedulerAdapter(schedulerAdapter)
            .build();
}
That’s everything complete, time to build and spin up the container environment to check it all works:
Checking our Solution
As normal let’s now check that the solution works. Start up the application as previously (instructions are available in Reviewing the initial application if you need a refresher!)
For payments, we need to reconsider our logic we’ve built:
If a payment is over $50 (but over 10 to make sure the fraud check is required!) then we time out, if it’s not we proceed as before.
So let’s try both scenarios starting with a payment over $50:
curl -X POST localhost:8080/submit -H 'Content-Type: application/json' -d '{"value": "150"}' | jq
Let’s bring up the payment in the Developer GUI and bring up the domain events view (search by unit of work id, click view, click domain events):
Here we can see that this time we’ve successfully got the timeout event coming through. For confirmation, if we now repeat the process with a value of say $25 we’ll see that the fraud check has happily processed successfully and not timed out so our flow has proceeded onto completion.
Persistent Scheduling
IPF provides another scheduling interface by default.  This is the persistent scheduler.  It is quartz backed and the major difference is that is is backed by a persistence layer and as such should the application fail, the schedules will be available post jvm shutdown.
To apply the persistent scheduler we simply need to replace the akka scheduler dependency with the one for our persistent scheduler:
<dependency>
    <groupId>com.iconsolutions.ipf.core.platform</groupId>
    <artifactId>ipf-flo-scheduler-persistent</artifactId>
</dependency>
If you want, go ahead and try this change, you can repeat the testing we did above to prove that our new scheduler is working.
Conclusions
In this section we’ve learnt how to setup timeouts on actions and work with when they are invoked.
DSL 6 - Using an aggregate function
DSL 8 - Versioning
------------------------------
DSL 8 - Versioning
Getting Started
The tutorial step uses the "handling_timeouts" solution of the project as it’s starting point.
If at anytime you want to see the solution to this step, this can be found on the "add_version" solution!
Versioning
Suppose that we have a flow running in production. A requirement then comes in to add a new step to the flow - so we want the in-flight transactions to stay on the current flow but all new transactions to start processing on the new updated flow. We can achieve this by versioning.
For the purpose of this tutorial we’re going to insert a step to call out to a CSM service. We’re going to do this by creating a new version of our execution flow that contains the extra step. So let’s get on and see how this works.
Defining the CSM Service
The first thing we need to do is to call our CSM service. We will:
Send the CSM a pacs.008
Receive a pacs002 from the CSM - this can either contains an accepted or rejected code.
For this we’ll need a new external domain and a request defined on it. It’s very similar to what we did in DSL 4 - Using an external domain. You’ll call our request "Clear and settle".
See if you can set it up yourself, and the solution is below:
Versioning the Flow
Time to set up our versioning, if we look at our current flow we’ll see:
Here we can see it says "No Versioning". Let’s go ahead and add versioning, to do this we will press ALT + ENTER on the flow and then select "New Version" from the drop down:
If you look in our navigator, we’ll now see:
Let’s open "Ipftutorialflow (V2)", what we’ll find is an exact clone of our original flow, except it now has the versioned specified as V2 (note - the original flow has Version now specified).
Now we’re ready to edit our version two flow. So whereas previously we had the following "Event Behaviour":
You’ll change this so that instead of moving to "Complete" and raising our additional event, we will move to a new status "Clear And Settling" and then call our Clear and Settle request (defined on the CSM Service).
If the clear and settle request is successful, we’ll move to completing and raise our event. If the clear and settle fails, we’ll move to Rejected.
Try and add those conditions now and the solution is below, remember we’ll need to add the appropriate States, Event definitions, Input Behaviour and lastly Event behaviour!
(If you need a recap, review: DSL 4 - Using an external domain)
First of here’s the state definition:
Then we’ll need an event for both positive and negative CSM response:
Don’t forget when setting up these events, that the CSM service is sending us back the pacs002 and we want to store that as business data!
Next up is our input behaviour, it’s very standard:
And finally our event behaviour!
Note here that we’ve applied the clear and settling logic to both the "Skip Fraud" and "Fraud Required" cases!
That’s everything we need to do, we’ve now successfully created a new flow version and updated it to use our CSM Service. If we look at the flow for version 2 we’ll see:
Whereas the flow for version 1 remains without this step.
That’s all our DSL work done, so let’s now move onto the implementation side.
Java Implementation
As normal, let’s start our implementation by regenerating our code base.
mvn clean install
In this case the build should fail:
If we check the IfpTutorialConfig.java we see:
This is failing because the aggregate function is unique to a flow, and now that we’ve created our new version we need to tell the application how to apply the aggregate function for each flow - as it’s perfectly possible the implementation logic may change. However, in our case we just need to do the same thing, so we’ll simply define the new functions in the same way. Note that the new versions will be named V1 and V2 respectively.
We’ve also added the new CSM Service, so we need to remember to include that in our domain configuration. For this we’ll just use the sample one provided.
Have a go at updating and the solution is below:
@Bean
public IpftutorialmodelDomain init(ActorSystem actorSystem, SchedulerPort schedulerAdapter) {
    // All adapters should be added to the domain model
    return new IpftutorialmodelDomain.Builder(actorSystem)
            .withTutorialDomainFunctionLibraryAdapter(input -> CompletableFuture.completedStage(new DuplicateCheckResponseInput.Builder(input.getId(), AcceptOrRejectCodes.Accepted).build()))
            .withAccountingSystemActionAdapter(new SampleAccountingSystemActionAdapter())
            .withFraudSystemActionAdapter(new FraudSystemActionAdapter())
            .withDecisionLibraryAdapter(input ->
                    input.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getIntrBkSttlmAmt().getValue().compareTo(BigDecimal.TEN)>0 ?
                            RunFraudCheckDecisionOutcomes.FRAUDREQUIRED : RunFraudCheckDecisionOutcomes.SKIPFRAUD)
            .withIpftutorialflowV1AggregateFunctionAdapter(input -> new ExtractCreditorAccountForFlowIpftutorialflowV1AggregateFunctionOutput(input.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getCdtrAcct()))
            .withIpftutorialflowV2AggregateFunctionAdapter(input -> new ExtractCreditorAccountForFlowIpftutorialflowV2AggregateFunctionOutput(input.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getCdtrAcct()))
            .withSchedulerAdapter(schedulerAdapter)
            .withCSMServiceActionAdapter(new SampleCSMServiceActionAdapter())
            .build();
}
(note - if you check domain-root/domain/target/classes/com/icon/tutorial/ipftutorialmodel/domain/IpftutorialmodelDomain.class, you can see V1 and V2 versions to be called)
That’s everything we have to do to make our code now work,  but before continuing let’s go back to our initiation controller (ipf-tutorial-app/src/main/java/com/icon/tutorial/ipftutorialsolution/app/controller/InitiationController.java) and consider how it’s working:
return Mono.fromCompletionStage(IpftutorialmodelDomain.initiation().handle(new InitiateIpftutorialflowInput.Builder(entityId)
              .withProcessingContext(ProcessingContext.builder()
                      .unitOfWorkId(unitOfWorkId)
                      .clientRequestId(clientRequestId)
                      .build())
              .withCustomerCreditTransfer(dummyPacs008)
              .build()).thenApply(done -> InitiationResponse.builder().requestId(clientRequestId).uowId(unitOfWorkId).aggregateId(done.getAggregateId()).build()));
So we’re sending it here just a common "InitiateIpftutorialflowInput" - how does the application know which flow to use? The answer is that by default it will always route "New" work to the latest (V2 in our case) flow. However, we can also choose which flow to send data to.
Now, for testing purposes let’s recall that we have a version fields on our InitiationRequest object, let’s use that to be able to pick and choose which flow to send to. So let’s update our initiation logic to be like the below:
  return Mono.fromCompletionStage(IpftutorialmodelDomain.initiation().handle(new InitiateIpftutorialflowInput.Builder(entityId)
               .withProcessingContext(ProcessingContext.builder()
                       .unitOfWorkId(unitOfWorkId)
                       .clientRequestId(clientRequestId)
                       .build())
               .withCustomerCreditTransfer(dummyPacs008)
                       .withVersion(request == null || request.getVersion() == null ? null : "V1".equals(request.getVersion()) ? IpftutorialflowFlowVersions.V1 : IpftutorialflowFlowVersions.V2)
               .build()).thenApply(done -> InitiationResponse.builder().requestId(clientRequestId).uowId(unitOfWorkId).aggregateId(done.getAggregateId()).build()));
Here we can see we’ve added a line to set the version on the request based off the version property we’ve supplied (note - the use of the IpftutorialflowFlowVersions enum which contains "V1" and "V2").
In addition to specifying the version as we did above, there are explicit input’s (ipftutorialmodel/inputs) you can use too. So instead of InitiateIpfTutorialflowInput in the above you could have used InitiateIpfTutorialflowV1Input for example.
Checking Our Solution
As normal let’s now check out solution works. Start up the application as previously (instructions are available in Reviewing the initial application if you need a refresher!)
You’ll start by sending in a payment without specifying a version
curl -X POST localhost:8080/submit | jq
Now the first thing to note is the response:
{
  "requestId": "c16a5c43-1038-4311-9d3f-8bf34efa0c81",
  "uowId": "0945fe73-521c-478e-9f62-df4ac6393091",
  "aggregateId": "IpftutorialflowV2|239f3e48-8d26-4a2f-8241-0997dc25f1c2"
}
So here we can see that we’ve hit our V2 flow from the aggregate id "IpftutorialflowV2|…​.". If we now double check the events (remember to update the aggregate id to match yours!):
Let’s bring up the payment in the Developer GUI and bring up the flow events view (search by unit of work id, click view) and we should see:
Here we can see that the process flow being executed is indeed our V2 flow, and looking at the events view shows the same (click domain events):
Now let’s try hitting our V1 version:
curl -X POST localhost:8080/submit -H 'Content-Type: application/json' -d '{"version": "V1"}' | jq
And again, looking at the response:
{ "requestId": "492a0177-d9c3-4845-bc81-f54c9aae917d", "uowId": "d8cf8b99-448b-44e7-8207-a015dc41623a", "aggregateId": "IpftutorialflowV1|b0e19b4a-34f6-4584-8109-eef311fd2a13" }
So here we can see that we’ve hit our V1 flow from the aggregate id "IpftutorialflowV1|…​.". If we now bring up the payment in the Developer GUI and bring up the flow view (search by unit of work id, click view) and we should see:
Showing us again we’re hitting the right process flow. If we check the domain events tab (click domain events) then we see there’s no CSM Service invocation:
If you want, you could now send in a V2 flow request specifically and see that working as well.
Additional Notes
Consequence of Flow Versions & Configuration
In the previous tutorial we configured ActionTimeouts with the following configuration;
ipf.flow.Ipftutorialflow.CheckingFraud.CheckFraud.timeout-duration=2s
Here we used the flow name, Ipftutorialflow, and it worked just fine. However with the versioning of our flows this has actually created 2 flows of different names (with different behaviours) and this needs to be reflected in the config. For each flow you wish to retain this configuration, you should set it as follows (note the addition of V1 and V2 to the names):
ipf.flow.IpftutorialflowV1.CheckingFraud.CheckFraud.timeout-duration=2s
ipf.flow.IpftutorialflowV2.CheckingFraud.CheckFraud.timeout-duration=2s
You can thus have different configurations per flow.
Or if you wish to have the same for all flows you can use the 'Any' wildcard (which will apply the configuration for ALL flows for this CheckFraud action);
ipf.flow.Any.CheckingFraud.CheckFraud.timeout-duration=2s
Handling In-Flight Transactions
There could be cases where a transaction was started on V1 of the flow but is paused waiting on a response/instruction to a long-running process external to the flow. During this time you have deployed V2 of the flow but the in-flight transactions are still not in a terminal state.
This scenario is perfectly valid and when the old transactions on V1 flow resume they will continue on the flow which they started on, in this case the V1 flow. Any new transactions that have been initiated since the upgrade would run on the V2 flow.
It’s not possible for a transaction initiated using one version of a flow (e.g. V1) to be resumed using another (e.g. V2). You either need to resume the flow on the original (V1) flow or initiate a new flow on the new version (V2).
Conclusions
In this section we’ve:
versioned our flow
added a CSM Service call to our versioned (V2) flow
showed the default behaviour of versioning
DSL 7 - Handling Timeouts
DSL 9 - Using Subflows
------------------------------
DSL 9 - Using Subflows
Getting Started
The tutorial step uses the "add_version" solution of the project as it’s starting point.
If at anytime you want to see the solution to this step, this can be found on the "add_subflow" solution!
What is a Subflow?
A "Subflow" is a reusable section of a flow. It effectively has all the same features as a flow but is not expected to be standalone but instead is included inside of another flow.
A subflow can therefore be :
Used within many different flows.
Used multiple times within the same flow.
An often seen example of a subflow is a sanctions check. It may be that many different flows all use a sanctions check or indeed that a sanctions check may be needed at different parts of the flow (for example following a resumption on delay).
For this section, we’ll use the sanctions example to integrate a subflow into our flow.
Our sanctions subflow, will need to perform the following actions:
When the subflow begins we make a request to a sanctions system.
Then the sanctions system can either return:
Passed - all is good, flow can proceed.
Wait
Then on the wait condition, we should wait until the sanction systems sends us one of three further messages:
Passed
Blocked
Failed
DSL Set Up
Adding the Sanctions Domain
Before we start looking at the subflow itself, we first need to add an external domain that is capable of supporting our flow. So let’s go ahead and do that.
Firstly, we’ll create a new external domain for our sanctions sytem. This is just like we did in  (New  v2Flo  External Domain) except that the response needs to use an extra capability we’ve not fully discussed before. So let’s start by adding the basics:
A name of "Sanctions Domain"
A description of "A sample sanctions system"
Add a request with details:
A name of "Check Sanctions"
A description of "Sample sanctions call"
Add the customer credit transfer to business data
Now we consider the responses.  There are a number of different ways we could model the required responses using combinations of response codes and reason codes like we did in the fraud system. Here however, we will do it by modelling multiple responses to our single sanctions request.
The first response is the "passed" scenario, we’ll call this a "no hit" scenario. For this we just add a simple response like we have before:
A name of "Sanctions No Hit"
A description of "The sanctions check has passed"
All other fields we’ll leave as the default.
The second response is the one when the initial check does not immediately pass and we are asked to wait. You’ll call this a "hit" scenario.
The key difference between the "hit" and "no hit" scenarios are that:
in the no-hit scenario it completes the request - i.e. we’re not expecting any further information from the sanctions system.
in the hit scenario it does not complete the request, we’re still expecting a final result of our sanctions call.
So this time, for our no-hit scenario we need to set our "Completing" flag to false to tell the system that we will expect subsequent messages from the sanctions system in response to the initial request.
Let’s set this response up:
A name of "Sanctions Hit"
A description of "The sanctions system is checking the outcome"
The completing flag unchecked
All other fields we’ll leave as the default.
Finally we also need the result response when the sanctions system eventually sends us a response. You’ll call this the "Final Sanctions Response".  So let’s add this:
A name of "Sanctions Final Response"
A description of "The final result from the sanctions system"
For the response codes we’ll need to create a new response code set for our "Sanctions Final Response" in the "Response Code Library". This is just as in DSL 4 Using an External Domain. In our case we’ll have three response codes: False Hit, Block, Reject.
All other fields we’ll leave as the default.
Once we’ve put all this together our sanctions system definition should look like:
And for reference our new "Sanctions Final Response Codes" will look like:
Note that we can simply add our response codes to the existing library that we used for our account validation codes.
That’s our sanctions system all set up and ready to use.
Adding the Subflow
Now that we have our sanctions domain set up, let’s go and create our subflow. We do this by right clicking on our model and selecting New  v2Flo  Subflow.
This should create a new subflow page:
The first thing to note is how similar this is to the flow page. That’s because it is effectively a specialised type of flow! So using this page should feel very familiar.
Let’s setup our new subflow:
Let’s give our sanctions flow a name of "Sanctions Subflow" and a description of "An example subflow for sanctions".
The next thing to consider is our states we are going to need. From the requirements we can see that there are three states we will need:
A "Checking Sanctions" state for when we make the initial request to sanctions and are awaiting a response.
A "Waiting" state for when we have received a wait notification from sanctions and are awaiting the final response.
A "Complete" state for when we have successfully completely a sanctions check.
A "Rejected" state for when the sanctions check has failed.
A "Blocked" state for when the sanctions check has resulted in a blocked notification.
Go ahead and set these up now. Consider what values you will need for the terminal flag and global state on each of these states and when complete the solution is below:
It’s really important within a subflow to get the terminal states correct. That’s because those are the states that the subflow will be able to report back to the parent flow. So here we have "Complete", "Rejected" and "Blocked" as terminal states. That’s because the "Checking Sanctions" and "Awaiting Final Result" states are intermediary states during the subflow processing. We can further emphasize that by the setting of the "PENDING" global state.
Now let’s carry on down our subflow and the next thing to consider is the events. Again going back to requirements we can see that we will need 4 events:
"Sanctions Passed" for a successful sanctions check
"Sanctions Rejected" for a failed sanctions check
"Sanctions Blocked" for a blocked sanctions check
"Sanctions Hit" for an wait notification.
Note here there are many different event names we could use. We’ve chosen to use the same event "Sanctions Passed" for both the direct and indirect (via wait) passing of the sanctions check. We could just have easily created two events to uniquely identify each.
Let’s add these Event Definitions now and we should see:
Continuing on down our subflow our next section to consider is the Input Behaviour.  Here we have to consider each of our three different responses from the sanctions system and how we want to handle them. Try to do this yourself and the solution is below:
Next up, on initiation we need to call the sanctions system. So we’ll need a new state "Checking Sanctions" and an initiation behaviour that moves us to the "Checking Sanctions" state and calls our sanctions system. Try that now and the solution is below:
Finally we need to handle our event behaviour. See if you can work it out, and then the solution is below:
The one interesting point to note here is how we’ve handled line 4 in the event behaviour. It would have been just as correct to have two lines here, one for the "Checking Sanctions" current state and one for the "Awaiting Final Result" current state. But we’ve chosen here to use the ability of having multiple states defined in the current state. This is simply a shorthand way to avoid repeating the same logic multiple times if the outcome is no different.
We’ve now completed all our sections of the flow, but if we look there is still an error showing on the "Perform Action" of the initiation behaviour. Let’s investigate this by validating the flow (ALT + ENTER then validate flow). It tells us:
It’s telling us that the subflow doesn’t have access to a customer credit transfer and hence it can’t make the call out to the sanctions system. In our case, our customer credit transfer belongs to the parent flow. So to provide it down to the subflow we need to add it to the initiation data, let’s do that and we should now see:
And our error has been resolved. As normal before completing let’s have a look at our graph (Tools > Open Flow Viewer) to find:
That’s our subflow all setup and ready to use, so the next question is how do we apply this to our parent flow. Let’s go back to it, we’ll update only in our latest V2 version:
We’re going to add our subflow in as a new step after our account validation. Currently we have:
To slide in the extra step, we want the validating account to call our subflow (not run the fraud check decision) and then run the fraud check on successfully passing sanctions instead.
If you remember back to DSL 5 - Using a decision, we introduced the concept of a pseudo state and in that case the specific type of a "Decision State". Here we want to use a different type of state - a "Subflow State". Let’s start by adding it after we received the "Account Validation Passed" result.
So instead of moving to the "Run Fraud Check" decision, lets create our "Subflow State".
To do this we first need to delete the existing decision state and then select "Create Subflow State".
You’ll enter the name as "Sanctions".
Then in the "Perform Action" box we’ll delete the existing call to Fraud and then select "Call Subflow" and then choose the "Sanctions Subflow". Once done we should look like:
If you inspect the error now we’ll see:
So here we can see that the flow needs us to define how to handle the fact that the subflow has reached those Complete, Rejected or Blocked states. Why only those 3? Because those are the ones we specified as terminal with the sanctions subflow.
Let’s setup our handling of the subflow terminating as "Event Behaviour" in the main V2 flow. To do this we start by adding a new Event Behaviour with current state of "In Sanctions". Then in the "For Event" we select "Subflow Completion" and then we’ll start by selecting "Complete".
In our case on receipt of complete from the sanctions flow we’ll need to create our "Run Fraud Check" decision and fire the fraud check decision as we had previously (note - until this is done "Run Fraud Check" shows as an 'Unresolved Reference'):
Now let’s do the same to handle the outcome from Rejected and Blocked, for now we’ll just send both to rejected.
Again, note here how we’ve used the "Any Of" capability to minimise the entry.
Finally, let’s look at how this has impacted our flow diagram (Tools > Open FlowViewer):
Here we can see that our subflow call has been represented by box. The box contain all of the subflow logic to minimise the complexity of the graph. However, if you look at the top of the graph there’s a new option "Expand Subflow". Let’s click this and apply to see:
And in this view we can see the subflow being expanded out so we can see the inner workings of the flow.
That’s all our DSL setup done. Now let’s move onto the implementation side.
Java Implementation
Defining the Adapter
Let’s switch to Intellij to work with the java side.
Firstly, let’s rebuild our project so that we can generate our new flow. To do this open a new terminal and run:
mvn clean install
There is nothing special about the subflow itself from a generation viewpoint, the changes we have to consider are simply that we have defined a new External Domain (Sanctions Domain) that we now have to implement. You’ll again here choose to just use the sample app’s implementation as we did in DSL 4 - Using an external domain, so try and add that now and solution is below when ready:
@Bean
public IpftutorialmodelDomain init(ActorSystem actorSystem, SchedulerPort schedulerAdapter) {
    // All adapters should be added to the domain model
    return new IpftutorialmodelDomain.Builder(actorSystem)
             .withTutorialDomainFunctionLibraryAdapter(input -> CompletableFuture.completedStage(new DuplicateCheckResponseInput.Builder(input.getId(), AcceptOrRejectCodes.Accepted).build()))
             .withAccountingSystemActionAdapter(new SampleAccountingSystemActionAdapter())
             .withFraudSystemActionAdapter(new FraudSystemActionAdapter())
             .withDecisionLibraryAdapter(input ->
                    input.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getIntrBkSttlmAmt().getValue().compareTo(BigDecimal.TEN)>0 ?
                            RunFraudCheckDecisionOutcomes.FRAUDREQUIRED : RunFraudCheckDecisionOutcomes.SKIPFRAUD)
             .withIpftutorialflowV1AggregateFunctionAdapter(input -> new ExtractCreditorAccountForFlowIpftutorialflowV1AggregateFunctionOutput(input.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getCdtrAcct()))
             .withIpftutorialflowV2AggregateFunctionAdapter(input -> new ExtractCreditorAccountForFlowIpftutorialflowV2AggregateFunctionOutput(input.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getCdtrAcct()))
             .withSchedulerAdapter(schedulerAdapter)
             .withCSMServiceActionAdapter(new SampleCSMServiceActionAdapter())
             .withSanctionsDomainActionAdapter(new SampleSanctionsDomainActionAdapter())
             .build();
}
Here we have added into our config the addition of the sanctions system specification.
Checking our Solution
As normal let’s now check out solution works. Start up the application as previously (instructions are available in Reviewing the initial application if you need a refresher!)
For payments, we’ll just fire in a standard one:
curl -X POST localhost:8080/submit | jq
Then as normal, if we now bring up the payment in the Developer GUI and bring up the flow graph (search by unit of work id, click view, click view graph) and we should see:
And we can see that our subflow has been expanded out to be part of the main running flow which has completed successfully. If we look at the events instead (click domain events):
We should note that the event name is the combination of both the prefix we provided in our pseudo state together with the actual name of the event in the subflow. This is important to realise as it is this capability that allows us to use our subflow in multiple places across our flow.  You can try this yourself if you want by adding a second sanctions subflow call into the flow!
Conclusions
In this section we’ve learnt how to create a subflow and invoke it from within our flow.
Having considered subflows and how to use them, let’s now turn our attention to other flows and how to call one flow from another in: DSL 10 - Calling other flows (Part One - Within a model)
DSL 8 - Versioning
DSL 10 - Calling other flows (Part One - Within a model)
------------------------------
DSL 10 - Calling Other Flows (Part One - Within a Model)
Getting Started
The tutorial step uses the add_subflow solution of the project as it’s starting point.
If at anytime you want to see the solution to this step, this can be found on the add_flow solution
Scenario
So far to date we have only considered a single flow, which takes in a pacs.008 and performs some actions on it. We could consider this to be an "execution" flow. Let’s now consider how we may do "initiation" and create a second flow that will:
Receive a pain.001
Convert the pain.001 to a pacs.008
Forward the pacs.008 to the execution flow
Determine the outcome of the execution flow and terminate appropriately.
So the key thing to consider here is how we communicate flow to flow. In this step we will only look at flows that are deployed together - namely when two flows are both defined in the same model. We will look at the alternative scenario in part two of calling other flows.
Whilst this section discusses how to use the DSL itself to setup flow to flow, it is important to realise that in essence flow to flow is effectively calling from one domain to another and as such it would be just as valid (and possible) to simply model the flow interaction using external domains.
DSL Setup
Adding the Initiation Flow
Let’s add a new flow by right clicking on the model and selecting New  v2Flo  Flow. Then in the flow edit we’ll set:
A name of "InitiationFlow"
A description of "Initiates payment processing"
A new state to represent flow rejection called "Rejected". This should have a REJECTED global code.
When done we should look like:
Now let’s move down to the initiation behaviour,  as per our requirement we need to receive a pain001. The first thing we’ll do here is use the pre-packaged initiation types. To do these we have to import them into our model.
You’ll do this by right clicking on our model and clicking "Model Properties" (or just using the ALT+ENTER shortcut). This will bring up the model properties view:
From here we’ll click the "+" and then enter "ISOInitiationTypes" (note that as you type it will remove non matching options so you should be able to select it before entering the whole string) and click ok.
Now if we go to our Initiation Behaviour on the new flow, we should have the option to select "Payment Initiation" so let’s do that.
If you can’t find a business data element that you are interested in, instead of adding the model like we did above, from the selection box press "CTRL + R". Then click the box for "include stub and non project models" (or press CTRL+R) again and you should be able to search for your business data library that contains the element you want.
Integrating the Flows
Now let’s integrate the two flows. To do this, we need to use another pseudo state just like in DSL 5 Using a decision. In this case it’s a "Flow State". So in the "Move To State" box, select "Create Flow State".
Let’s call our Flow State "Call Execution".
Then in the perform action state, we’ll select "Call Flow" and then select our ipftutorial flow. When done it should look like this:
Now we can see that the calling of the flow is underlined as an error. If you validate the flow (ALT + ENTER then Validate Flow) we’ll find:
Let’s work through each of these, firstly "Called Flow requires missing data: Customer Credit Transfer". This is because our execution flow needs to be given a credit transfer object, but at the moment the initiation flow doesn’t have one to give. To fix this, we’ll create a new aggregate function to map from the pain.001 to the pacs.008. If you need a reminder of aggregate functions, then review DSL 6 - Using an aggregate function. Try and add this new aggregate function now and when ready, the solution is below:
If we re-validate our flow, then we should no longer see the data error.
The state issues are due to the fact we’re not handling the output from Ipftutorialflow. Let’s go back to our initiation flow and add our Event Behaviour to fix this.
You’ll start by adding the behaviour when we’re in our "Call Execution" state. The event is the special bit here we need to select a special "On Flow State" event, and then select the "Complete" state from the execution flow. Finally, we’ll move to the "Complete" state from the initiation flow. Putting that together we have:
Now we can do the same for the Rejected and Timed Out state. For now, we’ll just send both to Rejected.
Viewing the Flows
That’s all our DSL changes complete, but before moving on let’s consider the graph for our new initiation flow. As normal, let’s open the Flow Viewer (Tools > Open Flow Viewer`):
Here we can see that our child flow is represented by the green box for the call to the ipf tutorial flow. Note that unlike with the subflow, we cannot expand the execution block as that is a separate flow and is not considered an embedded part of this flow.
That’s all our DSL work done, and we have completed the setup of the flow.
Java Implementation
Let’s now switch back to Intellij and look at how we plug this into our implementation code. As normal we’ll start by running a build from a terminal window:
mvn clean install
This will successfully generate all of our DSL related components.
Previously we were firing our requests directly into the ipf tutorial flow, whereas now we want to call the initiation flow. So let’s change "InitiateIpftutorialflowInput" to "InitiateInitiationflowInput" (in the ipftutorialsolution/app/controller/InitiationController.java). In doing this, the expect type of data supplied changes from the customer credit transfer to the payment initiation object. So we’ll also need to change that line,  for now we can use the Pain001Generator from within the ifptutorial-app.
Try and do this now and the solution is below:
var samplePain001 = Pain001Generator.generate();
 if (request != null && request.getValue() != null) {
    samplePain001.getPmtInf().get(0).getCdtTrfTxInf().get(0).getAmt().getInstdAmt().setValue(request.getValue());
}
return Mono.fromCompletionStage(IpftutorialmodelDomain.initiation().handle(new InitiateInitiationFlowInput.Builder(entityId)
        .withProcessingContext(ProcessingContext.builder()
                .unitOfWorkId(unitOfWorkId)
                .clientRequestId(clientRequestId)
                .build())
        .withPaymentInitiation(samplePain001)
        .build()).thenApply(done -> InitiationResponse.builder().requestId(clientRequestId).uowId(unitOfWorkId).aggregateId(done.getAggregateId()).build()));
Make sure you remember to clean up the imports (CTRL+SHIFT+O or/ CTRL+ALT+O on Windows).
Next we have to add the aggregate function for the initiation flow, now let’s revisit what we were trying to achieve with this function. We need something to map from the pain.001 to the pacs.008.
Here you could use any approach required to perform the mapping. In our case, we’re going to use a pre-built library that provides us with a pain.001 to pacs.008 mapping. So let’s add that now.
First we need to add the dependency to the pom.xml within the ipf-tutorial-app module (ipf-tutorial/ipf-tutorial-app/pom.xml).:
<dependency>
    <groupId>com.iconsolutions.ipf.core.mapper</groupId>
    <artifactId>mapping-library-spring</artifactId>
</dependency>
This will load in a Spring based implementation of various ISO to ISO mappings (). Have a look at the IsoMappingService class that has been brought in and you can see it has a method to apply a mapping from pain.001 to pacs.008:
public FIToFICustomerCreditTransfer mapPain001toPacs008(CustomerCreditTransferInitiation initiation) {
    return (FIToFICustomerCreditTransfer)this.transformationService.mapThenEnrichWithDefault(initiation, FIToFICustomerCreditTransfer.class);
}
Here we can see that it is calling a transformation service to apply the mapping. This is using Icon’s "Mapping Framework" to perform the mapping. The mapping framework can be used to build your own custom mappings, and this will be covered later in this series. Note that this implementation assumes as 1:1 ratio between pain.001 and pacs.008.
As we have chosen to use the spring based implementation, an instance of the IsoMappingService will be automatically injected into the spring context for us, so we simply need to add it to our tutorial class and then use it to provide the implementation of our aggregate function.
Try and add this as now just as we did for the tutorial flow’s aggregate function port before. When ready, the solution is below.
@Bean
public IpftutorialmodelDomain init(ActorSystem actorSystem, IsoMappingService mappingService, SchedulerPort schedulerAdapter) {
    // All adapters should be added to the domain model
    return new IpftutorialmodelDomain.Builder(actorSystem)
            .withTutorialDomainFunctionLibraryAdapter(input -> CompletableFuture.completedStage(new DuplicateCheckResponseInput.Builder(input.getId(), AcceptOrRejectCodes.Accepted).build()))
            .withAccountingSystemActionAdapter(new SampleAccountingSystemActionAdapter())
            .withFraudSystemActionAdapter(new FraudSystemActionAdapter())
            .withDecisionLibraryAdapter(input ->
                    input.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getIntrBkSttlmAmt().getValue().compareTo(BigDecimal.TEN)>0 ?
                            RunFraudCheckDecisionOutcomes.FRAUDREQUIRED : RunFraudCheckDecisionOutcomes.SKIPFRAUD)
            .withIpftutorialflowV1AggregateFunctionAdapter(input -> new ExtractCreditorAccountForFlowIpftutorialflowV1AggregateFunctionOutput(input.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getCdtrAcct()))
            .withIpftutorialflowV2AggregateFunctionAdapter(input -> new ExtractCreditorAccountForFlowIpftutorialflowV2AggregateFunctionOutput(input.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getCdtrAcct()))
            .withInitiationFlowAggregateFunctionAdapter(parameters -> new MapPain001ToPacs008ForFlowInitiationFlowAggregateFunctionOutput(mappingService.mapPain001toPacs008(parameters.getPaymentInitiation())))
            .withSchedulerAdapter(schedulerAdapter)
            .withCSMServiceActionAdapter(new SampleCSMServiceActionAdapter())
            .withSanctionsSystemActionAdapter(new SampleSanctionsSystemActionAdapter())
            .build();
}
That’s it, now when our flow calls the aggregate function it will be returned a fully converted pacs.008.
Checking our solution
As normal let’s now check out solution works. Start up the application as previously (instructions are available in Reviewing the initial application if you need a refresher!)
And we fire in a payment:
curl -X POST localhost:8080/submit | jq
And this time we’ll notice our aggregate id has changed:
{
  "requestId": "1a53c51c-c96e-4786-9f3f-d0d91f80b973",
  "uowId": "ba0b5c6c-855b-41ef-98a7-9b0ee121e6da",
  "aggregateId": "InitiationFlow|176a4e23-6299-49be-89bf-891ca12740de"
}
We can see here that it’s the initiation flow that’s being hit
Then as normal, if we now bring up the payment in the Developer GUI and bring up the flow view (search by unit of work id, click view) and we should see:
This is a really important moment to understand. We’ve brought back here all the flows associated to our unit of work id. And in doing so we’re now seeing two flows, rather than just our one.
Firstly we have the initiation flow, let’s look at its graph:
So we can see here that it’s calling the execution flow AND receiving the completion response back from it. So our main execution flow must have completed correctly. We can validate that by clicking on the graph for the ipftutorialv2 flow and see:
So as we expected it has completed successfully too.
Its also worth just checking the "Domain Events" where you can observe the Events and the Process Flow from which those events originated.
Conclusions
In this section we have successfully:
Created an initiation flow
Added the integration between the initiation and integration flow
Implemented the new adapters
Deployed and tested the application
Seen that the two flows are linked via the unit of work id.
Note that we have restricted this for now to flow’s within the same model, we’ll later look at how to do cross-model flows.
DSL 9 - Using Subflows
DSL 11 - Using additional events
------------------------------
DSL 11 - Using additional events
Getting Started
The tutorial step uses the add_flow solution of the project as it’s starting point.
If at anytime you want to see the solution to this step, this can be found on the raise_additional_event solution!
What is an Additional Event?
An Additional Event is simply an ability to generate an event from within the IPF processing. This is usually used as a method of adding descriptive information into the event history, but can also be used for performing conditional processing logic. In fact all the configuration we set up in could have been performed by using additional events.
In our scenario, when you look at the event history for an execution flow you can see:
[
  "Flow Initiated",
  "Duplicate Check Passed",
  "Account Validation Passed",
  "In Sanctions Sanctions Passed",
  "Run Fraud Check SKIP_FRAUD",
  "Clear and Settle Passed"
]
Here it is not immediately obvious that our flow has completed, it appears that we have only reached a clear and settle passed stage.
So to resolve this we will make the application raise an additional event to clearly show from the event history that we have completed.
DSL Setup
Adding the additional event
Firstly, let’s remind ourselves of the DSL logic we currently have for our Clear and Settle Passed event, we see this in the Event Behaviour:
Here instead of immediately completing, we want to raise our additional event. However, once we reach a terminal state the flow has finished and no other actions are allowed. For that reason, we’ll need a new State and a new state transition.
Let’s add a Completing state like this:
You’ll also need to define our additional Event Definition, call this Flow Complete and it gets added to the event definition of our flow just like anything else:
Note here we haven’t supplied any business data on the event, but we could do that. That data would then be populated from data contained from other events in our flow.
To actually raise our additional event we need to change our Event Behaviour (for Cleared And Settling) to move to the new Completing state (rather than Complete) and then raising an additional event. So here in the perform action box we need to choose to raise an additional event:
Then we can just select the event we want to raise, so in our case our new Flow Complete event.
When complete the Event Behaviour should look like this:
Finally, we need to transition to the Complete step again. This is simply a case of transition from Completing to Complete on receipt of our new Flow Complete event. So let’s add a new event behaviour to do this:
That’s it, our DSL work is complete. Let’s also look at the graph:
Java Implementation
The good news here is that there is no implementation required when adding additional events, it is all dealt with by the generation.
Checking our solution
As normal let’s now check out solution works. Start up the application as previously (instructions are available in Reviewing the initial application if you need a refresher!)
Then we can fire in a payment
curl -X POST localhost:8080/submit | jq
Now if we now bring up the payment in the Developer GUI  and bring up the domain events view (search by unit of work id, click view, click domain events) and we should see:
Here we can see that there is the new Flow Complete event present for the ipftutorialflowv2 process and hence we’ve shown that our additional event is correctly being fired.
If you look at the graph for the ipftutorialv2 flow you can see the extra event and transition:
Conclusions
In this section we’ve learnt how to use additional events.
DSL 10 - Calling other flows (Part One - Within a model)
DSL 12 - Using custom business data
------------------------------
DSL 12 - Using custom business data
Getting Started
The tutorial step uses the "raise_additional_event" solution of the project as it’s starting point.
If at anytime you want to see the solution to this step, this can be found on the "add_custom_types" solution!
What is custom business data?
Here by custom business data we mean the types of data we can create in our business data libraries. If you remember, the current library we set up we had:
So here we were using the predefined "CashAccount" data type that comes from the Icon data model. What if we wanted to use our own bespoke types?
To demonstrate this, we’re going to create a new bean and add this to a new business data element to show how it can be used within the DSL.
DSL Setup
Defining the bean
First let’s create our custom bean - this is going to be placed in the domain-root/external-libraries project. You’ll create a bean as follows:
package com.iconsolutions.ipf.tutorial.external;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import java.io.Serializable;
@Data
@AllArgsConstructor
@NoArgsConstructor
@Builder
public class IpfTutorialBean implements Serializable {
    private String stringField;
    private int intField;
}
So that’s our simple object, note we’re using lombok here as a shorthand to setting up the appropriate class characteristics. You can add the dependency for lombok by adding it to the pom.xml:
<dependency>
    <groupId>org.projectlombok</groupId>
    <artifactId>lombok</artifactId>
</dependency>
Important!
A key restriction here is that as the data points supplied need to be stored on the events, they MUST be serializable. Any class that isn’t won’t be available in the dropdown!
Now that we’ve constructed our class, we need to make sure it’s been built and added to the mps setup, to do this we simply need to rebuild the project:
mvn clean install
Adding the custom data
Let’s add a new business data element to our existing Business Data Library. You’ll give it a name of "Example Data" and a description of "Simple example data".
Now in the type select try and choose "IpfTutorialBean". You should find it’s not present.
To make it available, we simply press CTRL+R whilst on the selection box to bring up the search input. We then tick the checkbox at the top:
Note pressing CTRL+R for the second time will tick the checkbox too!
Top Tip
If you’re having trouble seeing your new class, try File  Invalidate Caches and then try again.
Search for the "IpfTutorialBean" and double click to add.
Then simply type "IpfTutorialBean" in the Data Type and should now be able to see and select our object. Once done, our business data element should look like:
For the purposes of testing, let’s add our sample data point to our the Initiation Behaviour of our InitiationFlow:
Finally it’s good to look in the project view, if we scroll to the bottom of our model we can see:
So here it show’s us all the classes that have been included in this way.
That’s us done, we’ve added a custom business data element and used it to sent it in on initiation.
Java Implementation
Updating the initiation
To test this, we’ll simply add the new object into our initiation call. We will need to add the data point to our initiation controller, first we’ll need to rebuild to pick up our new changes:
mvn clean install
And then once built we update our initiation controller to send in a new bean:
return Mono.fromCompletionStage(IpftutorialmodelDomain.initiation().handle(new InitiateInitiationFlowInput.Builder(entityId)
        .withProcessingContext(ProcessingContext.builder()
                .unitOfWorkId(unitOfWorkId)
                .clientRequestId(clientRequestId)
                .build())
        .withPaymentInitiation(samplePain001)
        .withExampleData(IpfTutorialBean.builder().intField(7).stringField("Test").build())
        .build()).thenApply(done -> InitiationResponse.builder().requestId(clientRequestId).uowId(unitOfWorkId).aggregateId(done.getAggregateId()).build()));
Checking our solution
As normal let’s now check out solution works. Start up the application as previously (instructions are available in Reviewing the initial application if you need a refresher!)
Then as normal we can send in a payment
curl -X POST localhost:8080/submit | jq
This time let’s look at the different events that have been received. If we now bring up the payment in the Developer GUI and bring up the flow view (search by unit of work id, click view, click domain events) and we should see the normal set of Domain Events. However this time if we click to see the body of the initiation flow’s "Flow Initiated" event we’ll see that it now has the new example data object available at the bottom of the event definition:
Extending beyond simple classes
Adding your own libraries
We can also add in any Maven dependency we want to give access to the beans within it. This can be done simply by adding the dependency into the external-library dependencies pom. Go ahead and give this a go for a library of your choice and see if you can make it appear for you in MPS!
Branching beyond the "external-libraries" module.
The original tutorial project supplied us with a module called "Extended libraries" (under domain-root), this module is responsible for bringing in external dependencies. When you create your own project’s later using the archetype this folder will still be built for you. The key here is that this module uses Maven shading to collapse itself and any dependencies, so that MPS can find the files on the class path. We can see this on the pom of our module:
<!-- Shade everything, Maven still is the source of truth for the versions, but we can get a
     Smoother MPS experience if it "sees" a single library of dependencies
-->
<build>
  <plugins>
    <plugin>
      <groupId>org.apache.maven.plugins</groupId>
      <artifactId>maven-shade-plugin</artifactId>
      <version>3.2.4</version>
      <configuration>
        <createDependencyReducedPom>false</createDependencyReducedPom>
        <shadedArtifactAttached>true</shadedArtifactAttached>
        <shadedClassifierName>shaded</shadedClassifierName>
      </configuration>
      <executions>
        <execution>
          <phase>package</phase>
          <goals>
            <goal>shade</goal>
          </goals>
        </execution>
      </executions>
    </plugin>
  </plugins>
</build>
Then the external library is referenced in the MPS model for the project so it can be looked up. We can do this to any module we wish.
Conclusions
In this section we looked at:
Creating our own java types and importing them as custom business data elements.
Reusing other external java types by importing them through Maven dependencies.
Now having configured looked at reuse of java types, let’s look at the next stage of reuse - DSL 13 - Using shared concepts (Part One - Within a Solution)
DSL 11 - Using additional events
DSL 13 - Using shared concepts (Part One - Within a Solution)
------------------------------
DSL 13 - Using shared concepts (Part One - Within a Solution)
Getting Started
The tutorial step uses the "add_custom_types" solution of the project as it’s starting point.
If at anytime you want to see the solution to this step, this can be found on the "shared_models_one" solution!
What is a shared concept?
In DSL 9 - Using Subflows we looked at setting up a subflow as a way of sharing logic. By creating the subflow, we allowed ourselves to use it in several places within our flow. However, what if we want to go one step further and reuse a subflow not just within the flows within our model, but within other models. This is where shared concepts come into play. It allows us to create modules that are themselves re-usable components that can be pulled into different projects.
You’ll do this by extracting the subflow out of our existing model and moving it into it’s own sanctions model and then reuse it within the origin tutorial model.
DSL Setup
Extracting the subflow logic
First of all we’ll need a brand new model in which to house our sanctions logic. To do this we select the solution in the project navigator and then right click and select New > Model. You’ll call our model com.icon.sanctions.
Press ok to create our model.
We should then be prompted to select our dependencies:
For now all we want to add is our actual flo-lang language. So let’s click on the "Used Languages" tab, then select the "+" symbol and add our v2Flo language:
After selecting it we should see:
That’s our model all set up so press "OK" to create and you should now see a second model appear in the navigator:
Let’s now think about what makes up our Sanctions logic, we have:
Sanctions Subflow
Sanctions System
Sanctions Response Codes
First let’s move our response codes. For this, we’ll first need to create a new response code library to house the sanctions codes within our new 'sanctions' model. For that, we simply right click on the Sanctions model and then go New  v2Flo  Response Code Library.
Now we need to move our sanctions response codes into our new library. So let’s open our response code library within the main tutorial model (ipftutorialmodel) and then click on the sanctions response codes and press F6  (or alternatively right click and select Refactor > Move Nodes) and you should get the move nodes popup:
Here it’s showing us out sanctions system is in the ipftutorialmodel. However we now want it in our sanctions model so we simply expand the sanctions model, select our new response code library and click "Refactor", and again "Refactor" on the pop-up.
Then we’ll confirm the refactor on the review screen and we should see the refactoring view appear:
Here we simply tell MPS to do the refactor and we should see our sanctions response codes now sitting in the our new library:
That’s it we’ve refactored our response codes into a different model.
Let’s now repeat the process to move the sanctions subflow into our sanctions model too.  For these as they live as top level elements on their own (i.e. they don’t exist within a library) we can simply move them directly. Do this now and then in the navigator we should see:
And we have successfully refactored all of our sanctions components into the new model.
The key thing to understand here is that the sanctions logic now exists in a different model, but that model has been shared with the current model as a dependency.  MPS did this for us when we did the refactor, but it is perfectly possible to do this manually too. If we click on the ipftutorialmodel in the navigator and press "ALT+ENTER" (or right click then "Model Properties" we’ll find:
If you note here, the bottom entry shows our sanctions model. If we wanted to we could remove it (using the - sign) and then re-add it (using the +) and searching for our model name.  It’s this ability to share a model with a different model that allows us to provide reusable components across many flows and solutions.
Top Tip
If you have a model that you know is only going to provide supporting models (i.e. it does not contain any flows) then you can slightly speed up the generation of your solution by clicking on the model, pressing ALT + ENTER and then going to the "Advanced" tab before checking the "Do Not Generate" box. It won’t cause issue’s if you don’t though!
That’s it from a DSL perspective.
Java Implementation
The good news here is that there is nothing we have to do from an implementation viewpoint to adapt to our newly refactored model. So hence we could just check the flow just like we have in previous steps.
Checking our solution
As normal let’s now check out solution works. Start up the application as previously (instructions are available in Reviewing the initial application if you need a refresher!)
And then we could send in a payment:
curl -X POST localhost:8080/submit | jq
And if we bring up the payment in the Developer GUI and look at the graph of our tutorial flow (search by unit of work id, click view, click ipf tutorial flow, click view graph) then we see:
If we compare this to the graph of DSL 9 - Using Subflows, we can see that everything is the same as it was and we have successfully extracted out our subflow to a different model.
Next up, we’ll take this a step further and look at moving our subflow into a different solution in: DSL 14 - Using shared concepts (Part Two - Across Solutions)
DSL 12 - Using custom business data
DSL 14 - Using shared concepts (Part Two - Across Solutions)
------------------------------
DSL 14 - Using shared concepts (Part Two - Across Solutions)
Getting Started
The tutorial step uses the "shared_models_one" solution of the project as it’s starting point.
If at anytime you want to see the solution to this step, this can be found on the "shared_models_two" solution!
A Quick Recap
In DSL 13 - Using shared concepts (Part One - Within a Solution), we extracted our sanctions logic into a separate model and imported that model so that our flow was still able to work. In doing so we enabled the sanctions logic to be reused across multiple models.  However, the limiting factor at the point was that we were doing this within a single overall solution.
This section looks at how we’d do this when we want to use the sanctions code across many different IPF applications (in separate 'Solutions'). We can consider this as per the following diagram:
To do this we’re going to swap out our current sanctions implementation that we’ve created, with a different sanctions implementation.
The sanctions implementation we’re going to use is available here:
git clone bitbucket.iconsolutions.com/scm/ipfv/ipf-tutorial-sanctions.git]
DSL Setup
Creating a New Solution
First of all lets create a new solution, to do this we right click on our project and choose New > Solution
Then we enter the name as "Sanctions".
We should now see the sanctions solution appear alongside our original solution in the navigator.
Next we’re going to move our Sanctions model from the main tutorial solution to the new Sanctions solution. To do this, we select the Sanctions Model in the navigator pane and the press "F6" (or right clickt then Refactor  Move Model) to see:
And here we simply select our new Sanctions solution and confirm the refactor.
Then we should be able to see our Sanctions model has moved over to the new solution.
This is largely so far just as we did on the previous step when looking at in-model reuse. However there is one key extra step we need to take here to tell the build that it now needs to include both models. To do this, we need to open the build solution ("Ipftutorialsolution"):
And then you should see it highlighting an error:
To resolve this we simply need to tell it to build our new solution as well. To do that selection the end of the 'solution' line, press return and you should see a new entry setup appear (in the Ipftutorialsolution/Ipftutorialsolution.msd):
Select the type as "solution" to see:
Here we can enter the name for the entry, we’ll call it "Sanctions" and enter the load path which is the path to the underlying mps solution file. We can do this by entering a / after the "load from ." and then navigating down our path. It will be similar to the original one above and go down to the Sanctions.msd file. Once complete it should look like:
If the red underlines appear, we may need to reload the solutions from disk to clean up. To do that, simply press "ALT+ENTER" on the underlined error and you should see:
And now press "Reload Modules From Disk". Once that’s done we can regenerate the build module (right click in project view > regenerate).
That’s all our DSL work done, we have now extracted our sanctions logic into a new solution.
Java Implementation
As with the last step, there is no java implementation work to do as MPS has taken care of everything for us.
Checking our Solution
As normal let’s now check out solution works. Start up the application as previously (instructions are available in Reviewing the initial application if you need a refresher!)
And then we could send in a payment:
curl -X POST localhost:8080/submit | jq
And if we bring up the payment in the Developer GUI and look at the graph of our tutorial flow (search by unit of work id, click view, click ipf tutorial flow, click view graph) then we see:
If we compare this to the graph of DSL 9 - Using Subflows, we can see that everything is the same as it was and we have successfully extracted out our subflow to a different Solution now.
Conclusions
In this section we have learnt how to extract components into a separate reusable Solution (within the same project) and to include the model (from the new Solution) within our main solution.
DSL 13 - Using shared concepts (Part One - Within a Solution)
DSL 15 - Using shared concepts (Part Three - remote models)
------------------------------
DSL 15 - Using shared concepts (Part Three - remote models)
Getting Started
The tutorial step uses the "shared_models_two" solution of the project as it’s starting point.
If at anytime you want to see the solution to this step, this can be found on the "shared_models_three" solution!
A Quick Recap
In DSL 14 - Using shared concepts (Part Two - Across Solutions), we extracted our sanctions logic into a separate solution and imported that so that our flow was still able to work. In doing so we enabled the sanctions logic to be reused across multiple solutions.
In practice however most projects will be set up as a service providing a single solution. Therefore, we now need to look at being able to import models that are contained in separate reusable solutions.
Dependencies
Before we start, we need to clone a new application. This is available here: IPF Tutorial Sanctions, to clone it:
Getting Sanctions Module
git clone ssh://git@bitbucket.iconsolutions.com:7999/ipfv/ipf-tutorial-sanctions.git
Once we’ve cloned the repository, let’s build it:
mvn clean install
Now let’s open our sanctions project in Intellij and have a look at what’s inside, it’s a little different to our main tutorial project.
So here all we have is our domain-root project. That’s because all we’re going to use this project for is to provide a reusable MPS project, we’re not going to build flows or implement any generated interfaces.
Now let’s open it in MPS, we’ll see that it contains our sanctions model that we created in our previous steps. Now we’re going to use it to replace our existing sanctions model and used this shared component.
How the Sanctions Module was Built
This module was built using the IPF Archetype with the following property set:
-DincludeApplication=n
This generates the project as a reusable MPS solution as described above.
Importantly the MPS module pom.xml enables the zipping of the files for import into any downstream projects:
The build script also looks a little different to a non-reusable module but this is all generated for you, and you should not need to modify this if using the IPF Archetype.
Importing the Module
The importing of the module is done within the plugin sections of the pom within our application.
Let’s open our main tutorial project in Intellij and then if you open the pom of the mps folder (domain-root/mps/pom.xml) we’ll see there already exists an entry for the maven-dependency plugin as follows:
<build>
  <plugins>
    <plugin>
      <groupId>org.apache.maven.plugins</groupId>
      <artifactId>maven-dependency-plugin</artifactId>
      <version>3.1.2</version>
      <executions>
          <execution>
              <id>copy</id>
....
Now all we need to do is add a section to unpack the sanctions mps module into our application. To do this we add a new execution as follows:
<build>
  <plugins>
    <plugin>
      <groupId>org.apache.maven.plugins</groupId>
      <artifactId>maven-dependency-plugin</artifactId>
      <version>3.1.2</version>
      <executions>
          <execution>
              <id>unpack-sanctions-plugin</id>
              <phase>initialize</phase>
              <goals>
                  <goal>unpack</goal>
              </goals>
              <configuration>
                  <artifactItems>
                      <artifactItem>
                          <groupId>com.iconsolutions.ipf.tutorial.sanctions.domain</groupId>
                          <artifactId>Sanctionsreuse</artifactId>
                          <version>1.0.4</version>
                          <type>zip</type>
                          <overWrite>true</overWrite>
                          <outputDirectory>${plugin_home}</outputDirectory>
                      </artifactItem>
                  </artifactItems>
              </configuration>
          </execution>
        <execution>
          <id>copy</id>
The key bits to notice here are the groupId, artifactId and version - it’s just like any other Maven dependency (note that the artifactId here is the solution name).
That’s all we need to add, if we wanted to add more solutions, we’d simply add more execute blocks ensuring that for each one we have entered the correct groupId, artifactId and version just as we would for a normal Maven dependency.
Now we’ve added that we’re at just the same stage as we would have been if we’d taken the "shared_concepts_three_initial" solution and skipped this part of the tutorial.
Using the Remote Model
Now that we’ve configured our application to import the model let’s use it. First we’ll rebuild the application to make sure the module is correctly pulled in.
mvn clean install
Once complete, open MPS. The first thing we’ll do is delete the existing sanctions solution. To do this, we simply click on it in the explorer and press delete. A delete confirmation box should appear:
Click the delete files button as we won’t need the underlying files any more and then press "Delete".
You should now see errors appear in the navigator like this:
Updating Model Properties
This is because that main solution holds a reference to the deleted solution that it doesn’t know what to do with. We will now replace this with a reference to our new solution. To do this this let’s start by clicking on the model and pressing "ALT+ENTER" (or right click followed by  "Model Properties". This should bring up the model properties:
We can see here the old sanctions model is highlighted in red, to remove it simply click on it and press the "-" symbol. You’ll receive a warning:
This is because there are still elements in the flow that are trying to connect to the sanctions model, that’s fine we’ll sort those in a minute and so it’s safe to click delete anyway (if you wanted, you could click "View Usages" and review those).
Now we need to add the new sanctions model so we press the "+" symbol and type sanctions into the box:
We should see our "Sanctionsreuse" model available, so let’s select this and press "OK". The new sanctions model will be added and for now it will be red too, press "Apply" and the red will disappear. Instead it will be a feint grey which means it’s imported but not currently used - that’s because we need to switch out the calls to the old model with the new one. You’ll do that next, so click "OK" to finish.
Now we only have an error showing on the solution level, so let’s click on our solution (Ipftutorialsolution) and again press ALT+ENTER to see the properties. You’ll see here that we now have both sanctions solutions imported:
That’s because adding the new one at model level has automatically associated it at solution level, so here we simply have to select our old Sanctions solution and press the "-" symbol to remove. And Apply.
Updating the Flow
Now that we’ve set it up to be able to reference our new sanctions model, we need to update our flow to use it. This is really simple, just click on the flow to bring it up in the editor and MPS will automatically do the rest for you!
Once done, right click on the solution in the navigator and rebuild it and everything should work fine.
Cleaning the Build Scripts
We also need to do a bit of build clean up, to both remove the old model and provide the references to the new one when we run our build through Maven.
Firstly, in DSL 14 - Using shared concepts (Part Two - Across Solutions) we added our sanctions solution to the build script so we now have to remove that. To do this we simply open the build project:
And then select the sanctions section and press delete to remove it.
You’ll see at this point we still have a red error on our Ipftutorialsolution in the build script. So to fix this we need to add the dependency into the build for our new module. So in the dependencies section add a new line by pressing return:
Let’s press CTRL+R to import the new sanctions module (this makes it available for us to add as a dependency):
In our new dependency line we can now add Sanctionsresue by selecting the Sanctionsreuse dependency. Note we will have to fill in the location section too, this can be selected and will be $plugin_home/Sanctionsreuse.
Once complete we should look like:
Finally, we simply need to select our red underlined Ipftutorialsolution and press ALT+ENTER then reload all from disk and the red line will disappear.
Once done, right click on the build solution in the navigator and rebuild it and everything should work fine.
Java Implementation
The hard work in this part of our tutorial series was all done above in the MPS dependencies and build set up. From a java implementation viewpoint there is nothing extra for us to do and everything should already have been taken care of for us. So here, we simply need to rebuild our application:
Checking our Solution
As normal let’s now check out solution works. Start up the application as previously (instructions are available in Reviewing the initial application if you need a refresher!)
And then we could send in a payment:
curl -X POST localhost:8080/submit | jq
And if we bring up the payment in the Developer GUI and look at the graph of our tutorial flow (search by unit of work id, click view, click ipf tutorial flow, click view graph) then we see:
If we compare this to the graph of DSL 9 - Using Subflows, we can see that everything is the same as it was and we have successfully extracted our Sanctions subflow to a different model which is completely stand alone from the Ipftutorialmodel.
Conclusions
In this section we have learnt how to reuse a common component within our solution.
DSL 14 - Using shared concepts (Part Two - Across Solutions)
DSL 16 - Dynamic Error Text
------------------------------
DSL 16 - Dynamic Error Text
Getting Started
The tutorial step uses the "shared_models_three" solution of the project as it’s starting point.
If at anytime you want to see the solution to this step, this can be found on the add_dynamic_text solution.
Purpose
In DSL 4 - Using an external domain, we introduced the concept of a reason code to be able to enrich our events with knowledge as to the underlying reason for the event being raised. At the time, we only used the name and description fields to describe the reason code. In this section, we’ll take this one step further and  show how we can dynamically setup our reason code text based on the information in the flow. Therefore, instead of just having a generic error message that is used for all payments, we can make our error message unique to our particular payment and hence provide enriched information.
In our case, we’re going to create some reason codes for our fraud response and use them to start providing dynamic error messages. Let’s get on and see how this is done.
Using the DSL
Placeholders
The first thing we need to do is introduce a new concept, "placeholders". Placeholders are a way of extracting information out of any given piece of business data (or combination of business data!) into a simple string format that we can use in messaging. The placeholder has two properties:
The name of the placeholder
A list of business data.
You can think of a placeholder as a function that takes in a list of business data elements and returns a string. In the simplest case, the business data may just be a string itself but in more complex cases you may for example wish to extract a specific field on the pacs.008.
Let’s create a placeholder to pretend to extract some data from our pacs.008. So we start by adding a new placeholder library by right clicking on our model and selecting New  Placeholder Library
This will look and feel like all the other libraries, so lets start by clicking "Add placeholder" and then entering:
a name of "Extract from Pacs008"
a business data element of "Customer Credit Transfer"
When complete, it should look like this:
Now let’s use our placeholder to create a dynamic text for our reason code.
Dynamic Text
Let’s add a new "Reason code set" to our existing "Reason code library" for our fraud system. You’ll add two reason codes, the first will just use a standard description whilst the second will also include the dynamic text definition.
So let’s start by adding two reason codes:
One with a name "Simple Reason" and description "A simple reason without dynamic text"
One with a name "Dynamic Reason" and description "A reason with dynamic text".
So far this is just like we did in our previous tutorial (). Now for the dynamic reason lets add a dynamic text message. We start by pressing return in the text block. Now we can type our message just like any normal text input. So let’s start by typing "This is a dynamic message. The special value is " into the box. Now we want to finish our text by grabbing the value of the "Extract from Pacs008" placeholder we setup. To do this we do this simply by pressing CTRL + SPACE and then selecting it from the dropdown of available terms.
When complete, our reason code set and reason codes should look like:
Finally, we need to add our new Fraud Reason codes into our response definition for our fraud check so that we can send back the reason code.
And that’s it from a DSL viewpoint, now lets turn to our implementation.
Java Implementation
Let’s now switch back to Intellij and look at how we plug this into our implementation code. As normal we’ll start by running a build from a terminal window in the root directory of our project:
mvn clean install
In our DSL work we created a placeholder. We described this as a function that takes some business data and returns a string. That’s exactly what it is! Each placeholder will result in a method on the domain’s "Placeholder Port".
Let’s look at the generated code in /domain-root/domain/target and we should now find the port for defining our placeholders like this:
package com.iconsolutions.ipf.tutorial;
import com.iconsolutions.ipf.payments.domain.clearing_and_settlement.pacs008.FIToFICustomerCreditTransfer;
public interface IpftutorialmodelPlaceholderPort {
  String executeExtractFromPacs008(FIToFICustomerCreditTransfer customerCreditTransfer);
}
So here we can see that we have generated a function that takes our FIToFICustomerCreditTransfer and returns a String. This is what we’ll need to implement and define on our configuration set up as normal. For now we’ll do this by implementing a very simple constant mapping that returns the string "Test Mapping". You’ll add this to the tutorial config as below:
public IpftutorialmodelDomain init(ActorSystem actorSystem, IsoMappingService mappingService, SchedulerPort schedulerAdapter) {
    // All adapters should be added to the domain model
    return new IpftutorialmodelDomain.Builder(actorSystem)
            .withTutorialDomainFunctionLibraryAdapter(input -> CompletableFuture.completedStage(new DuplicateCheckResponseInput.Builder(input.getId(), AcceptOrRejectCodes.Accepted).build()))
            .withAccountingSystemActionAdapter(new SampleAccountingSystemActionAdapter())
            .withFraudSystemActionAdapter(new FraudSystemActionAdapter())
            .withDecisionLibraryAdapter(input ->
                    input.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getIntrBkSttlmAmt().getValue().compareTo(BigDecimal.TEN)>0 ?
                            RunFraudCheckDecisionOutcomes.FRAUDREQUIRED : RunFraudCheckDecisionOutcomes.SKIPFRAUD)
            .withIpftutorialflowV1AggregateFunctionAdapter(input -> new ExtractCreditorAccountForFlowIpftutorialflowV1AggregateFunctionOutput(input.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getCdtrAcct()))
            .withIpftutorialflowV2AggregateFunctionAdapter(input -> new ExtractCreditorAccountForFlowIpftutorialflowV2AggregateFunctionOutput(input.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getCdtrAcct()))
            .withInitiationFlowAggregateFunctionAdapter(parameters -> new MapPain001ToPacs008ForFlowInitiationFlowAggregateFunctionOutput(mappingService.mapPain001toPacs008(parameters.getPaymentInitiation())))
            .withSchedulerAdapter(schedulerAdapter)
            .withCSMServiceActionAdapter(new SampleCSMServiceActionAdapter())
            .withSanctionsSystemActionAdapter(new SampleSanctionsSystemActionAdapter())
            .withPlaceholderAdapter(customerCreditTransfer -> "Test Mapping")
            .build();
}
The final thing we need to do is update our FraudCheckAdapter to provide the reason codes. To enable our testing we’ll set it up so that:
if the payment value > 40, we’ll reject with our dynamic reason code.
If the payment value > 30 (and below 40!) , we’ll reject with our normal description.
Let’s look at the code for that:
@Slf4j
public class FraudSystemActionAdapter implements FraudSystemActionPort {
    private Duration duration = Duration.ofMillis(10);
    @Override
    public CompletionStage<Void> execute(final CheckFraudAction action) {
        log.debug("Received an action of type {} for id {}", action.getActionName().name(), action.getId());
        if (action.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getIntrBkSttlmAmt().getValue().compareTo(new BigDecimal("50")) >= 0) {
            return CompletableFuture.supplyAsync(() -> "delaying response", CompletableFuture.delayedExecutor(duration.toNanos(), TimeUnit.NANOSECONDS))
                    .thenAccept(string -> log.debug("Pretending to timeout the fraud call for aggregate {}", action.getProcessingContext().getAssociationId()));
        } else if (action.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getIntrBkSttlmAmt().getValue().compareTo(new BigDecimal("40")) >= 0) {
            return CompletableFuture.supplyAsync(() -> "delaying response", CompletableFuture.delayedExecutor(duration.toNanos(), TimeUnit.NANOSECONDS))
                    .thenCompose((String string) -> IpftutorialmodelDomain.fraudSystem().handle(
                            new FraudCheckResponseInput.Builder(action.getId(), AcceptOrRejectCodes.Rejected)
                                    .withReasonCode(FraudReasonCodes.DynamicReason)
                                    .build())
                            .thenAccept((Done done) -> log.debug("Sent input of type {} for id {} with result {}", done.getCommandName(), action.getId(), done.getResult().name())));
        } else if (action.getCustomerCreditTransfer().getCdtTrfTxInf().get(0).getIntrBkSttlmAmt().getValue().compareTo(new BigDecimal("30")) >= 0) {
            return CompletableFuture.supplyAsync(() -> "delaying response", CompletableFuture.delayedExecutor(duration.toNanos(), TimeUnit.NANOSECONDS))
                    .thenCompose((String string) -> IpftutorialmodelDomain.fraudSystem().handle(
                                    new FraudCheckResponseInput.Builder(action.getId(), AcceptOrRejectCodes.Rejected)
                                            .withReasonCode(FraudReasonCodes.SimpleReason)
                                            .build())
                            .thenAccept((Done done) -> log.debug("Sent input of type {} for id {} with result {}", done.getCommandName(), action.getId(), done.getResult().name())));
        } else {
            return CompletableFuture.supplyAsync(() -> "delaying response", CompletableFuture.delayedExecutor(duration.toNanos(), TimeUnit.NANOSECONDS))
                    .thenCompose((String string) -> IpftutorialmodelDomain.fraudSystem().handle(new FraudCheckResponseInput.Builder(action.getId(), AcceptOrRejectCodes.Accepted).build()).thenAccept((Done done) -> log.debug("Sent input of type {} for id {} with result {}", done.getCommandName(), action.getId(), done.getResult().name())));
        }
    }
}
Checking our solution
As normal let’s now check out solution works. Start up the application as previously (instructions are available in Reviewing the initial application if you need a refresher!)
Now let’s test our application. You’ll start by sending a payment through of 45 USD - this should give us our dynamic error text.
curl -X POST localhost:8080/submit -H 'Content-Type: application/json' -d '{"value": "45"}' | jq
Then lets bring up the payment in the Developer GUI and look at the domain events (search by unit of work id, click view, click domain events) then we see:
Now if we click to see the body of our Fraud Check Failed event, we’ll see:
And we can see that our reason code has been generated, pulling in the value from our placeholder function.
However, if we repeat the process for a payment of say 35USD, instead of using the dynamic text we’ll revert to just using the description provided:
That’s everything working.
This is obviously a very simple implementation with some hardcoded text, but you can provide an implementation of the port (i.e. the IpftutorialmodelPlaceholderPort interface) which accesses any of the payment data;
public class PlaceHolderAdapter implements IpftutorialmodelPlaceholderPort {
    @Override
    public String executeExtractFromPacs008(FIToFICustomerCreditTransfer customerCreditTransfer) {
        return "failed for amount " + customerCreditTransfer.getCdtTrfTxInf().get(0).getIntrBkSttlmAmt().getValue().toString();
    }
}
Conclusions
In this section we learnt how we can use placeholders to provide enrichment information within error text.
DSL 15 - Using shared concepts (Part Three - remote models)
CON1 - Adding payment initiation
------------------------------
CON1 - Adding Payment Initiation
Getting Started
The tutorial step uses the add_dynamic_text solution of the project as it’s starting point.
If at anytime you want to see the solution to this step, this can be found on the add_payment_init solution.
In the DSL tutorial, we built an application that uses the Icon Payment DSL to build up a flow. To initiate this flow, our sample application had a simple rest controller that allowed us to initiate the flow. We were able to send in certain key values such as the payment value to help us test various different conditions within our flow. However in the real world, these payment instructions would be coming from an external source.
In this section, we’re going to use an existing test module - the "Sample Payment Initiator" as the external source. This is a simple application we can use to test with it has a few key properties:
It provides a simple simulator that allows us to generate pain001’s, providing an interface to set some of the key values.
The application can be used with different protocols (Kafka, JMS)
The applications comes with a set of pre-packaged "connectors". These are the client components, built using Icon’s connector framework that allow quick and easy integration between the main IPF app and the payment initiation sim.
In this tutorial, we’re going to use the Kafka version of the payment simulator.
Let’s get going and set everything up so that we can start sending messages to our IPF application from the Payment Initiation Simulator.
A Quick Recap
Let’s do a quick recap on the existing flow, the key one here is the initiation flow and it’s initiation behaviour:
So the key thing to note here is that we’re sending in a payment initiation (pain001) into start the flow. We won’t worry about the example data object here as it was just used as a method of illustrating custom types.
Now if we remember, when generated this will create a new method on the domain’s initiation controller that allows us to make flow initiation requests. In the current flow, we do this within the controller of the main ipf-tutorial-app application. Let’s remind ourselves of that code (taken from InitiationController):
Mono.fromCompletionStage(IpftutorialmodelDomain.initiation().handle(new InitiateInitiationFlowInput.Builder(entityId)
        .withProcessingContext(ProcessingContext.builder()
                .unitOfWorkId(unitOfWorkId)
                .clientRequestId(clientRequestId)
                .build())
        .withPaymentInitiation(samplePain001)
        .withExampleData(IpfTutorialBean.builder().intField(7).stringField("Test").build())
        .build()).thenApply(done -> InitiationResponse.builder().requestId(clientRequestId).uowId(unitOfWorkId).aggregateId(done.getAggregateId()).build()));
So the key here is that we are using the sample pain001 we generated as our payment initiation data when we construct the initiation input. You’ll do something very similar when setting up to use the payment initiation simulator.
Adding the Connector
First we need to add the dependency to the sample simulator we will use for payment initiator (add this to the "ipf-tutorial-app" pom.xml). This is here:
<dependency>
  <groupId>com.iconsolutions.ipf.sample.samplesystems</groupId>
  <artifactId>payment-initiation-connector-kafka</artifactId>
</dependency>
At the time of writing, the sample systems version is 2.0.41, and we need to add this to both the dependency management section of the root pom (adding the version), and the actual dependencies of the "ipf-tutorial-app" project.
Note also that we’ve chosen the Kafka implementation here as our protocol.
The Client Adapter
When the payment initiation simulator runs, it will post out messages to the relevant Kafka topic. We’re going to use the pre-packaged Icon connector in our application and that will read off that topic and processes messages. For this, the connector provides an interface, the PaymentInitiationClientAdapter that we’ll need to provide an implementation of this in ipf-tutorial-app. Our implementation will need to provide the same core logic that the current initiation controller does, i.e. take the pain001 and forward it onto the domain’s initiation methods.
Let’s start by looking at the definition of this interface:
public interface PaymentInitiationClientAdapter {
    ProcessingContext determineContextFor(PaymentInitiationRequest request);
    CompletionStage<Void> handle(ReceivingContext context, PaymentInitiationRequest request);
}
As you can see there are two methods it provides:
determineContextFor - This is the chance to provide upfront processing context, for example the unitOfWorkId which represents an IPF reference used to track all activity against.
handle - This is where we handle inbound messages, and for us we need to pass the message onto the flow.
Let’s consider what we want to do in our implementation.
determineContextFor - in our case we’re not too worried about id’s. But if there was a specific client request id or unit of work id that you required your messages to use this would be where to set it. However, we’ll use a generated unit of work id.
handle - this is where we need to take the pain001 out of the request object and pass it into the domain’s initiation methods.
See if you can work out how this code would work, once done the solution (to add to the ipf-tutorial-app) is below:
@Slf4j
@Service
public class SamplePaymentInitiationAdapter implements PaymentInitiationClientAdapter  {
    @Override
    public ProcessingContext determineContextFor(PaymentInitiationRequest paymentInitiationRequest) {
        return ProcessingContext.builder().unitOfWorkId(UUID.randomUUID().toString()).build();
    }
    @Override
    public CompletionStage<Void> handle(ReceivingContext receivingContext, PaymentInitiationRequest paymentInitiationRequest) {
        return IpftutorialmodelDomain.initiation().handle(
                new InitiateInitiationFlowInput.Builder(UUID.randomUUID().toString())
                    .withProcessingContext(receivingContext.getProcessingContext())
                    .withPaymentInitiation(paymentInitiationRequest.getPayload().getContent())
                .build())
                .thenAccept(done -> log.debug("Completed request"));
    }
}
Note here that we use springs "@Service" annotation to wire in the dependency.
That’s all our code done,  lets build the application:
mvn clean install -rf :ipf-tutorial-app
Note that we have only changed implementation code and not our flow so there is no need to rebuild the MPS components at this time.
Configuring the Application
Now it’s time to set up the configuration, So there’s two things not currently in here that we’ll need:
Default no-op crypto bean in ipf-tutorial-app
The configuration for our payment initiation connector to talk to kafka.
First, we need to add a default crypto bean to our ipf-tutorial-app (in IpfTutorialConfig), since we are not encrypting the data we will be configuring a NoOpCrypto bean
    @Bean
    public Crypto crypto() {
        return new NoopCrypto();
    }
Now we need to look at the configuration for the payment initiation connector.
The standard config like the topic’s it’s going to read from is provided out of the box. It’s going to read messages from the PAYMENT_INITIATION_REQUEST topic. So all we need to tell it how to connect to Kafka. We can do this in two ways.
Explicit Configuration
The first approach we can use is to configure the kafka setup explicitly for this connector. To do this, we need to note that the config root for the payment initiation connector is paymentinitiation. Therefore we need to supply our kafka configuration from this root as follows:
paymentinitiation.kafka {
  producer.kafka-clients {
    bootstrap.servers = "kafka:9092"
    client.id = "ipf-tutorial-app"
  }
  consumer.kafka-clients {
    bootstrap.servers = "kafka:9092"
    group.id = "ipf-tutorial-app"
  }
}
Note that here we are using "kafka:9092" as our bootstrap URL, this is what is expected for the container solution, but if using your own kafka you may need to change it.
Default Configuration
The alternative approach is to use a default configuration. All the IPF connectors will default to using a config root of akka. This means that if we have multiple services all using the same kafka we can define the configuration once and let all our services use that. We’re going to assume for this tutorial that is the case so we’ll set up our service like that. You’ll make a few tweaks now to give us extra functionality.
// default settings for kafka
common-kafka-client-settings {  (1)
  bootstrap.servers = "kafka:9092" (2)
}
akka.kafka {
  producer {
    kafka-clients = ${common-kafka-client-settings} (3)
  }
  consumer {
    kafka-clients = ${common-kafka-client-settings} (4)
  }
}
// end default kafka settings
Let’s review a few bits of this:
1
We extract our the raw kafka settings (group id, bootstrap servers) into a separate section. This means we can reuse this across different property sets. You can see that in points 3 and 4.
2
As per above this is the kafka bootstrap url and may be different for your environment.
3
Here we reuse our common kafka settings for the producer.
4
Here we reuse our common kafka settings for the consumer.
Running the Application
Docker
Kafka
Firstly, we need to setup a Kafka container. The below provides a sample entry to add to the application.yml for the Kafka setup for use in this tutorial.
zookeeper:
  image: zookeeper:latest
  container_name: zookeeper
  ports:
    - "2181:2181"
kafka:
  image: wurstmeister/kafka:2.13-2.7.1
  container_name: kafka
  depends_on:
    - zookeeper
  ports:
    - "9092:9092"
  environment:
    - KAFKA_BROKER_ID=0
    - KAFKA_AUTO_CREATE_TOPICS_ENABLE=true
    - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
    - KAFKA_LOG_RETENTION_MINUTES=10
    - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
    - KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS=1
    - KAFKA_LISTENERS=PLAINTEXT://:9092
    - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
So here we’re providing a simple Kafka server which will be running on port 9092 and the configuration we supplied to the application in the previous section will connect to this.
Payment Initiation Simulator
Next lets add the payment initiation simulator to our deployment. We can add the following into the application.yml
payment-initiation-simulator-kafka:
  image: registry.ipf.iconsolutions.com/sample-systems-payment-initiation-simulator-kafka:2.0.41
  container_name: payment-initiation-simulator-kafka
  ports:
    - "8082:55555"
  volumes:
    - ./config/payment-initiation-simulator-kafka:/payment-initiation-simulator-kafka/conf
  user: "1000:1000"
  depends_on:
    - kafka
Here we’ll need to supply our config for the payment initiation sim itself. To do this, we’ll add a new directory in the docker/config folder called payment-initiation-simulator-kafka. It requires an application.conf file that contains:
common-kafka-client-settings {
  bootstrap.servers = "kafka:9092"
}
connector {
  default-receive-connector {
    manual-start: true
  }
  default-send-connector {
    manual-start = false
    call-timeout = 30s
    queue-size = 50
    max-concurrent-offers = 500
    resiliency-settings {
      minimum-number-of-calls = 1
      max-attempts = 1
      reset-timeout = 1s
      initial-retry-wait-duration = 1s
      backoff-multiplier = 2
    }
  }
  validator-error-handler-dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      fixed-pool-size = 4
    }
  }
}
akka.kafka {
  producer {
    kafka-clients = ${common-kafka-client-settings}
  }
  consumer {
    kafka-clients = ${common-kafka-client-settings}
  }
}
Non Docker
Kafka
When not running in docker, we’ll need to use your own kafka environment. This should have two topics available:
PAYMENT_INITIATION_RESPONSE
PAYMENT_INITIATION_REQUEST
Next, we need to tell the IPF tutorial application how to connect to Kafka. You’ll then need to update the bootstrap config we set up in the previous section to reach your kafka environment.
Payment Initiation Simulator
Details for how to run the payment initiation simulator can be found here: Using the payment initiation simulator.
Testing the Application
As normal let’s now check out solution works. Start up the application as previously (instructions are available in Reviewing the initial application if you need a refresher!)
Now the payment initiation simulator will be available (along with the Zookeeper and Kafka setup).
You can access the simulator page here
When brought up, it should look like:
There’s a couple of key functions to note here - that we can both:
set load rate - this allows us to send a consistent rate of transactions, e.g. 10TPS, through to our application.
initiate single payment - this allows us to send a single payment with provided values.
The current activity tab we won’t use for now but allows the simulator to track responses to payments.
You’ll start by calling an individual payment. Click the initiate single payment button and then we’ll leave everything as default and click initiate payment. This will fire in a generated payment. A pop up box will appear that will continue to spin, this is because the simulator by default expects that there will be a response to it’s message. For now we don’t have this so the spinning is the expected behaviour.
Let’s bring up the Developer GUI ("IPF Transaction Explorer" used previously) and run a search. We should see a new record having been created a few seconds ago:
Let’s view this record and then go to the "messages tab":
Here we can see that IPF has "received" a PaymentInitiationRequest. You could view the body of the message if you want.
Feel free at this stage to try sending some different messages through with different values, or to send a fixed load through.
Conclusions
In this section we’ve learnt how to setup to connect to an external system that has a pre-packaged connector available.
Next up we’ll look in a little more detail at connectors in: CON2 - Writing your own connector
DSL 16 - Dynamic Error Text
CON2 - Writing your own connector (Kafka)
------------------------------
CON2 - Writing your own connector (Kafka)
Getting Started
The tutorial step uses the "add_payment_init" solution of the project as it’s starting point.
If at anytime you want to see the solution to this step, this can be found on the "add_kafka" solution!
In CON1 - Adding payment initiation, we connected our sample application with a payment initiator to allow us to receive payment instructions from a remote service. To do this, we used a pre-packaged connector so that all we had to do was implement the logic for processing upon receipt of the message. This time we’re going to take a step further back and actually write the connector ourselves.
You’ll do this by integrating in with a test sanctions system. This system:
Expects to receive a custom "SanctionsRequest" object.
Will return a custom "SampleEvent" object.
The system can work over either Kakfa or JMS. You’ll use kafka in this example!
A few basics
Let’s start with a few basics about the connector framework.
In this tutorial we’ll consider two types of connectors - "Sending" connectors and "Receiving" connectors. You’ll use these to put a message onto a kafka topic (send) and then also to process the response (receive).
As the request / response will be asynchronous, we will need some way of determining which response lines up to which request. We do this using "correlation".
Connectors in themselves are backed by Lightbend’s Alpakka framework and use streams to process. To work the require a "transport". A connector transport tells the connector how to communicate with the underlying protocol it is working with. It is therefore perfectly possible to use the same connector and then supply different transports to be able to communicate over different protocols. In this way, we can keep our logic of the processing of the message separated from the details of the underlying protocol.
There is a third type of connector, the "RequestReply" type, but this will be considered in a later tutorial.
Supporting Classes
The first thing we’ll do is import the domain definition for the sanctions system. To do this we need to add a dependency into our "ipf-tutorial-app" applications pom.xml:
<dependency>
    <artifactId>sanctions-domain</artifactId>
    <groupId>com.iconsolutions.ipf.sample.samplesystems</groupId>
</dependency>
Let’s look at the key classes we receive from this module. The first is the request object we send to the Sanctions system.
@Data
public class SanctionsRequest extends SampleEvent<SanctionsRequestPayload> {
    private Map<String, String> headers;
}
So here we have an object that is taking both a set of headers and a request payload. Have a look down the object hierarchy to get a feel for the request we’ll need to send, pay particular attention to the filtering request.
If we consider the response side, we can see similarly there is a SanctionsResponse object. Again checking the hierarchy we’ll see the core of the response is:
@Builder
@NoArgsConstructor
@AllArgsConstructor
@Data
@JsonInclude(JsonInclude.Include.NON_NULL)
public class FilteringResponse {
    private String status;
    private String additionalInf;
}
So the key element here is we will receive back a status.
Next we’ll pull in another dependency,
<dependency>
    <artifactId>sanctions-mapping</artifactId>
    <groupId>com.iconsolutions.ipf.sample.samplesystems</groupId>
</dependency>
This dependency, provides a prepackaged set of mappers (using Icon’s mapping framework) that provides a mapping from a pacs008 to a SanctionsRequest object. The key class to look at here is the SanctionsMapper class which provides this method:
public SanctionsRequest map(FIToFICustomerCreditTransfer fiToFICustomerCreditTransfer) {
    var filteringRequest = transformationService.mapThenEnrichWithDefault(fiToFICustomerCreditTransfer, FilteringRequest.class);
    SanctionsRequest sanctionsRequest = new SanctionsRequest();
    sanctionsRequest.setHeader(HeaderUtils.makeHeader("Sanctions", fiToFICustomerCreditTransfer.getCdtTrfTxInf().get(0).getPmtId().getTxId()));
    sanctionsRequest.setPayload(new SanctionsRequestPayload(filteringRequest));
    return sanctionsRequest;
}
Here we can see we are mapping from the pacs.008 (FIToFICustomerCreditTransfer) to return the SanctionsRequest.
Now we’ll use spring to provide to us an instance of the sanctions mapper that we can use. To do this we’ll create a new bean within the IpfTutorialConfig class as:
@Bean
public SanctionsMapper sanctionsMapper(ObjectMapper objectMapper) {
    return new SanctionsMapper(objectMapper);
}
Those are the building blocks we’re going to use, so let’s get on and start writing our connector.
The Send Connector
You’ll start by looking at the send connector, this is the one that will post a Sanctions Request message onto the appropriate topic for the external system to consume.
Firstly, let’s add the dependency for the connector framework’s kafka implementation.
<dependency>
    <groupId>com.iconsolutions.ipf.core.connector</groupId>
    <artifactId>connector-kafka</artifactId>
</dependency>
Now we’ll create a new configuration class, called "SanctionsConnectorConfiguration" and we’ll place it into a new "connectors" package under the existing config package
(Remember to mark your class with the spring @Configuration annotation! You’ll also use lombok’s @Slf4j annotation to provide access to a logging implementation)
Now let’s write our send connector, we’ll need to make a few decisions first:
Types - The definition of a send connector is SendConnector<D, T>. In this instance, the D represents the source (domain) object type and the T the target object type. In our connector will take in a FIToFICustomerCreditTransfer and send out a SanctionsRequest object.
Logging - We can provide a logging implementation to a connector. A message logger is any class that implements this simple functional interface:
public interface MessageLogger {
    void logMessage(MessageLogEntry var1);
}
In our instance, the application comes with a message logger that is part of the data processing. This has everything we need so we’ll just reuse it.
Correlation - For correlation we’re going to use another Icon provided utility. You’ll use Icon’s mongo correlation service. So we’ll need to bring in that dependency too:
<dependency>
    <groupId>com.iconsolutions.ipf.core.connector</groupId>
    <artifactId>connector-correlation-starter-mongodb</artifactId>
</dependency>
Mapping - As discussed above, for mapping we’ll use the SampleMapper that we’ve pulled in.
Let’s start by thinking at the class level. You’ll provide for class variables to support our connectors:
private final SanctionsMapper sanctionsMapper;
private final ObjectMapper objectMapper;
private final ClassicActorSystemProvider actorSystem;
private final CorrelationService correlationService;
private final MessageLogger messageLogger;
These are as discussed above, together with the actor system itself.
You’ll use spring’s dependency injection to provide these for us by using the @AllArgsConstructor annotation.
Let’s then create a new method to create the send connector:
@Bean(name = "sanctionsSendConnector")
public SendConnector<FIToFICustomerCreditTransfer, SanctionsRequest> sanctionsSendConnector(ConnectorTransport<SanctionsRequest> sanctionsSendConnectorTransport) {
    return SendConnectorBuilderHelper.<FIToFICustomerCreditTransfer, SanctionsRequest>builder("Sanctions", "sanctions.send-connector", actorSystem)
            .withConnectorTransport(sanctionsSendConnectorTransport)
            .withCorrelationIdExtractor(event -> CorrelationId.of(event.getHeader().getTechnical().getEventId()))
            .withCorrelationService(correlationService)
            .withSendTransportMessageConverter(this::convertToTransport)
            .withDomainToTargetTypeConverter(sanctionsMapper::map)
            .withMessageLogger(messageLogger)
            .build();
}
This is important enough to walk through each part in turn.
Firstly we pass into the method a ConnectorTransport<SanctionsRequest> sanctionsSendConnectorTransport. This is an implementation of a connector transport, if you remember our discussion above, the connector transport is used to provide the low level protocol information. For now we’re not going to worry about that, hence we’ll just pass it into our method.
Now let’s take each line in turn and explain what’s going on.
The builder construction - it takes three parameters:
The name of the connector - this is particularly useful later when we look at metrics.
The config-root for the connector. This allows us to pass the root path for the connector’s properties. All connector properties will start with this variable. This allows us to then build out connector level properties.
The actor system itself.
the transport - obviously here we just use the connector transport we have passed to the method.
the correlation extractor function - this is a function that will provide the connector with a unique id that is used for correlation. The id must be unique and be obtainable from the response message too. Here we use the event id on the header.
the correlation service - as per above, we’ll use Icon’s mongo backed implementation.
the transport message converter - this is a function that takes the sanctions request and converts it into a transport message for passing down the wire. In our case, we’ll make a simple implementation that creates a new TransportMessage with a string representation of our request as the payload.
private TransportMessage convertToTransport(SanctionsRequest request) {
    try {
        return new TransportMessage(new MessageHeaders(CryptoHelper.messageHeaders()), objectMapper.writeValueAsString(request));
    } catch (JsonProcessingException e) {
        throw new IconRuntimeException(e);
    }
}
the domain to target type converter - this is the function that will map from our domain type (pacs.008) to our target type (SanctionsRequest). As per above, will use the SanctionsMapper’s map method to do this.
the message logger - as discussed we’ll just use existing logger here.
That’s it, that’s our very first send connector built from scratch.
The Receive Connector
Now that we’ve written the code to setup our connector to send a message out to the sanctions system, we’ll need one to do the reverse and receive the response when it’s ready. So let’s again add a new method to our configuration, this time to construct our receive connector:
@Bean(name = "sanctionsReceiveConnector")
public ReceiveConnector<SampleEvent> sanctionsReceiveConnector(ReceiveConnectorTransport sanctionsReceiveConnectorTransport) {
    return ReceiveConnectorBuilderHelper.<SampleEvent>builder("SanctionsReceive", "sanctions.receive-connector", actorSystem)
            .withConnectorTransport(sanctionsReceiveConnectorTransport)
            .withCorrelationIdExtractor(event -> CorrelationId.of(event.getHeader().getTechnical().getOriginalEventId()))
            .withCorrelationService(correlationService)
            .withReceiveTransportMessageConverter(message -> sanctionsMapper.convertResponse(message.getPayload().toString()))
            .withReceiveHandler(this::sanctionsReceiveConnector)
            .withMessageLogger(messageLogger)
            .build();
}
As we did last time, let’s walk through the key points of this setup.
The builder construction - just as with send connectors we’ll pass in the name of the connector, the root config path and the actor system.
the transport - we’ll again need the protocol setup, this time for receiving a message. You’ll pass this into our method to allow us to deal with protocol details elsewhere.
the correlation extractor function - just like with the send, we need a function that will provide the same id as we had in the send function, but this time extracted from the response event. This time we’ll take the "original event id" from the header.
the correlation service - again we’ll use Icon’s mongo backed one.
the receive transport converter - here we need a function that converts from the raw message recieved into the expected response target type (a sample event). You’ll take that from our sanctions mapper functions.
the receive handler - this is the key function. This is a function that takes in the response message together with the context for it (taken from correlation in our case) and requires us to determine what to do with it. In our case, we want to construct a new domain sanctions input and send it into our domain. See if you can write this and when ready compare with our function below:
private CompletionStage<Void> sanctionsReceiveConnector(ReceivingContext receivingContext, SampleEvent sampleEvent) {
    return IpftutorialmodelDomain.sanctionsSystem().handle(new SanctionsNoHitInput.Builder(receivingContext.getProcessingContext().getAssociationId().getValue()).build())
            .thenAccept(done -> log.info("Completed {}", done));
}
the message logger - and again we’ll just use the existing logger.
That’s it, that’s our entire receive connector written and ready to go.
The Connector Transports
In the definitions for both our receive and send connectors, we passed in a transport definition. Now we need to set these up. For the moment, we’re just going to use Kafka, so we’ll need those versions. To do this will create a new spring configuration class called "SanctionsTransportConfiguration" which we’ll put in a new "transports" package under our connectors package.
The Sending Connector Transport
You’ll start with the sending side:
@Bean
public ConnectorTransport<SanctionsRequest> sanctionSendConnectorTransport(ClassicActorSystemProvider actorSystem) {
    Config config = AlpakkaConfigProvider.getProducerConfig(actorSystem.classicSystem(), "sanctions");
    StringSerializerProducerConfig producerConfig = new StringSerializerProducerConfig(config);
    return new KafkaConnectorTransport<>(
            "SanctionsSendKAFKA",
            producerConfig.topic(),
            producerConfig.producerSettings(),
            new StringProducerRecordKeyValueProvider(),
            producerConfig.restartSettings(),
            actorSystem
    );
}
At this stage we move into specific low level kafka configuration, the details of which lie outside of this tutorial. The key things to note however are:
We give the connector transport a name, again for use in metrics.
We provide the topic which the transport should use.
We provide the "producer" settings, these are standard kafka connection details for sending (producing) messages to a topic.
We provide a restart policy - this is how the transport should behave in the event of a failure.
We provide the actor system.
The Receive Connector Transport
This is very similar to the send:
@Bean
public ReceiveConnectorTransport sanctionsReceiveConnectorTransport(ClassicActorSystemProvider actorSystem) {
    Config config = AlpakkaConfigProvider.getConsumerConfig(actorSystem.classicSystem(), "sanctions");
    StringDeserializerConsumerConfig consumerConfig = new StringDeserializerConsumerConfig(config);
    return KafkaAckReceiveConnectorTransport.<String, String>builder()
            .withName("KafkaSanctionsReceiveConnectorTransport")
            .withTopics(Set.copyOf(consumerConfig.topics()))
            .withConsumerSettings(consumerConfig.consumerSettings())
            .withRestartSettings(consumerConfig.restartSettings())
            .withPartitions(consumerConfig.maxPartitions())
            .withActorSystem(actorSystem)
            .build();
}
The main differences are this time we use the kafka configuration for consuming messages from a topic (or potentially multiple topics!). We also have to decide how many partitions we want to apply to our topic. This will help with performance but for now is outside of the scope of this tutorial.
That’s our transport defined, all that remains now is setting up the actual configuration to use it.
Using the connector
Now we need to plug our connections into our flow for usage. The first thing to note here is that the receive connector part (method sanctionsReceiveConnector) is already done for us as the handler is sending on the response back to the domain.
So here we only need to worry about the sending side. For this, we simply need to swap out our sample sanctions adapter (SampleSanctionsSystemActionAdapter) with a real one that calls our new send connector’s send method. The action we supply to the adapter has all the information we’ll need to do that too! See if you can set that up now and when ready the solution is below.
@Bean
public IpftutorialmodelDomain init(ActorSystem actorSystem, IsoMappingService mappingService, SendConnector<FIToFICustomerCreditTransfer, SanctionsRequest> sanctionsConnector) {
    ...
            .withSanctionsSystemActionAdapter(checkSanctions -> sanctionsConnector.send(checkSanctions.getProcessingContext(), checkSanctions.getCustomerCreditTransfer()).thenAccept(done -> log.info("Completed sanctions call: {}", done)))
    ...
So here we can see we are injecting in our new send connector and then simply calling the send and logging the result. That’s all we need to do.
Configuration
You’ll add our configuration into our application configuration file (ipf-tutorial-app/application.conf).
For our connector to work, we need to add a number of things:
The restart settings
The number of partitions to use on send
The group and client ids for kafka to use
We’re going to add these into the common akka configuration that we created in the previous tutorial. We could just as easily add all these into the bespoke sanctions block, but then they wouldn’t be available to reuse for other connectors.
Restart Settings
Firstly, we need restart settings, ie to tell the connector what to do on failover. Here we’ll define the standard set that we’re going to use for all our connectors.
default-restart-settings {
  min-backoff = 1s
  max-backoff = 5s
  random-factor = 0.25
  max-restarts = 5
  max-restarts-within = 10m
}
We are telling our connector that in the event of transport failure, we will attempt to restart upto 5 times with an increasing time between each restart.
To use this, we’re going to add it to the default akka block. You’ll do this by adding a line under both the consumer and producer settings:
restart-settings = ${default-restart-settings}
Note the syntax here, how we are able to refer to a complex block from elsewhere in our hocon structure by using the ${…​} setup.
Partitions
Partitions allow us to define how many kafka partitions we should setup. In our case we’re just going to setup 10 partitions, we do that by adding it to the akka block we need to add a line to the consumer with:
max-partitions = 10
Client and Group Ids
Finally, we’ll add the client and group ids.
For the client id we need to add an entry to the producer config:
kafka-clients {
    client.id = ipf-tutorial-client
}
And for the group id we need to add an entry to the consumer config:
kafka-clients {
    group.id = ipf-tutorial-group
}
Sanctions Configuration
Now we’ve defined our common parts, because they are defined with the default akka block they are immediately available to us for our sanctions configuration. That means all we now have to do on a sanctions level is provide the explicit custom configuration for the sanctions setup itself. In this case, the only thing is the actual topics which need to be used.
If we remember back to our transport definition (SanctionsTransportConfiguration) we said:
Config config = getConsumerConfig(actorSystem.classicSystem(), "sanctions");
The "sanctions" parameter tells the connector to look for elements under the "sanctions" prefix in the configuration. So all we need to do is add our topics under this root as follows:
sanctions {
  kafka {
    producer {
      topic = SANCTIONS_REQUEST
    }
    consumer {
      topic = SANCTIONS_RESPONSE
    }
  }
}
So the first thing to note if we look at our transport definition (SanctionsTransportConfiguration) we said:
Summary
That’s everything from our application configuration side complete, just as a recap the full config for the connector should now look like:
sanctions {
  kafka {
    producer {
      topic = SANCTIONS_REQUEST
    }
    consumer {
      topic = SANCTIONS_RESPONSE
    }
  }
}
// default settings for kafka
default-restart-settings {
  min-backoff = 1s
  max-backoff = 5s
  random-factor = 0.25
  max-restarts = 5
  max-restarts-within = 10m
}
common-kafka-client-settings {
  bootstrap.servers = "kafka:9092"
}
akka.kafka {
  producer {
    kafka-clients = ${common-kafka-client-settings}
    restart-settings = ${default-restart-settings}
    kafka-clients {
      client.id = ipf-tutorial-client
    }
  }
  consumer {
    kafka-clients = ${common-kafka-client-settings}
    restart-settings = ${default-restart-settings}
    max-partitions = 10
    kafka-clients {
      group.id = ipf-tutorial-group
    }
  }
}
// end default kafka settings
Running the application
To run the application, the first thing we’ll need to do is setup the actual sanctions service that we will be talking to.
Docker Setup
If using docker, here’s the new entry for our application.yml (docker/application.yml)
  sanctions:
    image: registry.ipf.iconsolutions.com/sample-systems-sanctions-simulator-kafka:2.0.21
    container_name: sanctions-sim
    ports:
      - 5010:5005
      - 8088:55555
    environment:
      - SANCTIONS_MODE=normal
      - SANCTIONS_TRANSPORT=kafka
      - SANCTIONS_SIM_ENCRYPTION_ENABLED=FALSE
      - SANCTIONS_SIM_ENCRYPTION_KEYSTORE_PATH=file:///tmp/keys/connector/keystore-pkcs12-aes128.jks
    volumes:
      - ./config/keys:/tmp/keys:ro
      - ./config/sanctions:/sanctions-simulator-kafka/conf
      - ./logs/sanctions:/ipf/logs
    depends_on:
      - kafka
Note that the "2.0.21" version provided here is the latest version at the time of writing of this document.
Also note we are not setting any kafka configuration here, this is because the sample set provided above works with the docker environment.
To make things easier we’ll also add a logback.xml file for sanctions:
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <appender name="FILE"
              class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>/ipf/logs/sanctions-sim.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy">
            <fileNamePattern>/ipf/logs/sanctions-sim.log.%i</fileNamePattern>
            <minIndex>1</minIndex>
            <maxIndex>20</maxIndex>
        </rollingPolicy>
        <triggeringPolicy
                class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy">
            <maxFileSize>50MB</maxFileSize>
        </triggeringPolicy>
        <encoder>
            <pattern>%date{yyyy-MM-dd} %d{HH:mm:ss.SSS} %-5level %X{traceId} %logger{36} %X{sourceThread} %X{akkaSource} - %msg%n</pattern>
        </encoder>
    </appender>
    <root level="INFO">
        <appender-ref ref="FILE" />
    </root>
</configuration>
Non Docker Setup
Details for how to run the the sanctions simulator can be found here: Using the sanctions simulator
If required, also please ensure the config described above has the correct kafka setup for your environment.
Testing it all works
Now’s the time to check everything works, so let’s rebuild our application:
mvn clean install -rf :ipf-tutorial-app
And then we could send in a payment:
curl -X POST localhost:8080/submit | jq
And if we bring up the payment in the Developer GUI ('IPF Transaction Explorer') we can have a look at a couple of interesting things.
Firstly, if we go to the messages tab (search from the main page by unit of work id (uowId), click view on the transaction, click messages tab) we’ll see:
Here we can see that we now are recording the messages going both to (SENT) and from (RECEIVED) the sanctions system. If you "Click to view body" you can view the details of the transformed sanctions messages.
Secondly, just to confirm nothing else has changed we can look at the graph of our tutorial flow (click flows, click IptutorialFlowV2, click view graph) then we see:
And here we can see that our flow is working correctly!
Conclusions
In this tutorial we’ve achieved quite a lot! We’ve built our own kafka based connectors from scratch and used them to send messages to and from our remote service.
CON1 - Adding payment initiation
CON3 - Writing your own connector (Http)
------------------------------
CON3 - Writing your own Connector (HTTP)
Getting Started
The tutorial step uses the "add_kafka" solution of the project as it’s starting point.
If at anytime you want to see the solution to this step, this can be found on the "add_http" solution!
In CON2 - Writing your own connector (Kafka), we connected our application with an external sanctions system to make requests. To do this, we created our own Kafka connector and used it for communication to send requests and receive the responses over Kafka . In this tutorial, we’re going to change our protocols and look at using HTTP.
One important difference with the previous example, is that whereas when using Kafka we were using the connector in an asynchronous way, this time we need to use a synchronous HTTP call. This means that we need to use a different style of connector - the "RequestReplyConnector"
You’ll do this by integrating in with a test fraud system. This system:
Expects to receive a custom OlafRequest object.
Will return a custom OlafResponse object.
Supporting Classes
The first thing we’ll do is import the domain definition for the fraud system. To do this we need to add a dependency into our applications pom.xml:
<dependency>
    <artifactId>fraud-domain</artifactId>
    <groupId>com.iconsolutions.ipf.sample.samplesystems</groupId>
</dependency>
Let’s look at the key classes we receive from this module. The first is the request object we send to the fraud system.
@Data
public class OlafRequest extends SampleEvent<OlafRequestPayload> {
}
If we dig a little further we’ll find that the key element of the payload is the FraudRequest:
@Builder
@Data
@AllArgsConstructor
@NoArgsConstructor
public class FraudRequest {
    ObjectNode fiToFICstmrCdtTrf;
}
Here we can see that it is expecting to be provided with a pacs.008 in an object node format. This is just a jackson representation of the pacs008.
Mapping
In our Kafka example we used a pre-packaged mapper to convert from our pacs.008 to the OlafRequest. Here as an example of the different approaches available we’ll write our own mapper.
So let’s start by creating a mapping class that can take our pacs.008 and create our Fraud request for us. You’ll do this by creating a new "mappers" package and then adding a class for the "FraudMapper".
To create the FraudRequest object we’ll need to use the jackson mapper’s valueToTree method. We can then use this to construct our FraudRequest. Once we have this we wrap it in the OlafRequest object.
The olafRequest object itself expects a header which must contain a technical event id and a functional component (you can see this digging into the SampleEvent class and the Header class from samplesystems.shared.model). See if you can create this mapping method now, and when ready the solution is below:
@AllArgsConstructor
public class FraudMapper {
    private final ObjectMapper objectMapper;
    public OlafRequest mapToRequest(FIToFICustomerCreditTransfer fiToFICustomerCreditTransfer) {
        FraudRequest fraudRequest = new FraudRequest();
        fraudRequest.setFiToFICstmrCdtTrf(objectMapper.valueToTree(fiToFICustomerCreditTransfer));
        OlafRequest olafRequest = new OlafRequest();
        olafRequest.setHeader(Header.builder()
                .technical(Technical.builder().eventId("EventId").build())
                .functional(Functional.builder().build())
                .build());
        olafRequest.setPayload(new OlafRequestPayload(fraudRequest));
        return olafRequest;
    }
}
Note here that we’re not using any special frameworks to do our mapping as in this case it’s just a simple java mapping that is easiest.
Now let’s think about the response side. Firstly we’ll need a method to map from the body of the HTTP response to a FraudResponse object. For this again we’ll just use the object mapper:
public OlafResponse convertResponse(String messageText) {
    try {
        return objectMapper.readValue(messageText, OlafResponse.class);
    } catch (JsonProcessingException e) {
        throw new IconRuntimeException(e);
    }
}
Let’s add this method to our FraudMapper.
Finally, we’ll also need a method to map the OlafRequest object to a TransportMessage. In this case our transport message needs to include two things:
The olaf request itself as a string payload
A set of headers containing:
An "httpUrl" header containing the endpoint url to target
An "httpMethod" header, in this case just a constant "POST"
A "Content-Type" header, in this case just a constant "application/json"
For the endpoint URL we’ll retrieve it’s value from an injected HttpConnectorTransportConfiguration object. You’ll deal with the details of this when discussing the transport shortly, but for now all we need is to know we’ll use it’s getEndpointURL method ot retrieve the endpoint URL.
See if you can add the method to our FraudMapper, and then when ready the solution is below:
public TransportMessage mapToTransport(OlafRequest olafRequest) {
    try {
        MessageHeaders messageHeaders = new MessageHeaders(CryptoHelper.messageHeaders())
                .putHeader("httpUrl", fraudHttpConnectorTransportConfiguration.getEndpointUrl())
                .putHeader("httpMethod", "POST")
                .putHeader("Content-Type", "application/json");
        return new TransportMessage(messageHeaders, objectMapper.writeValueAsString(olafRequest));
    } catch (JsonProcessingException e) {
        throw new IconRuntimeException(e);
    }
}
We do need to use spring to provide to us an instance of the FraudMapper that we can use. To do this we’ll create a new bean within the IpfTutorialConfig class as:
@Bean
public FraudMapper fraudMapper(ObjectMapper objectMapper, HttpConnectorTransportConfiguration connectorTransportConfiguration) {
    return new FraudMapper(objectMapper, connectorTransportConfiguration);
}
That’s all of our mappings prepared, let’s go ahead and start creating our connector.
The Connector
Now that we have our mapper ready, we’re good to start building our connector.
Firstly, let’s add the dependency for the connector framework’s HTTP implementation.
<dependency>
    <groupId>com.iconsolutions.ipf.core.connector</groupId>
    <artifactId>connector-http</artifactId>
</dependency>
Now let’s write our connector, again you’ll need to make a few decisions first:
Types - The definition of a request-reply connector is RequestReplySendConnector<REQ_D, REQ_T, REP_D, REP_T>. In this instance:
the REQ_D represents the source (domain) object type
the REQ_T the target object type (which is sent to the HTTP service).
the REP_D represents the response (domain) object type (received back from the HTTP service)
the REP_T the target object type (returned from the connector’s send method).
In our connector will take in a FIToFICustomerCreditTransfer, transform it to an OlafRequest to send to the Fraud system and then return a OlafResponse. When we receive that OlafResponse we’ll leave it in that format (but we could map it to something else if we wanted!).
Logging - As with our previous example, we’ll make it as simple as possible and use the logging implementation that has been provided for us.
Mapping - Here we’ll use our mapper we discussed above!
Let’s start by thinking at the class level. You’ll create a new class within our connectors package for the FraudConnectorConfiguration.
Your class will need to access the actor system, the fraud mapper and the message logger implementation to support the creation of the request reply connector.
@Slf4j
@Configuration
@AllArgsConstructor
public class FraudConnectorConfiguration {
    private final ClassicActorSystemProvider actorSystem;
    private final MessageLogger messageLogger;
    private final FraudMapper fraudMapper;
    @Bean
    public RequestReplySendConnector<FIToFICustomerCreditTransfer, OlafRequest, OlafResponse, OlafResponse> fraudSendConnector(HttpConnectorTransport<OlafRequest> fraudHttpConnectorTransport) {
        return new RequestReplySendConnector.Builder<FIToFICustomerCreditTransfer, OlafRequest, OlafResponse, OlafResponse>("Fraud")
                .withActorSystem(actorSystem)
                .withMessageLogger(messageLogger)
                .withDomainToTargetTypeConverter(fraudMapper::mapToRequest)
                .withSendTransportMessageConverter(fraudMapper::mapToTransport)
                .withReceiveTransportMessageConverter(message -> fraudMapper.convertResponse(message.getPayload().toString()))
                .withConnectorTransport(fraudHttpConnectorTransport)
                .build();
    }
}
This is important enough to walk through each part in turn.
Firstly we pass into the method a HttpConnectorTransport<OlafRequest> fraudHttpConnectorTransport. This is an implementation of our HTTP connector transport. You’ll discuss creating that in a minute!
The builder construction - this takes a simple string of the name we want to provide of connector.
The actor system
the message logger - as discussed we’ll just use standard logging approaches here for now.
the domain to target type converter - here we’ll use our mapToRequest method we built on the FraudMapper, to map FIToFICustomerCreditTransfer to an OlafRequest.
the transport message converter - here we’ll use our mapToTransport method we built on the FraudMapper, to map the OlafRequest to a TransportMessage.
the receive transport type converter - here we’ll just supply a function that converts the message payload to a string and then uses our fraud mapper service to change into our OlafResponse.
the transport - obviously here we just use the connector transport we have passed to the method.
That’s it, that’s our very first send connector built from scratch.
You may wonder where the correlation controls are. In the request reply world, because everything is synchronous, the correlation features are a complexity we don’t have to worry about as everything is handled within the same thread!
That’s our connector defined, let’s now move onto the transports.
The Connector Transport
Now’s the time to set up the connector transport. In this case, it’s a HttpConnectorTransport you need.
@Configuration
@AllArgsConstructor
public class FraudTransportConfiguration {
    private ClassicActorSystemProvider actorSystem;
    @Bean
    public HttpConnectorTransportConfiguration fraudHttpConnectorTransportConfiguration() {
        return HttpConnectorTransportConfiguration.create(actorSystem.classicSystem().settings().config(), "fraud");
    }
    @Bean
    @SneakyThrows
    public HttpConnectorTransport fraudHttpConnectorTransport(HttpConnectorTransportConfiguration fraudHttpConnectorTransportConfiguration) {
        return new HttpConnectorTransport.Builder()
                .withName("OlafRequestReplyHttpConnectorTransport")
                .withActorSystem(actorSystem)
                .withTransportConfiguration(fraudHttpConnectorTransportConfiguration)
                .build();
    }
}
Using the connector
Now let’s look back at our FraudSystemActionAdapter. To make things simple in our current use case, you’ll simply adapt the existing success case (else) to send to the FraudSystem using the newly built HTTP connector, sending your pacs.008 to the fraud service. You’ll assume for brevity that any response is successful.
See if you can update the adapter now, and when ready the solution is below (the other else if conditions remain as before).
@Slf4j
public class FraudSystemActionAdapter implements FraudSystemActionPort {
    private RequestReplySendConnector<FIToFICustomerCreditTransfer, OlafRequest, OlafResponse, OlafResponse> fraudConnector;
    @Autowired
    public FraudSystemActionAdapter(RequestReplySendConnector<FIToFICustomerCreditTransfer, OlafRequest, OlafResponse, OlafResponse> fraudConnector) {
        this.fraudConnector = fraudConnector;
    }
    ...
        } else {
            return fraudConnector.send(action.getProcessingContext(),action.getCustomerCreditTransfer())
                    .thenCompose(response -> IpftutorialmodelDomain.fraudSystem().handle(new FraudCheckResponseInput.Builder(action.getId(), AcceptOrRejectCodes.Accepted).build())
                    .thenAccept((Done done) -> log.debug("Sent input of type {} for id {} with result {}", done.getCommandName(), action.getId(), done.getResult().name())));
        }
    }
}
You also need to make a modification to use the RequestReplySendConnector fraudConnector and the changed FraudSystemActionAdapter now that we added the FraudConnector to its constructor.
@Bean
  public IpftutorialmodelDomain init(ActorSystem actorSystem,
                   IsoMappingService mappingService,
                   SendConnector<FIToFICustomerCreditTransfer, SanctionsRequest> sanctionsConnector,
                   RequestReplySendConnector<FIToFICustomerCreditTransfer, OlafRequest, OlafResponse, OlafResponse> fraudConnector) {
    ...
                .withFraudSystemActionAdapter(new FraudSystemActionAdapter(fraudConnector))
    ...
That’s all our code done, next up let’s look at configuration.
Configuration
You’ll add our configuration into our application configuration file (ipf-tutorial-app/application.conf). Firstly, as per the discussion above we need to tell the connector what to do on failover. For this you will create a common restart configuration:
fraud {
  transport = http
  http {
    client {
      host = "localhost"
      port = "8084"
      endpoint-url = "/v1"
    }
  }
}
This configuration assumes that the fraud simulator will be running on the localhost and port 8084. This is what it will be when running from the command line (see non-docker setup). If we’re running from within docker, we’ll need to update only the host and port.
That’s everything from our application side complete.
Running the application
To run the application, the first thing you’ll need to do is set up the actual fraud service that you will be talking to. For this you need a new entry in the application.yml (docker/application.yml)
Docker Setup
  fraud-sim:
    image: registry.ipf.iconsolutions.com/sample-systems-fraud-simulator-http:2.0.40
    container_name: fraud-sim
    environment:
      - FRAUD_SIM_ENCRYPTION_ENABLED=FALSE
    ports:
      - 8089:8080
      - 8090:55555
    volumes:
      - ./config/fraud:/fraud-simulator-http/conf
      - ./logs:/ipf/logs
    user: "1000:1000"
Note that the "2.0.21" version provided here is the latest version at the time of writing of this document.
To make things easier you’ll also add a logback.xml file:
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <appender name="FILE"
              class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>/ipf/logs/fraud-sim.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy">
            <fileNamePattern>/ipf/logs/fraud-sim.log.%i</fileNamePattern>
            <minIndex>1</minIndex>
            <maxIndex>20</maxIndex>
        </rollingPolicy>
        <triggeringPolicy
                class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy">
            <maxFileSize>50MB</maxFileSize>
        </triggeringPolicy>
        <encoder>
            <pattern>%date{yyyy-MM-dd} %d{HH:mm:ss.SSS} %-5level %X{traceId} %logger{36} %X{sourceThread} %X{akkaSource} - %msg%n</pattern>
        </encoder>
    </appender>
    <root level="INFO">
        <appender-ref ref="FILE" />
    </root>
</configuration>
You’ll also need to update the application.conf in the docker setup to tell it the hostname of the new fraud simulator. To do this we simply add:
fraud {
  http {
    client {
      host = "fraud-sim"
      port = 8080
    }
  }
}
Non Docker Setup
Details for how to run the the fraud simulator can be found here: Using the fraud simulator
Testing it all works
Now’s the time to check everything works, so let’s rebuild the application:
mvn clean install -rf :ipf-tutorial-app
And then you can send in a payment:
curl -X POST localhost:8080/submit -H 'Content-Type: application/json' -d '{"value": "25"}' | jq
Note here you are sending in a value of 25. That’s to ensure you reach our happy path and call out to the Fraud Simulator.
And if you bring up the payment in the Developer GUI and look at the messages of the tutorial flow (search by unit of work ID, click view, click ipf tutorial flow, click messages) then you will see:
Here you can see that we are now also sending out OlafRequest fraud messages. If you look at the message data (Click to view body), you should see the Request message contains the full fiToFICstmrCdtTrf (pacs.008) and the response has the fraudFeedback payload.
Conclusions
In this section, we’ve established a new HTTP connection to an existing fraud server.
CON2 - Writing your own connector (Kafka)
TEST1 - Adding tests
------------------------------
TEST1 - Adding Tests
Getting Started
The tutorial step uses the add_http solution of the project as it’s starting point.
If at anytime you want to see the solution to this step, this can be found on the add_tests solution.
Upto now we’ve been focusing on experimenting with some of the different capabilities of the IPF Framework and how we can use them to quickly build up an application.
During that time, we haven’t looked at how we can test our application.
Here we’ll introduce Icon’s Test Framework and show how we can use it to test the application you’ve built.
We’ll assume in this tutorial a basic awareness of what the Test Framework is, and an understanding of both BDD and the Gherkin syntax.
The Icon Test Framework
Concepts
You’ll start our intro into the Test Framework by summarising some key concepts.
Message: An abstraction model for any 'message' that is handled by the framework implementation (request, response, payload etc).
A message is typed against a known Java type that represent the contents de-serialised form, also referred to as Document Type.
MessageType: A representation of the messages type that can be referred to through the BDD, there should be a one to one mapping between MessageType instance and a Messages associated Document type.
MessageDefinition: A contextual structure that provides functionality for handling messages of the configured type, serving as a point of Inversion of Control with the test-framework.
There should be a one-one mapping between the MessageDefinition instance and configured Message Type, and it is common to see both Message and MessageDefinition as arguments to core methods.
MessageBucket: A glorified collection that any messages received by the test-framework (either directly from Consumers, or secondary such as HTTP responses) are appended to.
The internal collection is encapsulated and a predicate-based accessor methods are provided in order to "fish" correlated messages from the bucket.
A successfully "fished" message is typically removed from the bucket and added to the test own Context object.
Transporter: An abstraction of a protocol on which a message may be sent to the target system e.g. HTTP, JMS etc
Context: A scenario context that holds test information and is accessible from any step, the internal data structure is thread local to facilitate parallelisation and the is cleared down between scenarios by JBehave lifecycle hooks.
Extensions
For this tutorial we’re going to use an extension on the Test Framework that is explicitly designed to make testing easier using the IPF product suite.
<dependency>
    <groupId>com.iconsolutions.ipf.core.test</groupId>
    <artifactId>ipf-test-fw-core</artifactId>
    <scope>test</scope>
</dependency>
The ipf-test-fw provides a number of useful things:
A set of pre-built steps that utilise the system events structure of an IPF application to provide rich processing steps that can be used for validation.
A set of pre-built steps that utilise the model operations capability to interrogate the actual aggregate of any given flow.
A set of common steps (scenario start / end)
A set of transporter utilities to allow easy set up of stubbed HTTP, Kafka and JMS services.
You’ll use these features throughout this tutorial.
Project Set Up
You’ll begin by putting the basics of a new project in place for the tests.
This will be a new Maven module which you’ll call ipf-tutorial-application-tests.
If you’re using IntelliJ you can do this by right-clicking on the ipf-tutorial project in the project view and selecting New  Module.
Then you should be prompted to add a new Maven module:
Then you should be prompted to add a new Maven module:
Press "Finish" to complete the project setup.
Once complete if you expand the module in the navigator, you can delete the ipf-tutorial-application-tests/src/main directory as we will only be working in the test folder here.
You’ll also a new directory "resources" under the ipf-tutorial-application-tests/src/test directory.
You’ll make this as a test resouces root (right click the folder > Mark Directory As > Test Resources Root).
Under the new resources directory we’ll add one more directory called "stories".
When complete our project structure should look like:
A First BDD Test
Now we have a project, let’s get on and start writing our first BDD test case.
To do this we need to create a "story" file.
Let’s create a new file called HappyPath.story and add it to the new stories directory.
There are some great plugins available within IntelliJ to help support the development of BDD test cases.
We recommend for example this one: IntelliJBehave.
When installed, it will provide ability to see which steps have already been implemented and provide click through capability to see the code.
Let’s now populate your story file:
Meta:
Narrative:
This test covers the basic tutorial happy path flow.
Scenario: Basic Happy Path
Given the IPF service is healthy
This is the basis of all the stories we’ll write for IPF.
The first line of the scenario "Given the IPF service is healthy" is one of the steps we’ll use for the ipf-test-fw capabilities to check that IPF has come up and is ready to process, this will ensure that all the connectors in the application are up and running before we start a test.
When running a test, if this step fails, always check the logs as it will tell you which connectors have failed.
This is normally down to a configuration error in your test!
Having confirmed our application is up and running, we need to start thinking about the different steps of our payment lifecycle.
You’ll do this in as minimal way as possible over this tutorial and then build out on that later.
Hence the summary below, is not an exhaustive list of all the options and abilities of the test-fw, nor a full test but simply and introduction to get us started!
1. A payment is sent in (we’ll assume via the HTTP controller for now)
When the channel sends a 'initiation request' with values:
| requestId | fraud-happy-request-id |
| value | 25 |
This is an important line to digest and understand.
Firstly, we use the term "channel" here, but we use this term to represent the invoker of the HTTP initiation call.
We then define the type of request we want to send in "initiation request" and we provide two values: the requestId (containing value 'fraud-happy-request-id') and the value (containing 25).
The value is easy to understanding, we are sending in a value < 30 to ensure we hit the right scenario.
The request ID will be used by our test to track the specific scenario we are running here.
The test framework is capable of running many tests in paralell, so we need to be able to uniquely identify our tests and it is this value that we will do that with.
It’ll become more obvious how this is used later!
2. An initiation flow is created.
Then a new InitiationFlow flow is started
3. The HTTP response is returned to the initiating call.
And the channel receives a 'initiation response'
4. The initiation flow creates a new ipftutorialv2 flow.
And a new IpftutorialflowV2 flow is started
5. The ipftutorialv2 flow process a couple of no-ops (duplicate check, account validation) and then calls the sanctions system.
And Sanctions receives a 'sanctions request'
So here we’re telling our test that the sanctions system must receive a new message type "sanctions request"
6. The sanctions system returns a response.
When Sanctions sends a 'sanctions response'
Again here we can see we’re using another definition of a message the "sanctions response".
7. If the payment value < 30, a call is made to the fraud system
Then Fraud receives a 'fraud request'
8. The fraud system returns a response.
When Fraud sends a 'fraud response'
9. The ipftutorialv2 flow uses sample no-op adapters to complete (clear and settle)
Then the 'InitiationFlow' flow is in state 'Complete'
10. The execution flow returns control to the initiation flow which also completes.
And the 'IpftutorialflowV2' flow is in state 'Complete'
Putting this all together we have our first full BDD test!
Meta:
Narrative:
E2E test to demonstrate testing a flow
Scenario: Execute Fraud Happy
Given the IPF service is healthy
When the channel sends a 'initiation request' with values:
| requestId | fraud-happy-request-id |
| value | 25 |
Then a new InitiationFlow flow is started
And the channel receives a 'initiation response'
And a new IpftutorialflowV2 flow is started
And Sanctions receives a 'sanctions request'
When Sanctions sends a 'sanctions response'
Then Fraud receives a 'fraud request'
When Fraud sends a 'fraud response'
Then the 'InitiationFlow' flow is in state 'Complete'
And the 'IpftutorialflowV2' flow is in state 'Complete'
It’s quite a simple test for now, and we’ll look to add some complexity and different elements of testing within it during the course of this tutorial, but for now this will be our starting test that we’re going to run.
Test Implementation
The first thing we need to do is to add our dependency to the ipf-test-fw as discussed.
You’ll also need to tell the code where the application code itself is, so we’ll need to add a dependency to the ipf-tutorial-app itself too.
<dependencies>
    <dependency>
        <groupId>com.iconsolutions.ipf.tutorial</groupId>
        <artifactId>ipf-tutorial-app</artifactId>
        <version>${project.version}</version>
        <scope>test</scope>
    </dependency>
    <dependency>
        <groupId>com.iconsolutions.ipf.core.test</groupId>
        <artifactId>ipf-test-fw-core</artifactId>
         <scope>test</scope>
    </dependency>
</dependencies>
There are a number of key things we now need to build in order to get our tests to run:
A "runner" - this is a class that will provide the spring boot test runner that will execute all our story files.
"Config classes" for the different external services - we’ll need to tell the test framework how we want to stub out the real services that are used as part of our flow.
So in our case, we’re going to have to provide config for Fraud, Sanctions and payment initialisation.
"Config" files - just like we did in our docker environment, we’ll need to supply
Let’s look at each of these now.
The Runner
The runner actually runs our tests, it’s responsibility is to determine all the available story files and execute the scenarios within them.
So let’s set up a new class like the below:
@SpringBootTest(classes = Application.class, webEnvironment = SpringBootTest.WebEnvironment.DEFINED_PORT)
@Import({AllTestConfig.class})
public class FeatureTestRunner extends IPFFeatureTestRunner {
}
You’ll add this into a new package com.iconsolutions.ipf.tutorial.test
The runner we’re using here extends the out of the box IPF Feature Test runner.
We’re telling it that we’re going to use our Application.class (from the ipf-tutorial-app project) as the basis of our spring boot test.
Note that we’re providing it the AllTestConfig import, this will enable a number of features we’ll use in setting up the running of our test, and we’ll discuss these later.
For our test, we’re going to use mongo as our database and Kafka as the transport layer for sanctions, so we need to consider how we will use those.
In this tutorial we’re going to use test containers to supply us with the docker implementations of both that the test will bootstrap as part of it’s execution.
To add these is really simple, firstly we need to change our class definition for the runner:
public class FeatureTestRunner extends IPFFeatureTestRunner implements KafkaIntegrationTestSupport, MongoIntegrationTestSupport {
So here you can see we’ve added test support for both Kafka and Mongo.
Now we also need to tell Kafka which topics we want to create and then start the containers.
You’ll do this within a static block within our runner:
static {
    kafkaContainer.withEnv("KAFKA_CREATE_TOPICS", "SANCTIONS_RESPONSE:1:1,SANCTIONS_REQUEST:1:1,PAYMENT_INITIATION_REQUEST:1:1,PAYMENT_INITIATION_RESPONSE:1:1");
    SingletonContainerPattern.startAllInParallel(mongoContainer, kafkaContainer);
}
So here we’re telling our tests which topics to create (for sanctions and the payment initiation over Kafka) and then we’re simply starting the two containers.
That’s everything on our runner for now, we’ll make add in a couple of extra config files later to complete it.
Configuration Classes
Payment Initiation
As discussed above, for now we’re going to use the HTTP use case for payment initiation.
In our BDD, we defined to messages the "initiation request" and the "initiation response".
You’ll need to create types and definitions for how the test framework should handle these requests.
Let’s create a new package called config.
In here we will create a class called InitiationConfig.
The first thing we need to do in our initiation config is to define the message types.
These message types need to implement the MessageType interface.
So let’s construct a new enum within our config file for the message types:
public enum InitiationTypes implements MessageType {
    INITIATION_REQUEST("initiate request"),
    INITIATION_RESPONSE("initiation response");
    private final String name;
    InitiationTypes(String aName) {
        name = aName;
    }
    @Override
    public String getName() {
        return name();
    }
    @Override
    public Set<String> getAliases() {
        return Sets.newHashSet(name);
    }
}
Here we can see we’re defining two message types.
The key bit to note here is that the names provided in the constructor for the types must match the names provided in your BDD story file.
Having set up a message type, we now need to define the messages.
For each message type we have setup (so both our initiationrequest and response in this case) we need to create a new "MessageDefinition" for that type.
You’ll start with the initiation response as it’s slightly easier:
@Bean
MessageDefinition<InitiationResponse> initiationResponseMessageDefinition() {
    return new DefaultMessageDefinition.Builder<InitiationResponse>()
            .withType(InitiationTypes.INITIATION_RESPONSE) (1)
            .withCausedByType(InitiationTypes.INITIATION_REQUEST) (2)
            .withDocumentTypeClass(InitiationResponse.class) (3)
            .withCorrelatingIdGet(doc -> Optional.ofNullable(doc.getDocument().getRequestId())) (4)
            .build();
}
Let’s walk through the key points here:
1
Here we are defining the type that this definition applies too, so that’s simple it’s the initiation response one.
2
As this is a response, we define the type that this message has been caused by - i.e. the request message.
This allows correlation to take place automatically over http.
3
Here we are definining the actual java type of the class, so again a simple one that the response that comes back from the initiation controller is a InitiationResponse.
4
Here we need to define a function that tells us how to correlate this response with the initial request.
So how will the test framework be able to know that a response and request are linked.
We’ll simply do this by using the request ID field - so here we are saying that the "request id" on the "InitiationResponse" object will contain the same value that we will define on the initiation request definition next.
Let’s now look at the request definition:
@Bean
MessageDefinition<InitiationRequest> initiationRequestMessageDefinition(@Value("${application.base.url}") String baseUrl) {
    return new DefaultMessageDefinition.Builder<InitiationRequest>()
            .withType(InitiationTypes.INITIATION_REQUEST) (1)
            .withDocumentTypeClass(InitiationRequest.class) (2)
            .withGenerator(props -> new InitiationRequest()) (3)
            .withCorrelatingIdGet(doc -> Optional.ofNullable(doc.getDocument().getRequestId())) (4)
            .withDestination(baseUrl + "/submit") (5)
            .withPreSend(message -> {
                ContextUtils.setClientRequestId(message.getDocument().getRequestId()); (6)
            })
            .build();
}
Again lets walk through each part of this definition:
1
This time our definition is for the request message type.
2
This time our definition is for actually sending the request so we’re using the InitiationRequest object.
3
We need to supply a generator.
This generator tells the test framework that when it needs to send an initiation request it will construct it using this generator method.
In our case, we just want a simple brand new InitiationRequest object.
You’ll use a more complicated generator definition later when looking at fraud.
4
Similarly to the response definition, we need to supply the correlation approach.
So here again we’re just going to use the request id.
So the two correlation functions (on the request and response) together mean that the request ID on the request object must match the request ID on the response object for the test framework to know they are the related pair.
5
For the destination, this is the HTTP address that the initiation request will be sent to.
Note here we are injecting the url path by properties so we’ll need to add a property for this into our application.conf file.
You’ll do this in the configuration file section below.
6
In the pre-send we can set any extra things that need to be done to help the test framework before the message is sent out.
In our case, we’re going to set the local request ID definition on the test to the one on our message.
This request id will then be available for use throughout our scenario - but will be unique to our scenario.
This is key in giving us the ability to process multiple scenario’s concurrently.
That’s our definitions done, so the final part of our initiation setup is to set the transport up, i.e. we need to provide the test framework with a mechanism to make a call to the initiation controller.
To do this we’re going to use another test-fw utility, the HttpSenderTestTransporter.
Let’s create another bean for it as follows:
@Bean
public MessageTransport initiationTransport(MessageDefinition<InitiationRequest> initiationRequestMessageDefinition, MessageDefinition<InitiationResponse> initiationResponseMessageDefinition) {
    return new HttpSenderTestTransporter.Builder<InitiationRequest, InitiationResponse>()
            .withIdentifier("initiation")
            .withRequestMessageDefinition(initiationRequestMessageDefinition)
            .withResponseMessageDefinition(initiationResponseMessageDefinition)
            .build();
}
So this is really simple, we construct a new instance and provide a unique identifier (if we have multiple sender transports each one will need a unique id, you could leave this blank but for tracing any issues a known name is better!).
We also provide access to the request and response definition we set up.
The transport will extract all the other information it needs from those message definitions!
That’s everything done, from an initiation viewpoint we’re all complete and ready to start testing.
Let’s now look at sanctions.
The Sanctions Definition
Now we move onto sanctions, the steps are essentially the same:
Create the message types
Create the definitions for the request and response
Define the transport.
The only difference here is this time rather than using the HttpSenderTransport, we’ll use a KafkaMessageTransport.
So let’s create a new class for our "SanctionsConfig".
Firstly let’s create our message type enum.
Remember from our BDD we have defined the "sanctions request" and "sanctions response" messages.
See if you can create our enum now and when ready the solution is below:
public enum SanctionsTypes implements MessageType {
    SANCTIONS_REQUEST("sanctions request"),
    SANCTIONS_RESPONSE("sanctions response");
    private final String name;
    SanctionsTypes(String aName) {
        name = aName;
    }
    @Override
    public String getName() {
        return name();
    }
    @Override
    public Set<String> getAliases() {
        return Sets.newHashSet(name);
    }
}
Now let’s think about our request definition.
The first thing to realise is that from the test framework perspective it’s going to "receive" the sanctions request.
So things are effectively reversed when considering the test framework - i.e. when we use a send connector to send out the message the test framework will receive it.
It’s also going to receive it from a Kafka topic, so we’ll need to provide a mechanism to convert from the serialised string on the topic to the actual Sanctions Request object.
Let’s see how that all works:
@Bean
MessageDefinition<SanctionsRequest> receiveSanctionsRequest() {
    return new DefaultMessageDefinition.Builder<SanctionsRequest>()
            .withType(SanctionsTypes.SANCTIONS_REQUEST) (1)
            .withDocumentTypeClass(SanctionsRequest.class) (2)
            .withSource("SANCTIONS_REQUEST") (3)
            .withFromStringMapper(s -> SerializationHelper.stringToObject(s, SanctionsRequest.class)) (4)
            .withCorrelatingIdGet(doc -> Optional.ofNullable(ContextUtils.getCorrelatingId())) (5)
            .build();
}
1
Here again we define our type.
2
And the java object.
3
The source field represents the Kafka topic we’re going to read from, note that it’s called source here as message definitions are protocol independent, so this would be the same definition in a jms world (replacing the topic idea for a queue name)
4
The fromStringMapper defines how we’re going to convert from the serialized string version of our message to our java class.
In our case, we’re just going to use a pre-defined stringToObject function (this is available from Icon’s SerializationHelper) which will do a simple jackson mapping.
5
This function tells the test framework where to get the correlating ID from which is used to match the response to the request message
Next is our response definition, like with payment initiation the key here is that we will also need to provide a generator function to create the new sanctions response.
@Bean
MessageDefinition<SanctionsResponse> sendSanctionsResponse() {
    return new DefaultMessageDefinition.Builder<SanctionsResponse>()
            .withType(SanctionsTypes.SANCTIONS_RESPONSE)
            .withDocumentTypeClass(SanctionsResponse.class)
            .withDestination("SANCTIONS_RESPONSE")
            .withGenerator(props -> {
                SanctionsResponse sanctionsResponse = new SanctionsResponse();
                sanctionsResponse.setHeader(HeaderUtils.makeHeader("Sanctions", ContextUtils.getClientRequestId()));
                sanctionsResponse.setPayload(new SanctionsResponsePayload());
                sanctionsResponse.getHeader().getTechnical().setOriginalEventId(((SanctionsRequest) PreviousMessages.getLastMessage(SanctionsTypes.SANCTIONS_REQUEST, false).getDocument()).getHeader().getTechnical().getEventId());
                return sanctionsResponse;
            })
            .withCorrelatingIdGet(doc -> Optional.ofNullable(ContextUtils.getClientRequestId()))
            .build();
}
Here we can see we create a new response object and add a default header to it.
The most interesting part is that we set the original event Id…​ WHY?
Finally we need to consider the transport again.
This time we’ll just use a KafkaMessageTransport implementation.
Let’s look at this:
@Bean
public MessageTransport sanctionsKafkaTransport(MessageDefinition<SanctionsRequest> sanctionsRequestMessageDefinition,
MessageDefinition<SanctionsResponse> sanctionsResponseMessageDefinition,
ClassicActorSystemProvider actorSystem) {
    return new KafkaTestTransporter.Builder<SanctionsRequest,SanctionsResponse>()
            .withIdentifier("sanctions") (1)
            .withPropertiesPath("sanctions") (2)
            .withRequestMessageDefinition(sanctionsRequestMessageDefinition) (3)
            .withResponseMessageDefinition(sanctionsResponseMessageDefinition) (4)
            .withActorSystem(actorSystem) (5)
            .build();
}
Let’s take a look at the key parts of this:
1
Again we provide a unique identifier for the transport.
2
For Kafka we’re going to retrieve our configuration from a properties file, we’ll look at the details of the configuration in the next section, here all we need to do is specify the path within the config file that our sanctions properties will be.
3
Here is simply our request message definition
4
Here is simply our response message definition
5
And finally we pass the actor system
That’s all our sanctions setup done, so let’s move onto Fraud.
The Fraud Definition
As normal, we’ll create a FraudConfig class and then start by adding the fraud message types and definitions.
T
Just like with Sanctions, the main complexity is the generation of the response object.
The most important point on the generation of the olaf response is that we need to ensure that the originalEventId field contains the original id from incoming request message.
This is required in order to be able to correlate.
private OlafResponse createAndEnrichResponseFrom(OlafRequest olafRequest) {
    String originalId = olafRequest.getHeader().getTechnical().getEventId();
    com.iconsolutions.samplesystems.shared.model.header.Header header = Header.copy(olafRequest.getHeader());
    header.getTechnical().setEventId(UUID.randomUUID().toString());
    header.getTechnical().setOriginalEventId(originalId);
    header.getTechnical().setEventType("FraudFeedback");
    OlafResponse response = new OlafResponse();
    response.setHeader(header);
    response.setPayload(new FraudFeedbackPayload(new FraudFeedback(new FraudFeedback.PaymentStatusChanged("FraudFeedbackOK", "0"), null)));
    return response;
}
The next key point to consider is how we find the olaf request to send to our message.
For this we’re going to use the "PreviousMessages" capability of the test framework which allows us to retrieve the last fraud request message.
That’s all the information we need to create our definitions.
See if you can do this and when ready the solution is below:
@Bean
MessageDefinition<OlafResponse> fraudResponseMessageDefinition() {
    return new DefaultMessageDefinition.Builder<OlafResponse>()
            .withType(FraudTypes.FRAUD_RESPONSE)
            .withCausedByType(FraudTypes.FRAUD_REQUEST)
            .withDocumentTypeClass(OlafResponse.class)
            .withCorrelatingIdGet(response -> Optional.of(ContextUtils.getClientRequestId()))
            .withGenerator((param) -> createAndEnrichResponseFrom((OlafRequest) PreviousMessages.getLastDocumentReceived(FraudTypes.FRAUD_REQUEST).getDocument()))
            .build();
}
@Bean
MessageDefinition<OlafRequest> fraudRequestMessageDefinition() {
    return new DefaultMessageDefinition.Builder<OlafRequest>()
            .withType(FraudTypes.FRAUD_REQUEST)
            .withDocumentTypeClass(OlafRequest.class)
            .withFromStringMapper(s -> SerializationHelper.stringToObject(s, OlafRequest.class))
            .withCorrelatingIdGet(fraudRequest -> Optional.of(ContextUtils.getClientRequestId()))
            .build();
}
public enum FraudTypes implements MessageType {
    FRAUD_REQUEST("fraud request"),
    FRAUD_RESPONSE("fraud response");
    private final String name;
    FraudTypes(String aName) {
        name = aName;
    }
    @Override
    public String getName() {
        return name();
    }
    @Override
    public Set<String> getAliases() {
        return Sets.newHashSet(name);
    }
}
Now for transports, we’ll use the FraudConsumerTransport.
Note that this is effectively the opposite way round to the way we were working in the payment initiation space.
There we were sending a message to IPF and reading the response.
Here IPF is sending us a message and reading the response.
Hence it is a consumer transport and not a sender one as in the payment initiation case.
@Bean
    public MessageTransport fraudTransport(FlowIdentifiersFinder finder,
                                           @Value("${fraud.http.client.port}") String port,
                                           MessageDefinition<OlafRequest> fraudRequestMessageDefinition,
                                           MessageDefinition<OlafResponse> fraudResponseMessageDefinition,
                                           ClassicActorSystemProvider actorSystem) {
        return new HttpConsumerTestTransporter.Builder()
                .withIdentifier("fraud") (1)
                .withPort(Integer.parseInt(port)) (2)
                .withOperation(new HttpOperation.Builder<>("v1", fraudRequestMessageDefinition, fraudResponseMessageDefinition).withHttpMethod(HttpMethod.POST).build()) (3)
                .withActorSystem(actorSystem)
                .build();
    }
The transport here is very similar to the other test transports we’ve done to date.
The key differences are:
1
We need to define the identifier
2
We need to define the port which it will be on.
3
We need to define the function that needs to be performed, i.e. the 'HttpOperation'.
To construct this we use the builder and specify these four parameters:
The path of the function
The method type it will accept (POST, GET or PUT)
The request message type
The response message type
That’s all our fraud setup done.
Processing Data
The last bit of configuration we have to do is to provide something that will listen to the processing data.
You’ll do this by adding a simple wiremock consumer.
<dependency>
    <groupId>com.github.tomakehurst</groupId>
    <artifactId>wiremock</artifactId>
    <version>2.27.1</version>
</dependency>
<dependency>
    <groupId>com.github.tomakehurst</groupId>
    <artifactId>wiremock-jre8-standalone</artifactId>
    <version>2.27.1</version>
</dependency>
@Configuration
public class DummyODSConsumer {
    @Bean
    public WireMockServer odsMock(ObjectMapper objectMapper) throws JsonProcessingException {
        WireMockServer wireMockServer = new WireMockServer(
                new WireMockConfiguration()
                        .port(8093)
                        .needClientAuth(true)
        );
        wireMockServer.start();
        wireMockServer.stubFor(WireMock.post(WireMock.urlEqualTo("/ipf-processing-data"))
                .willReturn(WireMock.aResponse()
                        .withStatus(200)));
        return wireMockServer;
    }
}
Here we are simply ignoring and returning a success response for the ODS call.
Updating the Runner
Now we have all our configuration files, we need to add them to our runner:
@SpringBootTest(classes = Application.class, webEnvironment = SpringBootTest.WebEnvironment.DEFINED_PORT)
@Import({AllTestConfig.class, FraudConfig.class, SanctionsConfig.class, InitiationConfig.class, DummyODSConsumer.class})
public class FeatureTestRunner extends IPFFeatureTestRunner implements KafkaIntegrationTestSupport, MongoIntegrationTestSupport {
Executing our Test
Now we should just be able to run our test by right clicking on the FeatureTestRunner class and clicking run.
Once run we should see something like:
Running in Maven
Finally, if we want to run in Maven we need to add an extra build plugin to ensure they are executed.
For this, we simply add the Maven failsafe plugin to our pom.xml as follows:
    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-failsafe-plugin</artifactId>
                <version>2.22.2</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>integration-test</goal>
                            <goal>verify</goal>
                        </goals>
                    </execution>
                </executions>
                <configuration>
                    <includes>
                        <include>**/*Runner.java</include>
                    </includes>
                    <excludes>
                        <exclude>**/*Test.java</exclude>
                        <exclude>**/*Tests.java</exclude>
                        <exclude>**/*InProgressRunner.java</exclude>
                        <exclude>**/*RepeatRunner.java</exclude>
                    </excludes>
                </configuration>
            </plugin>
        </plugins>
    </build>
Conclusions
In this section, we’ve set up and run a basic test fw test.
CON3 - Writing your own connector (Http)
RES1 - Resiliency and retry settings (HTTP)
------------------------------
RES1 - Resiliency and Retry Settings (HTTP)
Getting Started
The tutorial step uses the add_http solution of the project as it’s starting point.
If at anytime you want to see the solution to this step, this can be found on the resiliency solution.
In CON3 - Writing your own connector (HTTP), we connected our application with an external test fraud systems. This gave us an synchronous connection to an external system which is inherently less stable than using Kafka or JMS. And our landscape at this point in the tutorials looks like;
In this tutorial we are going to look at how we can control the resiliency and retry settings in a best effort to allow the HTTP call to be successful. We will do this by simulating failures of the fraud-sim such that HTTP calls to that service will fail.
Starting the Application
If the environment is not running, we need to start up our docker environment. To do this from the docker directory of the project, run the following command:
docker-compose -f application.yml up -d
This should start all applications and simulators. We can check whether the containers are started and healthy using the command:
docker-compose -f application.yml ps
Validate BAU Processing
Lets check everything is working BAU first will all simulator end points up and functioning, send in a payment:
curl -X POST localhost:8080/submit -H 'Content-Type: application/json' -d '{"value": "25"}' | jq
Checking the payment in the Developer GUI we can see the messages being sent and spot the OlafRequest & OlafResponse messages to the fraud-sim (search by unit of work id, click view, click ipf tutorial flow, click messages) then we see:
Failure Scenario Test
Assuming all is well with the BAU processing, lets test the scenario where the fraud-sim is down and OlafResponses are not coming back. The easiest way to do this is to stop the fraud-sim container:
docker stop fraud-sim
Once the container is down we can send in another payment request:
curl -X POST localhost:8080/submit -H 'Content-Type: application/json' -d '{"value": "24"}' | jq
Checking the payment in GUI again you should see the OlafRequest being sent but not OlafResponse coming back and the status of the transaction itself shows as PENDING:
Finally from the GUI we can see the system event which has been generated for this failure:
Its also worth check the container logs to see the exception and the specific errors (this will become important as we configure the service to retry the HTTP call). You will note there are no more errors, processing is effectively stalled with our current configuration:
2022-10-27 09:43:17.245 ERROR  c.i.i.c.c.RequestReplySendConnector   - Fraud Send connector returned a response without associated response, response: DeliveryOutcome(deliveryReport=DeliveryReport(outcome=FAILURE, deliveryException=java.util.concurrent.CompletionException: java.lang.IllegalStateException: No closed routees for connector: Fraud. Calls are failing fast), response=null)
2022-10-27 09:43:17.246 ERROR  c.i.i.c.c.RequestReplySendConnector   - Fraud Received a failure for UnitOfWorkId(value=5f97e12d-8bdd-4fbe-92c3-cbe9e2be58a6) with reason java.lang.IllegalStateException: java.util.concurrent.CompletionException: java.lang.IllegalStateException: No closed routees for connector: Fraud. Calls are failing fast
java.util.concurrent.CompletionException: java.lang.IllegalStateException: java.util.concurrent.CompletionException: java.lang.IllegalStateException: No closed routees for connector: Fraud. Calls are failing fast
Configure Timeout and Resiliency Settings
As things stand with the tutorial application it is not proactively configured for retry and has not set the resiliency settings to protect against intermittent errors on the HTTP synchronous connection. It is possible however to define resiliency settings to retry the HTTP call within a defined period and at configurable intervals. The default configuration is shown below, including both the connector settings and the resiliency settings.
Now we’ll update the max-attempts to be 6 which is intended to give sufficient retries of the HTTP call to allow the fraud-sim service to recover (attempts of 6, together with the backoff-multiplier of 2 seconds should give 5 attempts before the call-timeout of 30 seconds)
You’ll add our configuration into our application configuration file (ipf-tutorial-app/application.conf):
fraud {
  transport = http
  http {
    client {
      host = "fraud-sim"
      port = "8080"
      endpoint-url = "/v1"
    }
  }
  resiliency-settings {
    max-attempts = 6
  }
}
And we have to update the FraudConnectorConfiguration to provide the resiliency settings from the application config when building the RequestReplySendConnector.
...
        return new RequestReplySendConnector.Builder<FIToFICustomerCreditTransfer, OlafRequest, OlafResponse, OlafResponse>(
               "Fraud",
<1>                "fraud",
<2>                actorSystem
            ).withMessageLogger(messageLogger)
    }
There are two changes we have made here, both to add extra arguments to the constructor:
1
- We have defined the root of the properties we want to apply to our connector.  So here we use "fraud".
2
- We supply the actor system
Note that this also contains retryOnFailureWhen to effectively retry all failures as configured. We can do other things here to retry based on certain Exceptions, for example;
...
                .retryOnFailureWhen(this::isRetryableFailure)
Where isRetryableFailure is the result of a test against a defined list of exceptions you want to retry.
Failure Scenario Test 2
Now we can apply this configuration by rebuilding the ipf-tutorial-app container (mvn clean install -rf :ipf-tutorial-app) and starting it, then running through the following test steps:
GIVEN the fraud-sim is stopped && ipf-tutorial-app has resiliency settings to retry HTTP calls
WHEN a payment is initiated && the fraud-sim recovered within the 30 second connector timeout
THEN we the payment will complete processing with delay and retries evident in the logs
docker stop fraud-sim
curl -X POST localhost:8080/submit -H 'Content-Type: application/json' -d '{"value": "23"}' | jq
Wait 5 seconds (this will allow the Connector to retry).
docker start fraud-sim
If you are observing the ipf-tutorial-app logs (change the logback.xml for ipf-tutorial-app to have     <logger name="com.iconsolutions.ipf" level="DEBUG"/>
) and you should see retry entries like (note - this is the decision to retry the actual retry happens once the backoff period has expired):
2022-10-31 14:30:50.941 WARN   c.i.i.c.c.t.HttpConnectorTransport   - Failure reply for association ID [UnitOfWorkId(value=5931c9c4-66f0-4a87-964b-b00ff11954ee)] with exception [UnknownHostException: fraud-sim] and message [TransportMessage(, )]
2022-10-31 14:30:50.949 DEBUG  c.i.i.c.c.r.*ResiliencySettings*   - *retryOnResult decided to retry this attempt since it was a failure*: DeliveryReport(outcome=FAILURE, deliveryException=akka.stream.StreamTcpException: Tcp command [Connect(fraud-sim:8080,None,List(),Some(10 seconds),true)] failed because of java.net.UnknownHostException: fraud-sim)
(strictly speaking an UnknownHostException shouldn’t be retried but its a quick way to demonstrate retry processing and the resiliency settings).
Once the backoff period has passed the actual retry will take place:
2022-10-31 14:30:52.950 DEBUG  c.i.i.c.c.r.ResiliencyPassthrough   - Calling UNKNOWN : using OlafRequestReplyHttpConnectorTransport
Checking the payment in GUI again you should see the OlafRequest being sent, but the success response in the Messages tab appears after the delay (approximately 15 seconds).
A few things to note:
You can flexibly configure the retries by thinking about the backoff-multiplier & the initial-retry-wait-duration. For example
initialRetryWaitDuration
backoffMultiplier
First 5 attempt intervals
1
2
1, 2, 4, 8, 16
5
2
5, 10, 20, 40, 80
1
5
1, 5, 25, 125, 625
This retry happened within the 30 seconds connector timeout. Thus you should also be considering the call-timeout in conjunction with the resiliency settings.
As the tutorial is currently written, if the retry is not a success within that 30 seconds this will return to the flow and the fraud check won’t have been completed.
This is a good example of something which is short term transient and resolves itself quickly. Where that is not the case we have a number of options to configure additional transport end points, to "retry" from the flow by defining appropriate business logic in the IPF DSL.
We also have the options to react differently to actual business responses (using retryOnResultWhen), to retry on certain business error codes returned from the called application. But this should be balanced with how much logic you want at the connector level versus within the flow logic.
The resiliency component is implemented with resilience4j. See docs on the Resilience4j framework for more information on these settings and behaviours.
Conclusions
In this section, we’ve established potential options for configuring retries on the HTTP Connector. Next steps might be to explore Kafka connectors and other options to configure retry when the call-timeout is exceeded.
TEST1 - Adding tests
RUN1 - Running your application on Kubernetes
------------------------------
RUN 1 - Running on Kubernetes
Prerequisites
In order to run this tutorial you will need to run a kubernetes cluster locally:
Below are some possible options you could use:
Docker Desktop
minikube
Rancher desktop
Install and startup your desired solution for running a Kubernetes cluster locally
Kubernetes (K8s) is an open-source system for automating deployment, scaling and management of containerised applications.
Deploy IPF Tutorial on Kubernetes
For running the IPF tutorial as a clustered application on Kubernetes, we will do the following:
Step 1 - Create a Namespace
Step 2 - Create Service Account
Step 3 - Create Configmap
Step 4 - Create imagePullSecret
Step 5 - Create MongoDB
Step 6 - Create Developer App
Step 7 - Create a Deployment
Requirements
Step 1 - Create a Namespace
kubectl create namespace ipf-tutorial
Step 2 - Create Service Account
Create a 'serviceAccount.yaml' file and copy the following admin service account manifest.
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: ipf-tutorial
  namespace: ipf-tutorial
rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "watch", "list"]
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ipf-tutorial
  namespace: ipf-tutorial
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: ipf-tutorial
  namespace: ipf-tutorial
subjects:
  - kind: ServiceAccount
    name: ipf-tutorial
    namespace: ipf-tutorial
roleRef:
  kind: Role
  name: ipf-tutorial
  apiGroup: rbac.authorization.k8s.io
The 'serviceAccount.yaml' creates a ipf-tutorial role, 'ipf-tutorial' ServiceAccount and binds the 'Role' to the service account.
The 'ipf-tutorial' role has all the permissions to query the API for IPF pods running within the namespace.
Now create the service account using kubectl.
kubectl apply -f serviceAccount.yaml
Step 3 - Create Configmap
Create a Configmap file named 'configmap.yaml' and copy the following config manifest.
apiVersion: v1
kind: ConfigMap
metadata:
  name: ipf-tutorial-service-cm
  namespace: ipf-tutorial
data:
  application.conf: |
    akka {
      loglevel = "INFO"
      cluster {
        # undefine seed nodes to allow for Kubernetes to describe cluster topography
        seed-nodes = []
        sharding {
          distributed-data.majority-min-cap = 2
        }
      }
      # Use Kubernetes API to discover the cluster
      discovery {
        kubernetes-api {
          pod-label-selector = "app=%s"
        }
      }
      actor.provider = cluster
    }
    akka.remote.artery.canonical.hostname = ${POD_IP}
    akka.management {
      # available from Akka management >= 1.0.0
      health-checks {
        readiness-path  = "health/ready"
        liveness-path   = "health/alive"
      }
      # use the Kubernetes API to create the cluster
      cluster.bootstrap {
        contact-point-discovery {
          service-name              = "ipf-tutorial-service"
          discovery-method          = kubernetes-api
          required-contact-point-nr = 2
        }
      }
    }
    management.endpoints.web.exposure.include = "*"
    flow-restart-settings {
      min-backoff = 1s
      max-backoff = 5s
      random-factor = 0.25
      max-restarts = 5000
      max-restarts-within = 3h
    }
    ipf.behaviour.retries.initial-timeout = 3s
    ipf.behaviour.config.action-recovery-delay = 3s
    # CHANGEME
    ipf.mongodb.url = "mongodb://location_of_mongodb_cluster:27017/ipf_tutorial"
  logback.xml: |
    <?xml version="1.0" encoding="UTF-8"?>
    <configuration>
      <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <target>System.out</target>
        <encoder>
          <pattern>[%date{ISO8601}] [%level] [%logger] [%marker] [%thread] - %msg MDC: {%mdc}%n</pattern>
        </encoder>
      </appender>
      <appender name="ASYNC" class="ch.qos.logback.classic.AsyncAppender">
        <queueSize>8192</queueSize>
        <neverBlock>true</neverBlock>
        <appender-ref ref="CONSOLE" />
      </appender>
      <logger name="akka" level="WARN" />
      <root level="INFO">
        <appender-ref ref="ASYNC"/>
      </root>
    </configuration>
Now create the config map using kubectl.
kubectl apply -f configmap.yaml
Step 4 - Create imagePullSecret
Substitute docker-server, docker-username and docker-password for appropriate values
kubectl create secret docker-registry registrysecret --docker-server=**** --docker-username=********* --docker-password=******* --namespace ipf-tutorial
Step 5 - Create MongoDB
Create a statefulset file named 'infrastructure.yaml' and copy the following manifest. This creates MongoDB which is required by the tutorial application
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongo
  namespace: ipf-tutorial
spec:
  selector:
    matchLabels:
      role: mongo
  serviceName: "mongo"
  replicas: 1
  template:
    metadata:
      labels:
        role: mongo
    spec:
      imagePullSecrets:
        - name: "registrysecret"
      terminationGracePeriodSeconds: 10
      containers:
        - name: mongo
          image: https://hub.docker.com/_/mongo/tags/mongo:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 27017
        - name: mongo-exporter
          image: bitnami/mongodb-exporter:0.11.2
          imagePullPolicy: IfNotPresent
          ports:
            - name: mongo-exporter
              containerPort: 9216
              protocol: TCP
          env:
            - name: MONGODB_URI
              value: "mongodb://localhost:27017"
kubectl apply -f infrastructure.yaml
Step 6 - Create Developer App
Create a deployment file named developerApp.yaml and copy the following manifest. This creates the developer application which is required by the tutorial application to view flow events
${registry_service} - substitute for the location of the docker registry
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ipf-developer-service
  namespace: ipf-tutorial
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/path: "/"
    prometheus.io/port: "9001"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ipf-developer-service
      product: ipfv2
  template:
    metadata:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/path: "/"
        prometheus.io/port: "9001"
      labels:
        app: ipf-developer-service
        product: ipfv2
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - ipf-developer-service
              topologyKey: kubernetes.io/hostname
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      serviceAccountName: ipf-tutorial
      imagePullSecrets:
        - name: "registrysecret"
      containers:
        - name: ipf-developer-service
          image: $\{registry_service}/ipf-developer-app:latest
          imagePullPolicy: Always
          ports:
            - name: actuator
              containerPort: 8081
          env:
            - name: "POD_NAME"
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: "POD_IP"
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: "KUBERNETES_NAMESPACE"
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: "IPF_JAVA_ARGS"
              value: "-Dma.glasnost.orika.writeClassFiles=false -Dma.glasnost.orika.writeSourceFiles=false"
          resources:
            limits:
              memory: "2Gi"
            requests:
              memory: "2Gi"
              cpu: "1000m"
          volumeMounts:
            - mountPath: /ipf-developer-app/conf/logback.xml
              name: config-volume
              subPath: logback.xml
            - mountPath: /ipf-developer-app/conf/application.conf
              name: config-volume
              subPath: application.conf
      volumes:
        - name: config-volume
          configMap:
            name: ipf-developer-service-cm
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ipf-developer-service-cm
  namespace: ipf-tutorial
data:
  application.conf: |
    flow-restart-settings {
     min-backoff = 1s
     max-backoff = 5s
     random-factor = 0.25
     max-restarts = 5
     max-restarts-within = 10m
    }
    spring.data.mongodb.uri = ${?ipf.mongodb.url}
    actor-system-name = ipf-developer
    ipf.mongodb.url = "mongodb://mongo:27017/ipf"
    ods.security.oauth.enabled = false
    application.write.url="http://ipf-tutorial-app:8080"
    ipf.processing-data.ingress.transport=http
  logback.xml: |
    <?xml version="1.0" encoding="UTF-8"?>
    <configuration>
      <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <target>System.out</target>
        <encoder>
          <pattern>[%date{ISO8601}] [%level] [%logger] [%marker] [%thread] - %msg MDC: {%mdc}%n</pattern>
        </encoder>
      </appender>
      <appender name="ASYNC" class="ch.qos.logback.classic.AsyncAppender">
        <queueSize>8192</queueSize>
        <neverBlock>true</neverBlock>
        <appender-ref ref="CONSOLE" />
      </appender>
      <logger name="akka" level="WARN" />
      <root level="INFO">
        <appender-ref ref="ASYNC"/>
      </root>
    </configuration>
---
apiVersion: v1
kind: Service
metadata:
  name: ipf-developer-service
  namespace: ipf-tutorial
  labels:
    name: ipf-developer-service
spec:
  ports:
    - protocol: TCP
      port: 8081
      targetPort: 8081
      name: ipf-developer-service
  selector:
    app: ipf-developer-service
Step 7 - Create a Deployment
Create a Deployment file named 'deployment.yaml' and copy the following deployment manifest.
${registry_service} - substitute for the location of the docker registry
${tutorial-service-version} - version of tutorial app
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ipf-tutorial-service
  namespace: ipf-tutorial
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/path: "/"
    prometheus.io/port: "9001"
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ipf-tutorial-service
      product: ipfv2
  template:
    metadata:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/path: "/"
        prometheus.io/port: "9001"
      labels:
        app: ipf-tutorial-service
        product: ipfv2
    spec:
      #      affinity:
      #        podAntiAffinity:
      #          requiredDuringSchedulingIgnoredDuringExecution:
      #            - labelSelector:
      #                matchExpressions:
      #                  - key: app
      #                    operator: In
      #                    values:
      #                      - ipf-tutorial-service
      #              topologyKey: kubernetes.io/hostname
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      serviceAccountName: ipf-tutorial
      imagePullSecrets:
        - name: "registrysecret"
      containers:
        - name: ipf-tutorial-service
          image: ${registry_service}/ipf-tutorial-app:${tutorial-service-version}
          imagePullPolicy: Always
          ports:
            - name: actuator
              containerPort: 8080
            - name: akka-artery
              containerPort: 55001
            - name: akka-management
              containerPort: 8558
            - name: akka-metrics
              containerPort: 9001
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health/alive
              port: akka-management
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: akka-management
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 10
            timeoutSeconds: 1
          env:
            - name: "POD_NAME"
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: "POD_IP"
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: "KUBERNETES_NAMESPACE"
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: "IPF_JAVA_ARGS"
              value: "-Dma.glasnost.orika.writeClassFiles=false -Dma.glasnost.orika.writeSourceFiles=false"
          resources:
            limits:
              memory: "2Gi"
            requests:
              memory: "2Gi"
              cpu: "1000m"
          volumeMounts:
            - mountPath: /ipf-tutorial-app/conf/logback.xml
              name: config-volume
              subPath: logback.xml
            - mountPath: /ipf-tutorial-app/conf/application.conf
              name: config-volume
              subPath: application.conf
      volumes:
        - name: config-volume
          configMap:
            name: ipf-tutorial-service-cm
the affinity rules (commented out), if used are a way of specifying where/on which nodes pods should be scheduled. This can be useful for ensuring that pods are scheduled on different nodes for example.
In this IPF Kubernetes deployment we have used the following:
securityContext for IPF pod
Liveness and readiness probe to monitor the health of the IPF pod.
Application and logging configuration files stored as Kubernetes ConfigMap items.
Create the deployment using kubectl.
kubectl apply -f deployment.yaml
Check the deployment status.
kubectl get deployments -n ipf-tutorial
Now, you can get the deployment details using the following command.
kubectl describe deployments --namespace=ipf-tutorial
RES1 - Resiliency and retry settings (HTTP)
Create your own project
------------------------------
BUILD1 - Creating a New Project
Throughout the tutorials so far, we have taken the example IPF tutorial project and worked through our example based of that. The key point here however is that we started from a pre-existing IPF project that was all setup and ready to use. Now we consider the question - "How do we create a brand new IPF project?".
For this we introduce the IPF Archetype.
Introducing the Archetype
The archetype is a maven archetype project that will bootstrap a brand-new project. It will create a new project structure that should be very familiar from the tutorial namely:
A domain-root module that contains all the MPS setup. A brand-new flow is generated as part of this for you, the same one we saw in the initial solution of the tutorial!
A local-e2-test module that contains a basic testing setup for the project.
An <project>-app module that contains a spring boot based starter project that is hooked into the application.
This should be very familiar from the tutorials.
Running the Archetype
Let’s run the archetype.
The first thing to note is that when run, the archetype will generate a new folder from the directory you are currently in, this folder must not exist. You’ll also need to make sure your current directory does not include any pom files.
When ready we run this command to start the archetype:
mvn archetype:generate -DarchetypeGroupId=com.iconsolutions.ipf.core.archetype -DarchetypeArtifactId=icon-archetype -DarchetypeVersion=1.*.*
You must supply the target version of both the archetype (archetypeVersion) and use the compatible IPF version (ipfVersion).
The versions must be compatible, please check and replace the numbers based on IPF’s release documentation and/or the details for your target environment!
After a few seconds you should be prompted to provide the solution name.
Define value for property 'solutionName' (should match expression '^[A-Z][a-z0-9]+$'):
Here we note that it is telling us that the solution name must be alpha numeric and start with a capital letter, the rest being lower case. Let’s enter a name of "Mytest" and then press return. Then we’ll be prompted to:
Define value for property 'modelName' (should match expression '^[a-z]+$'):
Here we’re being asked for the mps model name, this time it can only contain a lower case alphabetic expression. Let’s enter a name of "mymodel" and then press return. Then we’ll be prompted to:
Define value for property 'flowName' (should match expression '^[a-zA-Z ]+$'):
Here we’re being asked for the mps flow name, this time it can be any upper or lower case alphabetic expression. Let’s enter a name of "Myflow" and then press return. Then we’ll be prompted to:
Define value for property 'artifactId' (should match expression '^[a-z]+$'):
Here we’re being asked to provide the Maven artifact id for the new project. Note that this will be the name of your overall project and it will also be the name of the directory into which all the application code is generated. As it’s a maven artifact id, it needs to follow all the same rules for it. Let’s enter a name of "mytest" and then press return. You’ll be prompted to:
Define value for property 'groupId' (should match expression '^[a-z]+$'):
Here we’re being asked to provide the Maven group id of the project. Let’s enter a name of "mygroup" and then press return. Then we’ll be prompted to:
Define value for property 'ipfVersion':
Here we’re being asked for the core IPF version you want to use. The version must be compatible with the Archetype version, please check and use the correct versions in IPF’s release documentation and/or the details for your target environment!
Let’s enter that and press return. Then we’ll be prompted to:
Define value for property 'version' 1.0-SNAPSHOT: :
Here we can provide the version for our new project, you can just press return to use the default provided (1.0-SNAPSHOT) but you can enter any valid Maven version number here. For now let’s just use the default and press return. You’ll then be prompted to:
Define value for property 'package' mygroup:
Here we can define the base package name for our application, by default we use the group id here but again if required you can overwrite it and provide your own value. Let’s just use the default again and press return.
That’s all the properties we need to enter, so at this point the archetype will print a list of all the properties it has been provided:
solutionName: Mytest
modelName: mymodel
flowName: Myflow
artifactId: mytest
groupId: mygroup
ipfVersion: 2023.1.0
includeApplication: y
includeMappings: n
includeSystemConnectors: n
includeMonitoring: y
groupId: mytest
artifactId: mytest
version: 1.0-SNAPSHOT
package: mygroup
Y: :
If these are correct, we can simply press return here to confirm. If they are not correct, we can enter an "N" and then press return which will allow us to review and update the properties. Let’s confirm it by pressing return.
Now the archetype will go off and start generating the project for you, when it’s complete you should see:
[INFO] Project created from Archetype in dir: /build/mytest
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  10.505 s
[INFO] Finished at: 2022-09-12T08:37:21+01:00
[INFO] ------------------------------------------------------------------------
So here we can see that I ran the archetype from the /build directory and as my artifactid was mytest it’s told you that it’s created the project in /build/mytest. Note that the time here show’s the total time taken to run the archetype that includes all the time entering the properties above. The actual generation of the archetype code itself should take no more than a few seconds.
Once your familiar with the archetype and it’s properties, you can use it in "batch" mode. By specifying the -B parameter and then entering all the required properties on the initial command, the archetype will run without any further manual intervention. For example, to do this for the project we’ve just created we could run (see Note above about choosing compatible versions):
mvn archetype:generate -B -DarchetypeGroupId=com.iconsolutions.ipf.core.archetype -DarchetypeArtifactId=icon-archetype  -DarchetypeVersion=1.*.* -DgroupId=mytest -DartifactId=mytest -Dversion=1.0-SNAPSHOT -DipfVersion=202*.*.* -DsolutionName=Mytest -DmodelName=mymodel -DflowName=Myflow
Let’s now look at the /build/mytest directory and we’ll see:
➜  mytest ls -ltr
total 44
drwxrwxr-x 2 bensavage bensavage  4096 Sep 12 08:39 bin
drwxrwxr-x 3 bensavage bensavage  4096 Sep 12 08:39 mytest-app
drwxrwxr-x 8 bensavage bensavage  4096 Sep 12 08:39 domain-root
-rw-rw-r-- 1 bensavage bensavage 10284 Sep 12 08:39 mvnw
-rw-rw-r-- 1 bensavage bensavage  2412 Sep 12 08:39 Readme.md
-rw-rw-r-- 1 bensavage bensavage  2769 Sep 12 08:39 pom.xml
-rw-rw-r-- 1 bensavage bensavage  6734 Sep 12 08:39 mvnw.cmd
drwxrwxr-x 3 bensavage bensavage  4096 Sep 12 08:39 local-e2e-test
So here we can see we have the core folders we discussed above, the domain-root, local-e2e-test and mytest-app.
There is also a Readme.md file the contains information on how to run the generated project, have a look at that now.
Having generated our project, let’s build it. Move to the base directory of the project and run:
mvn clean install
As with the main tutorial projects, this will generate all of our MPS code together with the docker images for the application itself. It will then run tests against the configured container environment to make sure everything is up and running ok. After a few minutes, we should get a successful build result.
That’s it, we’ve successfully generated an IPF project and are ready to get going. The normal next step would be to open up the flow in MPS and start building it out. To do this we simply open the domain-root/mps folder in our MPS install.
Conclusions
In this section we’ve learnt how to create a brand-new project.
RUN1 - Running your application on Kubernetes
Sanctions Simulator
------------------------------
UTILS3 - Sanctions Simulator
This document covers how to use the sanctions simulator. There are two recommended approaches for the tutorial, both docker and non-docker. Let’s look at these:
Using the Application Through Docker
Below provides a simple docker file entry for the developer application:
  sanctions-sim:
    image: registry.ipf.iconsolutions.com/sample-systems-sanctions-simulator-kafka:2.0.41
    container_name: sanctions-sim
    ports:
      - 5010:5005
      - 8088:55555
    environment:
      - SANCTIONS_MODE=normal
      - SANCTIONS_TRANSPORT=kafka
      - SANCTIONS_SIM_ENCRYPTION_ENABLED=FALSE
    volumes:
      - ./config/sanctions-sim:/sanctions-simulator-kafka/conf
      - ./logs:/ipf/logs
    user: "1000:1000"
    depends_on:
      - kafka
In addition, the application will need configuration which is placed in the application.conf file within a config directory relative to the above docker entry. The configuration file contains:
common-kafka-client-settings {
  bootstrap.servers = "kafka:9092"
}
akka.kafka {
  producer {
    kafka-clients = ${common-kafka-client-settings}
  }
  consumer {
    kafka-clients = ${common-kafka-client-settings}
  }
}
When executed the sanctions sim will listen for data on the SANCTIONS_REQUEST topic and post back to the SANCTIONS_RESPONSE. The logs will be exported to logs/ipf-developer.app relative to the docker file.
Using the Application Without Docker
Prerequisites
To run this simulator requires:
A Java 11 runtime
A Kafka environment that either has:
Auto topic creation enabled
Topics created for SANCTIONS_REQUEST and SANCTIONS_RESPONSE
Configuration Options
Property
Description
Default Value
common-kafka-client-settings.bootstrap.servers
Defines the server bootstrap settings for kafka.
localhost:9092
server.port
Defines the port the application will run on
8082
simulator.http.port
Defines the port upon which to listen for http requests.
Download
The sanctions simulator is available here:
nexus.ipf.iconsolutions.com/repository/ipf-releases-temp/com/iconsolutions/ipf/sample/samplesystems/sanctions-simulator-kafka/2023.1.0/sanctions-simulator-kafka-2023.1.0-runnable.jar
Running
Once downloaded, place it into this directory and run:
java -cp "sanctions-simulator-kafka-2023.1.0-runnable.jar:config" -D"loader.main"="com.iconsolutions.samplesystems.sanctions.simulator.SanctionsSimulatorKafkaApplication" "org.springframework.boot.loader.PropertiesLauncher"
The application will then start and be available from:
localhost:55555/index.html
Note that on Windows, the colon in the run command should be replaced by a semicolon.
Create your own project
Payment Initiation Simulator
------------------------------
UTILS2 - Payment Initiation Simulator
This document covers how to use the ipf-developer-app. There are two recommended approaches for the tutorial, both docker and non-docker. Let’s look at these:
Using the Application Through Docker
Below provides a simple docker file entry for the developer application:
  payment-initiation-simulator-kafka:
    image: registry.ipf.iconsolutions.com/sample-systems-payment-initiation-simulator-kafka:2.0.41
    container_name: payment-initiation-simulator-kafka
    ports:
      - "8082:55555"
    volumes:
      - ./config/payment-initiation-simulator-kafka:/payment-initiation-simulator-kafka/conf
    user: "1000:1000"
    depends_on:
      - kafka
In addition, the application will need configuration which is placed in the "application.conf" file within a config directory relative to the above docker entry. The conf file contains:
common-kafka-client-settings {
  bootstrap.servers = "kafka:9092"
}
connector {
default-receive-connector {
manual-start: true
}
  default-send-connector {
    manual-start = false
    call-timeout = 30s
    queue-size = 50
    max-concurrent-offers = 500
    resiliency-settings {
      minimum-number-of-calls = 1
      max-attempts = 1
      reset-timeout = 1s
      initial-retry-wait-duration = 1s
      backoff-multiplier = 2
    }
  }
  validator-error-handler-dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      fixed-pool-size = 4
    }
  }
}
akka.kafka {
producer {
kafka-clients = ${common-kafka-client-settings}
}
consumer {
kafka-clients = ${common-kafka-client-settings}
}
}
When executed the ipf-developer-app will listen for data on port 8082. This port will also be exposed to the host machine. The logs will be exported to logs/ipf-developer.app-log relative to the docker file.
Using the Application Without Docker
Prerequisites
To run this simulator requires:
A Java 11 runtime
A Kafka environment that either has:
Auto topic creation enabled
Topics created for PAYMENT_INITIATION_REQUEST and PAYMENT_INITIATION_RESPONSE
Configuration Options
Property
Description
Default Value
common-kafka-client-settings.bootstrap.servers
Defines the server bootstrap settings for kafka.
localhost:9092
server.port
Defines the port the application will run on
8082
Download
The payment initiation simulator is available here:
nexus.ipf.iconsolutions.com/repository/ipf-releases-temp/com/iconsolutions/ipf/sample/samplesystems/payment-initiation-simulator-kafka/2023.1.0/payment-initiation-simulator-kafka-2023.1.0-runnable.jar
Running
Once downloaded, place it into this directory and run:
java -cp "payment-initiation-simulator-kafka-2023.1.0-runnable.jar:config" -D"loader.main"="com.iconsolutions.samplesystems.paymentinitiation.simulator.PaymentInitiationSimulatorKafkaApplication" "org.springframework.boot.loader.PropertiesLauncher"
The application will then start and be available from:
localhost:55555/index.html
Note that on Windows, the colon in the run command should be replaced by a semicolon.
Sanctions Simulator
Fraud Simulator
------------------------------
UTILS4 - Fraud Simulator
This document covers how to use the fraud simulator. There are two recommended approaches for the tutorial, both docker and non-docker. Let’s look at these:
Using the Application Through Docker
Below provides a simple docker file entry for the developer application:
  fraud-sim:
    image: registry.ipf.iconsolutions.com/sample-systems-fraud-simulator-http:2.0.40
    container_name: fraud-sim
    environment:
      - FRAUD_SIM_ENCRYPTION_ENABLED=FALSE
    ports:
      - 8089:8080
      - 8090:55555
    volumes:
      - ./config/fraud-sim:/fraud-simulator-http/conf
      - ./logs:/ipf/logs
    user: "1000:1000"
The fraud simulator requires no specialised configuration beyond the defaults. When executed the fraud-simulator will listen for data on port 55555 on the docker network. This is mapped to port 8090 on the host machine. The logs will be exported to logs/fraud-sim.app-log relative to the docker file.
Using the Application without Docker
Prerequisites
To run this simulator requires:
A java 11 runtime
Configuration
Property
Description
Default Value
simulator.http.port
Defines the port upon which to listen for http requests.
55555
server.port
Defines the port the application will run on
8080
The fraud, payment-initiation and sanctions simulator will all listen on port 55555, so when running together on one host it is important to override the properties to give them unique ports.
Download
The fraud simulator is available here:
nexus.ipf.iconsolutions.com/repository/ipf-releases-temp/com/iconsolutions/ipf/sample/samplesystems/fraud-simulator-http/2023.1.0/fraud-simulator-http-2023.1.0-runnable.jar
Running
Once downloaded, place it into this directory and run:
java -cp "fraud-simulator-http-2023.1.0-runnable.jar:config" -D"loader.main"="com.iconsolutions.samplesystems.fraud.simulator.FraudSimulatorHttpApplication" "org.springframework.boot.loader.PropertiesLauncher"
The application will then start and be available from:
localhost:55555/index.html
Note that on Windows, the colon in the run command should be replaced by a semicolon.
Payment Initiation Simulator
Developer App
------------------------------
UTILS1 - IPF Developer App
This document covers how to use the ipf-developer-app. There are two recommended approaches for the tutorial, both docker and non-docker. Let’s look at these:
Using the Application Through Docker
Below provides a simple docker file entry for the developer application:
  ipf-developer-app:
    image: registry.ipf.iconsolutions.com/ipf-developer-app:1.0.29
    container_name: ipf-developer-app
    ports:
      - 8081:8081
    volumes:
      - ./config/ipf-developer-app:/ipf-developer-app/conf
      - ./logs:/ipf/logs
    environment:
      - IPF_JAVA_ARGS=-Dma.glasnost.orika.writeClassFiles=false -Dma.glasnost.orika.writeSourceFiles=false -Dconfig.override_with_env_vars=true
    user: "1000:1000"
    depends_on:
      - ipf-mongo
    healthcheck:
      test: [ "CMD", "curl", "http://localhost:8081/actuator/health" ]
In addition, the application will need configuration which is placed in the application.conf file within a config directory relative to the above docker entry. The conf file contains:
flow-restart-settings {
  min-backoff = 1s
  max-backoff = 5s
  random-factor = 0.25
  max-restarts = 5
  max-restarts-within = 10m
}
spring.data.mongodb.uri = ${?ipf.mongodb.url}
actor-system-name = ipf-developer
ipf.mongodb.url = "mongodb://ipf-mongo:27017/ipf"
ods.security.oauth.enabled = false
application.write.url = "http://ipf-tutorial-app:8080"
ipf.processing-data.ingress.transport = http
When executed the ipf-developer-app will listen for data on port 8081. This port will also be exposed to the host machine. The logs will be exported to logs/ipf-developer.app-log relative to the docker file.
Using the Application Without Docker
Prerequisites
To run this simulator requires:
A Java 11 runtime
A MongoDB database
Configuration
Configuration is through the application.conf file located in the config directory.
Property
Description
Example Value
ipf.mongodb.url
Defines the mongodb URL the developer application should use.
"mongodb://ipf-mongo:27017/ipf"
application.write.url
Defines the host and port that the main ipf application will expose it’s query service on
ipf-tutorial-app:8080
server.port
Defines the port the application will run on
8081
Download
The ipf-developer-app is available in the ipf-releases Nexus repository, for example version 1.0.29 is:
nexus.ipf.iconsolutions.com/repository/ipf-releases/com/iconsolutions/ipf/developer/ipf-developer-app/1.0.29/ipf-developer-app-1.0.29-runnable.jar
You must choose and download the correct version of the ipf-developer-app for the IPF Release you are using.
The versions must be compatible, please check and replace the numbers based on IPF’s release documentation and/or the details for your target environment!
Running
Once downloaded, place it into this directory and run (replacing the version you are using):
java -cp "ipf-developer-app-1.*.**-runnable.jar:config" -D"ma.glasnost.orika.writeClassFiles"=false -D"ma.glasnost.orika.writeSourceFiles"=false -D"config.override_with_env_vars"=true -D"loader.main"="com.iconsolutions.ipf.developer.app.IpfDeveloperApplication" "org.springframework.boot.loader.PropertiesLauncher"
The application will then start and be available from:
localhost:8081/explorer.html
Note that on Windows, the colon in the run command should be replaced by a semicolon.
Fraud Simulator
Using Debulker
------------------------------
Using Debulker
What is a Debulker?
A Debulker is a application module responsible for splitting large files into components which will be used for processing. Debulker expects to receive a notification about the file that needs to be debulked, together with the configuration name that is going to be used to split the file into components.
Debulker in your application
First step is to add the debulker starter maven modules to your IPF application pom:
<!--Responsible for debulking-->
<dependency>
    <artifactId>com.iconsolutions.ipf.debulk</artifactId>
    <groupId>ipf-debulker-starter</groupId>
</dependency>
<!--Responsible for cleaning up after bulk components are processed by interested party-->
<dependency>
    <artifactId>com.iconsolutions.ipf.debulk</artifactId>
    <groupId>ipf-debulker-housekeeping-starter</groupId>
</dependency>
This modules depends on some additional modules:
component-store - responsible for storing components produced by debulker.
ipf-debulker-new-file-notification - responsible for consuming the file notification which should trigger debulking of the file.
ipf-debulker-archiver - responsible for archiving the bulk file after it was successfully processed.
ipf-debulker-client-processing - notifies an external system that debulking is finished and produced components can be processed. This also consumes notification of successful processing of those components by the external system, so that it can start performing housekeeping.
Adding dependencies needed for the starter modules
<!-- ipf-component-store implementation which uses mongodb to store and read components -->
<dependency>
    <groupId>com.iconsolutions.ipf.componentstore</groupId>
    <artifactId>ipf-component-store-mongo</artifactId>
</dependency>
<!-- Kafka receive connector implementation of ipf-debulker-new-file-notification which consumes FileNotification message which tells debulker to process bulk file -->
<dependency>
    <groupId>com.iconsolutions.ipf.debulk</groupId>
    <artifactId>ipf-debulker-new-file-notification-connector-kafka</artifactId>
</dependency>
<!-- ipf-debulker-archiver implementation which archives processed bulk file to local file system -->
<dependency>
    <groupId>com.iconsolutions.ipf.debulk</groupId>
    <artifactId>ipf-debulker-archiver-local</artifactId>
</dependency>
<!-- Kafka connector implementation of ipf-debulker-client-processing -->
<dependency>
    <groupId>com.iconsolutions.ipf.debulk</groupId>
    <artifactId>ipf-debulker-client-processing-connector-kafka</artifactId>
</dependency>
Configuring Debulker in your application
In order to split the file of specific type (xml, json…​) and structure, configuration for splitting needs to be provided. This can be done by via ipf.debulker.configurations property. It expects an array of configuration objects, each one containing:
name (string) - used to uniquely identify the configuration. File notification will contain the configuration name which will be used to debulk the file.
splitter (string) - splitter type which will be used for extracting the components. Currently there are xml and json.
component-hierarchy (object) - tree structure representing the hierarchy of the components which will be extracted from the bulk file. Each node can have configured child nodes which will be extracted as separate components. The content of the child components will be ommited from the parent component.
Configuration example for debulking pain.001.001.09 XML file.
ipf.debulker {
  configurations = [
    {
      name = "pain.001.001.09"
      splitter = "xml"
      component-hierarchy {
        marker = "Document"
        children = [
          {
            marker = "CstmrCdtTrfInitn.PmtInf"
            children = [
              {
                marker = "CdtTrfTxInf"
              }
            ]
          }
        ]
      }
    }
  ]
}
This configuration tells us that the pain.001 XML file will be debulked into single Document component, which contains all child elements except CstmrCdtTrfInitn.PmtInf elements, which will be extracted as separate components. Each child PmtInf component will contain all child elements except CdtTrfTxInf elements, which will be extracted as separate child components of each PmtInf.
In case of a pain.001 XML file which has 3 PmtInf elements, each one containing 3 CdtTrfTxInf elements, debulker will produce 12 components:
1 Document component
3 PmtInf components
9 CdtTrfTxInf components
Configuring Archiver
Since we are using ipf-debulker-archiver-local for archiving, only thing that we need to configure is a path where bulk files will be archived:
ipf.debulker.archiving.path="/tmp/bulk_archive"
Docker Setup for ipf-debulker-tutorial-app
  ipf-debulker-tutorial-app:
    image: registry.ipf.iconsolutions.com/ipf-debulker-tutorial-app:latest
    container_name: ipf-debulker-tutorial-app
    ports:
      - 8080:8080
      - 8559:8558
      - 5006:5005
      - 55002:55001
      - 9002:9001
    volumes:
      - ./config/ipf-debulker-tutorial-app:/ipf-debulker-tutorial-app/conf
      - ./logs:/ipf/logs
    environment:
      - IPF_JAVA_ARGS=-Dma.glasnost.orika.writeClassFiles=false -Dma.glasnost.orika.writeSourceFiles=false -Dconfig.override_with_env_vars=true
    depends_on:
      - ipf-mongo
      - kafka
    healthcheck:
      test: [ "CMD", "curl", "http://localhost:8080/actuator/health" ]
In order to work ipf-debulker-tutorial-app requires MongoDB and Kafka.
Running the application
You can start application.yml using next command:
docker-compose -f application.yml up -d
Testing the application
Now the application is started we can test it, this is done by:
Providing a source data file in the expected location.
Sending a FileNotification to the ipf-debulk, via Kafka, to notify the application a file is ready for processing.
Validating that the file is debulked.
Sending a ComponentProcessingCompleteCommand, via Kafka, to trigger housekeeping which will delete components from component store and remove the bulk file.
Validating that the housekeeping is performed.
Step 1 - Creation of pain.001 file which will be debulked
Since we have configuration for debulking pain.001 XML file, we will use that one for testing.
Pain.001 file sample.
<Document xmlns="urn:iso:std:iso:20022:tech:xsd:pain.001.001.09">
    <CstmrCdtTrfInitn>
        <GrpHdr>
            <MsgId>abc</MsgId>
        </GrpHdr>
        <PmtInf>
            <PmtInfId>1</PmtInfId>
            <NbOfTxs>2</NbOfTxs>
            <CdtTrfTxInf>
                <PmtId>
                    <EndToEndId>1</EndToEndId>
                </PmtId>
                <Amt>
                    <InstdAmt Ccy="GBP">500.00</InstdAmt>
                </Amt>
            </CdtTrfTxInf>
            <CdtTrfTxInf>
                <PmtId>
                    <EndToEndId>2</EndToEndId>
                </PmtId>
            </CdtTrfTxInf>
        </PmtInf>
        <PmtInf>
            <PmtInfId>2</PmtInfId>
            <NbOfTxs>2</NbOfTxs>
            <CdtTrfTxInf>
                <PmtId>
                    <EndToEndId>3</EndToEndId>
                </PmtId>
            </CdtTrfTxInf>
            <CdtTrfTxInf>
                <PmtId>
                    <EndToEndId>4</EndToEndId>
                </PmtId>
            </CdtTrfTxInf>
        </PmtInf>
        <SplmtryData>
            <Envlp/>
        </SplmtryData>
    </CstmrCdtTrfInitn>
</Document>
File like this is already created and it is located in the solutions/add-debulker/docker/bulk_files/ directory.
Step 2 - Sending FileNotification to Kafka
We have configured the application to take FileNotifications from Kafka, that notification has several properties which must be provided:
configName - name of the configuration which will be used by debulker to debulk the bulk file into components.
bulkId - to correlate produced components by debulker.
fileProvider - name of the provider which will be used to retreive the bulk file for processing.
filePath - path to the file.
FileNotification message to send:
{
  "configName": "pain.001.001.09",
  "bulkId": "pain.001.12345",
  "fileProvider": "local",
  "filePath": "/tmp/bulk_files",
  "fileName": "pain_001_test.xml"
}
we can push the fileNotification to Kafka using Kafka console producer:
./kafka-console-producer.sh --topic FILE_NOTIFICATION_REQUEST --bootstrap-server localhost:9092
Message we are sending should be in one line:
{"configName": "pain.001.001.09", "bulkId": "pain.001.12345", "fileProvider": "local", "filePath": "/tmp/bulk_files/", "fileName": "pain_001_test.xml"}
Step 3 Validating that the file is debulked
At this point the debulker should have received the notification, accessed the file and debulked it. There are a couple of things we should check:
pain_001_test.xml file should exist in bulk_archive directory
components should be present in the component store
InitiateComponentProcessingCommand (notifies interested party that bulk components are ready for processing) is sent to kafka. Default topic is CLIENT_PROCESSING_REQUEST.
Easiest way to check components are via component store REST API.
You will need to add ipf-component-store-service maven dependency:
<dependency>
    <groupId>com.iconsolutions.ipf.componentstore</groupId>
    <artifactId>ipf-component-store-service</artifactId>
</dependency>
We should run next command from command line to get all components related to our bulk:
curl http://localhost:8080/v1/components/pain.001.12345 | json_pp
This should be the expected output for debulked pain_001_test.xml file (component number and their content should be the same):
[
   {
      "bulkId" : {
         "value" : "pain.001.12345"
      },
      "content" : "<CdtTrfTxInf><PmtId><EndToEndId>1</EndToEndId></PmtId><Amt><InstdAmt Ccy=\"GBP\">500.00</InstdAmt></Amt></CdtTrfTxInf>",
      "creationTime" : "2023-02-27T14:15:52.788Z",
      "custom" : null,
      "id" : {
         "value" : "a78d25e5-3625-4fe6-86dd-b220b92d9e14"
      },
      "index" : 2,
      "marker" : "Document.CstmrCdtTrfInitn.PmtInf.CdtTrfTxInf",
      "parentId" : {
         "value" : "a78d25e5-3625-4fe6-86dd-b220b92d9e14"
      }
   },
   {
      "bulkId" : {
         "value" : "pain.001.12345"
      },
      "content" : "<CdtTrfTxInf><PmtId><EndToEndId>2</EndToEndId></PmtId></CdtTrfTxInf>",
      "creationTime" : "2023-02-27T14:15:52.845Z",
      "custom" : null,
      "id" : {
         "value" : "cc1a2d7c-4f94-4c94-a755-242c99881162"
      },
      "index" : 3,
      "marker" : "Document.CstmrCdtTrfInitn.PmtInf.CdtTrfTxInf",
      "parentId" : {
         "value" : "cc1a2d7c-4f94-4c94-a755-242c99881162"
      }
   },
   {
      "bulkId" : {
         "value" : "pain.001.12345"
      },
      "content" : "<PmtInf><PmtInfId>1</PmtInfId><NbOfTxs>2</NbOfTxs></PmtInf>",
      "creationTime" : "2023-02-27T14:15:52.851Z",
      "custom" : null,
      "id" : {
         "value" : "bd8f8a58-e9d8-4e16-805e-5b7b1579584c"
      },
      "index" : 1,
      "marker" : "Document.CstmrCdtTrfInitn.PmtInf",
      "parentId" : {
         "value" : "bd8f8a58-e9d8-4e16-805e-5b7b1579584c"
      }
   },
   {
      "bulkId" : {
         "value" : "pain.001.12345"
      },
      "content" : "<CdtTrfTxInf><PmtId><EndToEndId>3</EndToEndId></PmtId></CdtTrfTxInf>",
      "creationTime" : "2023-02-27T14:15:52.857Z",
      "custom" : null,
      "id" : {
         "value" : "ab312e01-9385-4846-93c4-75bdd0d94c66"
      },
      "index" : 5,
      "marker" : "Document.CstmrCdtTrfInitn.PmtInf.CdtTrfTxInf",
      "parentId" : {
         "value" : "ab312e01-9385-4846-93c4-75bdd0d94c66"
      }
   },
   {
      "bulkId" : {
         "value" : "pain.001.12345"
      },
      "content" : "<CdtTrfTxInf><PmtId><EndToEndId>4</EndToEndId></PmtId></CdtTrfTxInf>",
      "creationTime" : "2023-02-27T14:15:52.863Z",
      "custom" : null,
      "id" : {
         "value" : "fd9a777d-e012-44a5-8560-903cdafe65f6"
      },
      "index" : 6,
      "marker" : "Document.CstmrCdtTrfInitn.PmtInf.CdtTrfTxInf",
      "parentId" : {
         "value" : "fd9a777d-e012-44a5-8560-903cdafe65f6"
      }
   },
   {
      "bulkId" : {
         "value" : "pain.001.12345"
      },
      "content" : "<PmtInf><PmtInfId>2</PmtInfId><NbOfTxs>2</NbOfTxs></PmtInf>",
      "creationTime" : "2023-02-27T14:15:52.868Z",
      "custom" : null,
      "id" : {
         "value" : "f9086dbb-5f8e-43ee-bf83-5ee92f0255bb"
      },
      "index" : 4,
      "marker" : "Document.CstmrCdtTrfInitn.PmtInf",
      "parentId" : {
         "value" : "f9086dbb-5f8e-43ee-bf83-5ee92f0255bb"
      }
   },
   {
      "bulkId" : {
         "value" : "pain.001.12345"
      },
      "content" : "<Document xmlns=\"urn:iso:std:iso:20022:tech:xsd:pain.001.001.09\"><CstmrCdtTrfInitn><GrpHdr><MsgId>abc</MsgId></GrpHdr><SplmtryData><Envlp></Envlp></SplmtryData></CstmrCdtTrfInitn></Document>",
      "creationTime" : "2023-02-27T14:15:52.873Z",
      "custom" : null,
      "id" : {
         "value" : "c35a269e-4e58-4b16-83b5-32c2bb5000f4"
      },
      "index" : 0,
      "marker" : "Document",
      "parentId" : {
         "value" : "c35a269e-4e58-4b16-83b5-32c2bb5000f4"
      }
   }
]
Checking whether InitiateComponentProcessingCommand is sent to kafka by starting console consumer(this is the command which is sent to the client application/flow to inform it a bulk has been received, debulked and the components are ready for processing):
./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic CLIENT_PROCESSING_REQUEST --from-beginning
There should be a single message which should look like this:
{"bulkId":"pain.001.12345"}
Step 4: Triggering debulker housekeeping
Next step would be to send the ComponentProcessingCompleteCommand to notify debulker that components are processed and debulker can perform housekeeping (delete the file, remove components from component store). Default topic for sending this message is CLIENT_PROCESSING_RESPONSE
Starting kafka console producer:
./kafka-console-producer.sh --topic CLIENT_PROCESSING_RESPONSE --bootstrap-server localhost:9092
Sending the message:
{"bulkId": "pain.001.12345"}
Step 5: Validating that housekeeping is performed
There are a couple of things we should check:
pain_001_test.xml file should be removed from bulk_files directory
all components related to bulkId="pain.001.12345" should be removed from component store
Conclusions
In this section we:
Successfully configured debulker in our IPF application
Processed a pain.001 XML file and validated that components are produced and file is archived.
Triggered housekeeping of bulk by deleting a bulk file and deleting components from component store.
Developer App
------------------------------